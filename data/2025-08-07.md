<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 133]
- [cs.CL](#cs.CL) [Total: 67]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.LG](#cs.LG) [Total: 78]
- [cs.AI](#cs.AI) [Total: 31]
- [cs.RO](#cs.RO) [Total: 16]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Text2VR: Automated instruction Generation in Virtual Reality using Large language Models for Assembly Task](https://arxiv.org/abs/2508.03699)
*Subin Raj Peter*

Main category: cs.CV

TL;DR: 本文提出了一种利用大型语言模型（LLMs）从文本输入自动生成虚拟培训内容的方法，解决了VR培训应用开发中的资源挑战，提升了培训效果并降低了开发成本。


<details>
  <summary>Details</summary>
Motivation: 虽然VR技术能够提供沉浸式、互动且无风险的培训环境，但在培训应用开发中，创建准确且吸引人的教学内容需要大量时间、专业知识和资源，这些挑战限制了VR培训的推广和应用。

Method: 提出一个结合LLM模块和智能模块的系统：LLM模块从输入文本中提取任务相关信息，智能模块将这些信息解释后，通过指令生成器创建培训内容（包括物体颜色变化和动画演示）。

Result: 该方法能够自动生成VR培训内容，提高培训效率同时显著减少开发开销，使VR培训更具扩展性和适应性。

Conclusion: 利用LLMs自动生成VR培训内容的方法不仅提升了培训效果，还解决了开发成本问题，推动VR培训更广泛地应用于不断变化的工业需求中。

Abstract: Virtual Reality (VR) has emerged as a powerful tool for workforce training,
offering immersive, interactive, and risk-free environments that enhance skill
acquisition, decision-making, and confidence. Despite its advantages,
developing VR applications for training remains a significant challenge due to
the time, expertise, and resources required to create accurate and engaging
instructional content. To address these limitations, this paper proposes a
novel approach that leverages Large Language Models (LLMs) to automate the
generation of virtual instructions from textual input. The system comprises two
core components: an LLM module that extracts task-relevant information from the
text, and an intelligent module that transforms this information into animated
demonstrations and visual cues within a VR environment. The intelligent module
receives input from the LLM module and interprets the extracted information.
Based on this, an instruction generator creates training content using relevant
data from a database. The instruction generator generates the instruction by
changing the color of virtual objects and creating animations to illustrate
tasks. This approach enhances training effectiveness and reduces development
overhead, making VR-based training more scalable and adaptable to evolving
industrial needs.

</details>


### [2] [Outlier Detection Algorithm for Circle Fitting](https://arxiv.org/abs/2508.03720)
*Ahmet Gökhan Poyraz*

Main category: cs.CV

TL;DR: 本文提出了一种基于极坐标的异常值检测算法（PCOD），用于在圆形拟合应用中提高精度。该算法将点集转换为极坐标，通过计算局部和全局标准差来识别异常值，并在工业垫圈的高精度直径测量中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 在工业质量控制中，圆形拟合常受噪声影响而精度降低，传统方法通常先进行异常值检测和去除。现有的算法在噪声环境下效果不佳，因此需要一种更有效的异常值检测方法以提升圆形拟合的准确性。

Method: 1. 将点集转换为极坐标；2. 计算局部和全局标准差；3. 通过比较局部均值与全局标准差识别异常点。随后，使用该方法对工业垫圈进行了直径测量：首先通过机器视觉系统获取图像，经过预处理（包括亚像素边缘检测）得到边缘点；然后用PCOD算法清理这些点；最后应用10种不同的圆形拟合算法进行拟合。

Result: 在实验中，PCOD算法结合多种拟合算法均表现出优异的性能，相比其他5种异常值检测方法具有更高的精度，在工业垫圈数据集上验证了其准确性。

Conclusion: PCOD算法在异常检测和去除方面优于现有方法，有效提升了圆形拟合在工业应用（如高精度直径测量）中的精度和可靠性。

Abstract: Circle fitting methods are extensively utilized in various industries,
particularly in quality control processes and design applications. The
effectiveness of these algorithms can be significantly compromised when the
point sets to be predicted are noisy. To mitigate this issue, outlier detection
and removal algorithms are often applied before the circle fitting procedure.
This study introduces the Polar Coordinate-Based Outlier Detection (PCOD)
algorithm, which can be effectively employed in circle fitting applications. In
the proposed approach, the point set is first transformed into polar
coordinates, followed by the calculation of both local and global standard
deviations. Outliers are then identified by comparing local mean values with
the global standard deviation. The practicality and efficiency of the proposed
method are demonstrated by focusing on the high-precision diameter measurement
of industrial washer parts. Images from a machine vision system are processed
through preprocessing steps, including sub-pixel edge detection. The resulting
sub-pixel edge points are then cleaned using the proposed outlier detection and
removal algorithm, after which circle fitting is performed. A comparison is
made using ten different circle fitting algorithms and five distinct outlier
detection methods. The results indicate that the proposed method outperforms
the other approaches, delivering the best performance in terms of accuracy
within the dataset, thereby demonstrating its potential for enhancing circle
fitting applications in industrial environments.

</details>


### [3] [Enhancing Diameter Measurement Accuracy in Machine Vision Applications](https://arxiv.org/abs/2508.03721)
*Ahmet Gokhan Poyraz,Ahmet Emir Dirik,Hakan Gurkan,Mehmet Kacmaz*

Main category: cs.CV

TL;DR: 该论文针对使用远心镜头的相机测量系统中由机械和软件因素引起的测量误差问题，提出两种基于已知参考件的创新方法（转换因子法和像素法），显著将误差从13-114微米降低至1-2微米。


<details>
  <summary>Details</summary>
Motivation: 现有测量系统即使用远心镜头也难以避免误差，尤其在测量直径不同的零件时，现有方法会导致13-114微米的显著误差，因此需要开发更高精度的测量方案。

Method: 1. **转换因子法**：利用已知参考件估算转换因子，再计算未知零件直径（mm）
2. **像素法**：直接基于参考件的像素级直径信息估算未知零件直径（mm）
实验装置使用工业相机和远心镜头，在玻璃件（1-12mm）和金属件（3-24mm）上测试。

Result: 原始测量误差为13-114微米，使用新方法后：
- 所有尺寸零件的误差均降至1-2微米
- 仅需少量参考件即可实现视场内所有零件的高精度测量

Conclusion: 所提方法显著降低工业相机远心镜头系统的测量误差，将误差降低两个数量级（降至微米级），提升了测量可靠性和适用性，为直径测量研究提供了突破性进展。

Abstract: In camera measurement systems, specialized equipment such as telecentric
lenses is often employed to measure parts with narrow tolerances. However,
despite the use of such equipment, measurement errors can occur due to
mechanical and software-related factors within the system. These errors are
particularly evident in applications where parts of different diameters are
measured using the same setup. This study proposes two innovative approaches to
enhance measurement accuracy using multiple known reference parts: a conversion
factor-based method and a pixel-based method. In the first approach, the
conversion factor is estimated from known references to calculate the diameter
(mm) of the unknown part. In the second approach, the diameter (mm) is directly
estimated using pixel-based diameter information from the references. The
experimental setup includes an industrial-grade camera and telecentric lenses.
Tests conducted on glass samples (1-12 mm) and metal workpieces (3-24 mm) show
that measurement errors, which originally ranged from 13-114 micrometers, were
reduced to 1-2 micrometers using the proposed methods. By utilizing only a few
known reference parts, the proposed approach enables high-accuracy measurement
of all parts within the camera's field of view. Additionally, this method
enhances the existing diameter measurement literature by significantly reducing
error rates and improving measurement reliability.

</details>


### [4] [Multimodal Video Emotion Recognition with Reliable Reasoning Priors](https://arxiv.org/abs/2508.03722)
*Zhepeng Wang,Yingjian Zhu,Guanghao Dong,Hongzhu Yi,Feng Chen,Xinming Wang,Jun Xie*

Main category: cs.CV

TL;DR: 该研究提出了一种可信赖的多模态情感识别框架，利用大语言模型（如Gemini）生成细粒度的模态可分离推理痕迹作为先验知识，并在融合阶段注入以丰富跨模态交互。同时，该研究引入了平衡的双对比学习策略来解决多模态情感识别中的类别不平衡问题。在MER2024基准测试上的实验显示，该框架带来了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 多模态情感识别在实际应用中的主要问题包括如何有效融合来自不同模态的信息以及数据集中的严重类别不平衡问题。虽然MLLMs具有强大的知识推理能力，但其推理的可靠性尚未在情感识别领域得到充分探索。本研究旨在引入MLLMs进行先验推理生成，并设计一种新的针对类别不平衡问题的学习方法。

Method: 1. 利用大语言模型Gemini生成细粒度的、模态可分的推理痕迹（reasoning traces），作为先验知识嵌入。
2. 在融合阶段融入这些先验知识，以增强跨模态交互。
3. 提出平衡的双对比学习（Balanced Dual-Contrastive Learning）损失，同时在类间和类内层面调整特征分布平衡，缓解类别不平衡问题。
4. 在轻量级融合网络上实现方法，用于情感预测。

Result: 在MER2024基准测试上的实验表明，方法取得了实质性性能提升。具体而言，由MLLM产生的可靠推理能够与轻量级融合网络的领域适应性相结合，实现鲁棒且可扩展的情感识别。

Conclusion: 通过引入大语言模型生成的先验知识，结合平衡的双对比学习损失，本研究构建了一个高效的多模态情感识别框架，解决了模态融合和类别不平衡的关键问题。实验证明该方法显著提高了多模态情感识别性能，展示了MLLMs与轻量级模型协同工作的潜力。

Abstract: This study investigates the integration of trustworthy prior reasoning
knowledge from MLLMs into multimodal emotion recognition. We employ Gemini to
generate fine-grained, modality-separable reasoning traces, which are injected
as priors during the fusion stage to enrich cross-modal interactions. To
mitigate the pronounced class-imbalance in multimodal emotion recognition, we
introduce Balanced Dual-Contrastive Learning, a loss formulation that jointly
balances inter-class and intra-class distributions. Applied to the MER2024
benchmark, our prior-enhanced framework yields substantial performance gains,
demonstrating that the reliability of MLLM-derived reasoning can be
synergistically combined with the domain adaptability of lightweight fusion
networks for robust, scalable emotion recognition.

</details>


### [5] [From Waveforms to Pixels: A Survey on Audio-Visual Segmentation](https://arxiv.org/abs/2508.03724)
*Jia Li,Yapeng Tian*

Main category: cs.CV

TL;DR: 本文是关于视听分割（AVS）领域的一篇综述，全面介绍了该领域的任务定义、数据集、评估方法和技术演进。文章分析了各种架构设计、多模态融合方法、解码器策略以及训练范式（从监督学习到训练无关方法），并评估了不同因素对模型性能的影响。最后提出了当前挑战（如时间建模不足、视觉偏向、复杂场景稳健性差等）和未来方向（改进时序推理与多模态融合、利用基础模型增强泛化能力、减少标注依赖等）。


<details>
  <summary>Details</summary>
Motivation: 视听分割（AVS）作为多模态感知领域的重要研究方向，可实现对发声物体的精细化定位与理解。然而该领域缺乏系统性综述，研究者难以把握技术全貌与核心挑战。本文旨在梳理AVS领域的发展脉络，通过归纳技术路线、对比方法性能、揭示现存问题，为后续研究提供清晰的技术蓝图和研究方向。

Method: 1) 任务定义与背景：明确AVS任务目标（视听对齐+发声对象分割）及技术边界；2) 数据处理：整理常用数据集（包括视觉/音频/标注数据规范）和评估指标（如mIOU/dice/JF）；3) 技术分析框架：按三大模块拆解现有方法——单模态编码（视觉/音频骨干网络）、多模态融合（特征级/预测级融合策略）、解码器设计（掩码/轮廓/时序感知解码）；4) 训练范式：对比监督/弱监督/无监督方法的差异；5) 横向量化评估：在标准数据集上统一测试不同方法，剖析架构选择、融合机制、损失函数等对性能的影响。

Result: 1) 多模态融合策略直接影响性能：特征级融合在简单场景有效，预测级融合对时序上下文建模更优；2) 视觉主导现象明显：多数方法音频仅起辅助作用；3) 弱监督方法性能逼近全监督：可节省90%标注成本；4) 基础模型表现出潜力：如CLIP驱动的zero-shot方法可达监督模型80%性能；5) 现存瓶颈：视频级方法mIOU仅65%左右（静态图像方法达74%），说明时间建模能力不足；复杂声景下性能下降30-40%，反映稳健性缺陷。

Conclusion: AVS技术正处于快速发展阶段，核心挑战在于平衡模态贡献、强化时序推理、提升复杂环境鲁棒性。未来应关注三大方向：1) 开发高效时空联合建模架构（如Transformer-based video models）；2) 利用大规模预训练基础模型突破少样本/泛化瓶颈；3) 结合自监督与认知推理构建更智能的分割系统（如神经符号方法）。该综述为建立更健壮、高效的视听感知系统提供了理论基础与技术路线参考。

Abstract: Audio-Visual Segmentation (AVS) aims to identify and segment sound-producing
objects in videos by leveraging both visual and audio modalities. It has
emerged as a significant research area in multimodal perception, enabling
fine-grained object-level understanding. In this survey, we present a
comprehensive overview of the AVS field, covering its problem formulation,
benchmark datasets, evaluation metrics, and the progression of methodologies.
We analyze a wide range of approaches, including architectures for unimodal and
multimodal encoding, key strategies for audio-visual fusion, and various
decoder designs. Furthermore, we examine major training paradigms, from fully
supervised learning to weakly supervised and training-free methods. Notably, we
provide an extensive comparison of AVS methods across standard benchmarks,
highlighting the impact of different architectural choices, fusion strategies,
and training paradigms on performance. Finally, we outline the current
challenges, such as limited temporal modeling, modality bias toward vision,
lack of robustness in complex environments, and high computational demands, and
propose promising future directions, including improving temporal reasoning and
multimodal fusion, leveraging foundation models for better generalization and
few-shot learning, reducing reliance on labeled data through selfand weakly
supervised learning, and incorporating higher-level reasoning for more
intelligent AVS systems.

</details>


### [6] [A Large Language Model Powered Integrated Circuit Footprint Geometry Understanding](https://arxiv.org/abs/2508.03725)
*Yida Wang,Taiting Lu,Runze Liu,Lanqing Yang,Yifan Yang,Zhe Chen,Yuehai Wang,Yixin Liu,Kaiyuan Lin,Xiaomeng Chen,Dian Ding,Yijie Li,Yi-Chao Chen,Yincheng Jin,Mahanth Gowda*

Main category: cs.CV

TL;DR: 本文提出了LLM4-IC8K框架，用于自动标注印刷电路板（PCB）上集成电路（IC）的封装几何。该框架将IC机械图视为图像，利用大语言模型（LLM）进行结构化几何解释。为了解决当前大规模多模态模型（LMM）在几何感知上的不足，作者引入了一种两阶段训练方法，并创建了包含8608个样本的多模态数据集ICGeo8K。实验证明，该方法优于现有LMM。


<details>
  <summary>Details</summary>
Motivation: 由于PCB上IC封装几何标注的图纸结构复杂且抽象，传统的自动化解析方法面临挑战。目前尚无直接从IC机械图自动标注封装几何的方法。因此，本文旨在解决这一问题，并发现当前的大规模多模态模型在几何感知上的不足，从而提出了新的解决方案。

Method: LLM4-IC8K框架将IC机械图视为图像输入，利用多模态LLM进行结构化几何解释。该框架通过三个子任务模拟工程师思维：感知引脚数量、计算每个引脚的中心坐标以及估算每个引脚的尺寸。方法分为两阶段：首先在合成数据上训练模型以学习基础几何推理，然后在真实世界图纸数据上进行微调，提高模型的鲁棒性和准确性。作者为此创建了ICGeo8K数据集，包含8608个样本（4138个手工采集真实样本和4470个合成样本）。

Result: 经过广泛实验，提出的模型在ICGeo8K数据集上的表现超过了当前的SOTA模型。这表明LLM4-IC8K能够有效解决封装几何标注问题。

Conclusion: 本文提出的LLM4-IC8K框架通过分阶段训练和特定任务设计，成功解决了当前LMM在几何感知上的不足，实现了在无人监督的情况下进行精确几何标注。新提出的大型数据集也为后续研究提供了基础。这个方法有望用于电子设计自动化（EDA）行业中，提高工作效率。

Abstract: Printed-Circuit-board (PCB) footprint geometry labeling of integrated
circuits (IC) is essential in defining the physical interface between
components and the PCB layout, requiring exceptional visual perception
proficiency. However, due to the unstructured footprint drawing and abstract
diagram annotations, automated parsing and accurate footprint geometry modeling
remain highly challenging. Despite its importance, no methods currently exist
for automated package geometry labeling directly from IC mechanical drawings.
In this paper, we first investigate the visual perception performance of Large
Multimodal Models (LMMs) when solving IC footprint geometry understanding. Our
findings reveal that current LMMs severely suffer from inaccurate geometric
perception, which hinders their performance in solving the footprint geometry
labeling problem. To address these limitations, we propose LLM4-IC8K, a novel
framework that treats IC mechanical drawings as images and leverages LLMs for
structured geometric interpretation. To mimic the step-by-step reasoning
approach used by human engineers, LLM4-IC8K addresses three sub-tasks:
perceiving the number of pins, computing the center coordinates of each pin,
and estimating the dimensions of individual pins. We present a two-stage
framework that first trains LMMs on synthetically generated IC footprint
diagrams to learn fundamental geometric reasoning and then fine-tunes them on
real-world datasheet drawings to enhance robustness and accuracy in practical
scenarios. To support this, we introduce ICGeo8K, a multi-modal dataset with
8,608 labeled samples, including 4138 hand-crafted IC footprint samples and
4470 synthetically generated samples. Extensive experiments demonstrate that
our model outperforms state-of-the-art LMMs on the proposed benchmark.

</details>


### [7] [TIR-Diffusion: Diffusion-based Thermal Infrared Image Denoising via Latent and Wavelet Domain Optimization](https://arxiv.org/abs/2508.03727)
*Tai Hyoung Rhee,Dong-guw Lee,Ayoung Kim*

Main category: cs.CV

TL;DR: 本文提出了一种基于扩散模型的TIR图像去噪框架，利用潜在空间和小波变换优化，结合新颖的损失函数和级联细化阶段，实现了高性能的去噪效果，并在多个数据集上验证了其优越性和零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 针对热红外(TIR)图像在机器人感知任务中受到固定图案噪声影响的问题，传统方法难以有效去除非均匀噪声，影响后续目标检测、定位和建图任务。因此，需要一种强大的去噪方法以提升TIR图像质量。

Method: 1. 利用预训练的Stable Diffusion模型作为基础。2. 通过结合潜在空间损失和离散小波变换(DWT)/双树复小波变换(DTCWT)损失的新损失函数进行微调。3. 引入级联细化阶段，增强细节以生成高质量结果。

Result: 在基准数据集上的实验表明，该方法在性能上优于现有先进去噪方法。同时，在多样而具有挑战性的真实世界TIR数据集上展示了较强的零样本泛化能力。

Conclusion: 该方法显著提高了TIR图像的去噪效果，对机器人实际部署具有重要价值，其零样本泛化能力尤其适合复杂环境中的应用场景。

Abstract: Thermal infrared imaging exhibits considerable potentials for robotic
perception tasks, especially in environments with poor visibility or
challenging lighting conditions. However, TIR images typically suffer from
heavy non-uniform fixed-pattern noise, complicating tasks such as object
detection, localization, and mapping. To address this, we propose a
diffusion-based TIR image denoising framework leveraging latent-space
representations and wavelet-domain optimization. Utilizing a pretrained stable
diffusion model, our method fine-tunes the model via a novel loss function
combining latent-space and discrete wavelet transform (DWT) / dual-tree complex
wavelet transform (DTCWT) losses. Additionally, we implement a cascaded
refinement stage to enhance fine details, ensuring high-fidelity denoising
results. Experiments on benchmark datasets demonstrate superior performance of
our approach compared to state-of-the-art denoising methods. Furthermore, our
method exhibits robust zero-shot generalization to diverse and challenging
real-world TIR datasets, underscoring its effectiveness for practical robotic
deployment.

</details>


### [8] [What is Beneath Misogyny: Misogynous Memes Classification and Explanation](https://arxiv.org/abs/2508.03732)
*Kushal Kanwar,Dushyant Singh Chauhan,Gopendra Vikram Singh,Asif Ekbal*

Main category: cs.CV

TL;DR: 该论文提出了多模态方法MM-Misogyny，用于检测、分类和解释含性别歧视的社交网络模因（Memes）。提出的模型能分别分析模因中的文本和图像特征并进行融合，借助分类器和大型语言模型提供细粒度的分析。数据集WBMS包含四个类别的性别歧视模因，实验结果证明了模型优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现代社交网络中，看似无害的模因可能传播有害的意识形态如性别歧视。这些性别歧视模因本质多模态，且在不同社会背景下表现形式隐晦，因此检测和理解其为何具有性别歧视性是挑战。本文旨在解决检测、分类和解释性别歧视模因的问题。

Method: 提出新方法MM-Misogyny：1. 对模因的文本和图像模态分别处理；2. 利用交叉注意力机制融合模态信息；3. 通过分类器进行检测和分类；4. 借助大型语言模型（LLM）对性别歧视模因生成解释。在从网络收集的新数据集WBMS上实验，数据集将性别歧视模因分为厨房(Kitchen)、领导力(Leadership)、工作(Working)、购物(Shopping)四个类别。

Result: 实验结果证明MM-Misogyny模型在检测和分类性别歧视模因方面优于现有方法，且能提供细粒度的性别歧视行为解释。

Conclusion: 本文提出并验证了一种有效的多模态方法来检测、分类和解释性别歧视模因，并构建了新的数据集WBMS。该工作有助于理解性别歧视在日常生活中的运作方式。

Abstract: Memes are popular in the modern world and are distributed primarily for
entertainment. However, harmful ideologies such as misogyny can be propagated
through innocent-looking memes. The detection and understanding of why a meme
is misogynous is a research challenge due to its multimodal nature (image and
text) and its nuanced manifestations across different societal contexts. We
introduce a novel multimodal approach, \textit{namely},
\textit{\textbf{MM-Misogyny}} to detect, categorize, and explain misogynistic
content in memes. \textit{\textbf{MM-Misogyny}} processes text and image
modalities separately and unifies them into a multimodal context through a
cross-attention mechanism. The resulting multimodal context is then easily
processed for labeling, categorization, and explanation via a classifier and
Large Language Model (LLM). The evaluation of the proposed model is performed
on a newly curated dataset (\textit{\textbf{W}hat's \textbf{B}eneath
\textbf{M}isogynous \textbf{S}tereotyping (WBMS)}) created by collecting
misogynous memes from cyberspace and categorizing them into four categories,
\textit{namely}, Kitchen, Leadership, Working, and Shopping. The model not only
detects and classifies misogyny, but also provides a granular understanding of
how misogyny operates in domains of life. The results demonstrate the
superiority of our approach compared to existing methods. The code and dataset
are available at
\href{https://github.com/kushalkanwarNS/WhatisBeneathMisogyny/tree/main}{https://github.com/Misogyny}.

</details>


### [9] [StorySync: Training-Free Subject Consistency in Text-to-Image Generation via Region Harmonization](https://arxiv.org/abs/2508.03735)
*Gopalji Gaur,Mohammadreza Zolfaghari,Thomas Brox*

Main category: cs.CV

TL;DR: 提出了一种无需训练的方法来生成视觉故事中连贯且主题一致的图像序列。通过在预训练的扩散模型中动态调整跨图像注意力和区域特征协调，解决了现有方法需微调或重训练导致的效率低、影响模型原有能力的问题。


<details>
  <summary>Details</summary>
Motivation: 使用文本到图像扩散模型生成图像序列以讲述视觉故事时，维持不同场景间主题一致性的挑战明显。现有基于微调或重训练的方法计算成本高、耗时长，且常损害模型原有能力。

Method: 提出了基于预训练扩散模型的无需训练方法，包含两个核心机制：1) 通过masked cross-image attention sharing动态对齐批量图像中的主题特征；2) Regional Feature Harmonization用于细化视觉相似细节，提升主题一致性。

Result: 实验证明该方法在多种场景中能生成具有视觉一致主题的图像序列，同时保持扩散模型的创造力。

Conclusion: 该方法高效、无需训练，可与现有预训练扩散模型无缝协作，在维持模型原始生成能力的前提下显著提升跨图像主题的一致性。

Abstract: Generating a coherent sequence of images that tells a visual story, using
text-to-image diffusion models, often faces the critical challenge of
maintaining subject consistency across all story scenes. Existing approaches,
which typically rely on fine-tuning or retraining models, are computationally
expensive, time-consuming, and often interfere with the model's pre-existing
capabilities. In this paper, we follow a training-free approach and propose an
efficient consistent-subject-generation method. This approach works seamlessly
with pre-trained diffusion models by introducing masked cross-image attention
sharing to dynamically align subject features across a batch of images, and
Regional Feature Harmonization to refine visually similar details for improved
subject consistency. Experimental results demonstrate that our approach
successfully generates visually consistent subjects across a variety of
scenarios while maintaining the creative abilities of the diffusion model.

</details>


### [10] [Fusion of Pervasive RF Data with Spatial Images via Vision Transformers for Enhanced Mapping in Smart Cities](https://arxiv.org/abs/2508.03736)
*Rafayel Mkrtchyan,Armen Manukyan,Hrant Khachatrian,Theofanis P. Raptis*

Main category: cs.CV

TL;DR: 本文提出了一种深度学习方法，结合DINOv2架构和多用户/基站的RF数据与开源地图数据，以提高建筑地图的准确性。该方法利用视觉Transformer架构在一个统一框架中处理RF和地图模态，捕获空间依赖和结构先验，提升地图精度。在合成数据集上的实验显示，该方法在IoU等指标上显著超越基线。


<details>
  <summary>Details</summary>
Motivation: 传统智能城市地图构建技术（如卫星影像、激光雷达扫描和人工标注）存在成本高、可访问性差和准确性不足的问题。开源地图作为AI应用的地面实况来源，但存在人为错误和环境动态变化带来的偏差，影响神经网络性能。因此需要一种融合多种数据源的方法提升地图准确性。

Method: 提出基于深度学习的方法，整合DINOv2架构处理地图数据和多无线用户设备/基站采集的射频（RF）数据。采用视觉Transformer架构在统一框架中联合处理RF和地图两种模态，捕捉空间依赖和结构先验信息。模型仅利用聚合路径损耗信息解决地图构建问题。

Result: 在合成的数据集上评估，使用三个性能指标：Jaccard指数（IoU）、Hausdorff距离和Chamfer距离。所提方法的宏观IoU达到65.3%，显著优于三个基线：(1)错误地图基线（40.1%）；(2)文献中纯RF方法（37.3%）；(3)非AI融合基线（42.2%）。

Conclusion: 通过融合开源地图与RF数据，并利用深度学习方法（特别是DINOv2架构和视觉Transformer），可以显著提升建筑地图的准确性。该方法克服了传统地图技术的局限性和开源地图的偏差问题，为智能城市应用的精确环境测绘提供了有效解决方案。

Abstract: Environment mapping is an important computing task for a wide range of smart
city applications, including autonomous navigation, wireless network operations
and extended reality environments. Conventional smart city mapping techniques,
such as satellite imagery, LiDAR scans, and manual annotations, often suffer
from limitations related to cost, accessibility and accuracy. Open-source
mapping platforms have been widely utilized in artificial intelligence
applications for environment mapping, serving as a source of ground truth.
However, human errors and the evolving nature of real-world environments
introduce biases that can negatively impact the performance of neural networks
trained on such data. In this paper, we present a deep learning-based approach
that integrates the DINOv2 architecture to improve building mapping by
combining maps from open-source platforms with radio frequency (RF) data
collected from multiple wireless user equipments and base stations. Our
approach leverages a vision transformer-based architecture to jointly process
both RF and map modalities within a unified framework, effectively capturing
spatial dependencies and structural priors for enhanced mapping accuracy. For
the evaluation purposes, we employ a synthetic dataset co-produced by Huawei.
We develop and train a model that leverages only aggregated path loss
information to tackle the mapping problem. We measure the results according to
three performance metrics which capture different qualities: (i) The Jaccard
index, also known as intersection over union (IoU), (ii) the Hausdorff
distance, and (iii) the Chamfer distance. Our design achieves a macro IoU of
65.3%, significantly surpassing (i) the erroneous maps baseline, which yields
40.1%, (ii) an RF-only method from the literature, which yields 37.3%, and
(iii) a non-AI fusion baseline that we designed which yields 42.2%.

</details>


### [11] [VQ-DeepISC: Vector Quantized-Enabled Digital Semantic Communication with Channel Adaptive Image Transmission](https://arxiv.org/abs/2508.03740)
*Jianqiao Chen,Tingting Zhu,Huishi Song,Nan Ma,Xiaodong Xu*

Main category: cs.CV

TL;DR: 这篇论文提出了一个名为VQ-DeepISC的矢量量化数字语义通信系统，通过结合深度联合信源信道编码和矢量量化技术，实现了高效的基于索引的图像传输。系统利用Swin Transformer提取分层语义特征，并通过注意力机制进行信道自适应优化。此外，引入了分布正则化和指数移动平均来防止码本崩溃，并在IEEE 802.11a标准下使用QPSK和OFDM进行数字传输。实验结果表明系统在重建保真度上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统的语义特征离散化面临着在压缩过程中保持连续性和上下文信息的挑战，同时需要确保对信道退化的鲁棒性。现有的数字通信系统在传输高维的语义特征时效率低下，同时训练过程中的码本崩塌问题也会严重影响系统性能。

Method: 1）设计了一个基于Swin Transformer的骨干网络用于分层语义特征提取；2）引入矢量量化模块将特征投影到离散空间生成索引；3）设计注意力驱动的信道自适应模块动态优化索引传输；4）在训练中施加KL散度分布正则化（使码本使用分布均匀）并采用指数移动平均更新码本；5）使用QPSK调制和OFDM实现标准IEEE 802.11a的数字传输

Result: 实验结果表明，提出的VQ-DeepISC系统在图像传输任务中具有卓越的重建保真度，性能超过了现有基准方法。

Conclusion: 论文成功开发了一个鲁棒的数字语义通信系统，其提出的矢量量化、分布正则化和信道自适应机制有效解决了语义特征离散化和传输的核心挑战，为语义通信向实用化推进提供了重要方案。

Abstract: Discretization of semantic features enables interoperability between semantic
and digital communication systems, showing significant potential for practical
applications. The fundamental difficulty in digitizing semantic features stems
from the need to preserve continuity and context in inherently analog
representations during their compression into discrete symbols while ensuring
robustness to channel degradation. In this paper, we propose a vector quantized
(VQ)-enabled digital semantic communication system with channel adaptive image
transmission, named VQ-DeepISC. Guided by deep joint source-channel coding
(DJSCC), we first design a Swin Transformer backbone for hierarchical semantic
feature extraction, followed by VQ modules projecting features into discrete
latent spaces. Consequently, it enables efficient index-based transmission
instead of raw feature transmission. To further optimize this process, we
develop an attention mechanism-driven channel adaptation module to dynamically
optimize index transmission. Secondly, to counteract codebook collapse during
training process, we impose a distributional regularization by minimizing the
Kullback-Leibler divergence (KLD) between codeword usage frequencies and a
uniform prior. Meanwhile, exponential moving average (EMA) is employed to
stabilize training and ensure balanced feature coverage during codebook
updates. Finally, digital communication is implemented using quadrature phase
shift keying (QPSK) modulation alongside orthogonal frequency division
multiplexing (OFDM), adhering to the IEEE 802.11a standard. Experimental
results demonstrate superior reconstruction fidelity of the proposed system
over benchmark methods.

</details>


### [12] [Tobler's First Law in GeoAI: A Spatially Explicit Deep Learning Model for Terrain Feature Detection Under Weak Supervision](https://arxiv.org/abs/2508.03745)
*Wenwen Li,Chia-Yu Hsu,Maosheng Hu*

Main category: cs.CV

TL;DR: 开发一种弱监督深度学习模型，用于自然特征（特别是火星陨石坑）的目标检测，该方法结合了空间显式设计和注意力机制，在缺乏标注数据的情况下提升检测性能并推广到地球及其他行星的自然和人造特征。


<details>
  <summary>Details</summary>
Motivation: GeoAI（地理空间人工智能）面临两大挑战：训练数据不足及AI模型设计中忽视空间原则和空间效应。现有方法在自然特征检测（如火星陨石坑）中依赖大量人工标注且未充分整合地理学第一定律（空间自相关）。

Method: 1. 提出基于Tobler地理学第一定律的空间显式弱监督目标检测方法（仅需图像级标签）；
2. 在检测流程中集成注意力图并设计多阶段训练策略（包括预训练注意力模块与端到端微调）；
3. 以火星陨石坑检测为案例验证，模型可泛化至地球及其他行星的自然/人造特征。

Result: 模型在火星陨石坑检测任务中显著减少人工标注需求，验证了弱监督方法的有效性；实验证明其能泛化到多种地表目标（如地球上的自然/人造特征），性能优于传统监督方法（尤其在数据稀缺场景）。

Conclusion: 该研究通过融合空间原理（地理学第一定律）与深度学习（注意力机制、多阶段训练），提升了GeoAI在弱监督目标检测中的理论方法框架，为地理空间分析提供了更普适的解决方案。

Abstract: Recent interest in geospatial artificial intelligence (GeoAI) has fostered a
wide range of applications using artificial intelligence (AI), especially deep
learning, for geospatial problem solving. However, major challenges such as a
lack of training data and the neglect of spatial principles and spatial effects
in AI model design remain, significantly hindering the in-depth integration of
AI with geospatial research. This paper reports our work in developing a deep
learning model that enables object detection, particularly of natural features,
in a weakly supervised manner. Our work makes three contributions: First, we
present a method of object detection using only weak labels. This is achieved
by developing a spatially explicit model based on Tobler's first law of
geography. Second, we incorporate attention maps into the object detection
pipeline and develop a multistage training strategy to improve performance.
Third, we apply this model to detect impact craters on Mars, a task that
previously required extensive manual effort. The model generalizes to both
natural and human-made features on the surfaces of Earth and other planets.
This research advances the theoretical and methodological foundations of GeoAI.

</details>


### [13] [Closed-Circuit Television Data as an Emergent Data Source for Urban Rail Platform Crowding Estimation](https://arxiv.org/abs/2508.03749)
*Riccardo Fiorista,Awad Abdelhalim,Anson F. Stewart,Gabriel L. Pincus,Ian Thistle,Jinhua Zhao*

Main category: cs.CV

TL;DR: 提出用于估计地铁站台占用率的计算机视觉方法，包括目标检测、人群分类和语义分割，以及一种新的基于线性优化的计数方法，在隐私保护数据集上验证有效性。


<details>
  <summary>Details</summary>
Motivation: 实时准确估计地铁站台占用率有助于优化运营决策，提高安全性和乘客体验。但现有方法依赖间接代理数据（如自动检票系统数据或人工观察），而利用闭路电视（CCTV）视频可实现实时、准确估计。

Method: 对比了三种计算机视觉方法：1）使用YOLOv11、RT-DETRv2和APGCC进行物体检测计数；2）使用自定义训练的Crowd-ViT视觉变换器进行人群分级；3）使用DeepLabV3进行语义分割。并提出了一种新的基于线性优化的方法处理图像深度导致的乘客密度变化。

Result: 在包含600小时视频的WMATA隐私保护数据集上测试结果表明，计算机视觉方法能显著提升人群密度估计准确率，独立于其他数据源提供实时占用率数据，支撑及时运营响应。

Conclusion: CCTV视频可有效实现精确、实时的地铁站台拥挤度估计，为缓解拥挤提供数据支持。

Abstract: Accurately estimating urban rail platform occupancy can enhance transit
agencies' ability to make informed operational decisions, thereby improving
safety, operational efficiency, and customer experience, particularly in the
context of crowding. However, sensing real-time crowding remains challenging
and often depends on indirect proxies such as automatic fare collection data or
staff observations. Recently, Closed-Circuit Television (CCTV) footage has
emerged as a promising data source with the potential to yield accurate,
real-time occupancy estimates. The presented study investigates this potential
by comparing three state-of-the-art computer vision approaches for extracting
crowd-related features from platform CCTV imagery: (a) object detection and
counting using YOLOv11, RT-DETRv2, and APGCC; (b) crowd-level classification
via a custom-trained Vision Transformer, Crowd-ViT; and (c) semantic
segmentation using DeepLabV3. Additionally, we present a novel, highly
efficient linear-optimization-based approach to extract counts from the
generated segmentation maps while accounting for image object depth and, thus,
for passenger dispersion along a platform. Tested on a privacy-preserving
dataset created in collaboration with the Washington Metropolitan Area Transit
Authority (WMATA) that encompasses more than 600 hours of video material, our
results demonstrate that computer vision approaches can provide substantive
value for crowd estimation. This work demonstrates that CCTV image data,
independent of other data sources available to a transit agency, can enable
more precise real-time crowding estimation and, eventually, timely operational
responses for platform crowding mitigation.

</details>


### [14] [Modular Transformer Architecture for Precision Agriculture Imaging](https://arxiv.org/abs/2508.03751)
*Brian Gopalan,Nathalia Nascimento,Vishal Monga*

Main category: cs.CV

TL;DR: 该论文提出了一种质量感知模块化深度学习框架，用于无人机视频中的杂草分割。该框架通过分析图像质量条件（如模糊和噪声），并将输入路由到针对每种退化类型优化的专用预处理和变换器模型中，以解决常见的图像退化问题。系统首先使用平均绝对偏差和拉普拉斯算子分析无人机图像的噪声和模糊情况，然后动态地将数据路由到三种视变换器模型之一：用于干净图像的基线模型、用于降噪的带有Fisher Vector编码的改进变换器模型，或用于纠正模糊的带有Lucy-Robinson解码器的模型。该路由策略在分割质量和计算效率方面优于现有的基于CNN的方法。


<details>
  <summary>Details</summary>
Motivation: 解决精准农业中高效准确地从无人机视频进行杂草分割的关键需求，特别是针对常见图像退化问题，如模糊和噪声，以提高分割质量和计算效率。

Method: 1. 使用平均绝对偏差（MAD）分析图像噪声，使用拉普拉斯算子分析图像模糊程度；
2. 根据分析结果，动态路由到三个视变换器模型中的一个：
   - 基线模型处理干净图像；
   - 改进的变换器模型（带有Fisher Vector编码）处理含噪声图像；
   - 另一个带有Lucy-Robinson解码器的变换器模型处理模糊图像。

Result: 该路由策略使得系统在分割质量和计算效率上均优于现有的基于CNN的方法，展现了在农业深度学习应用中的显著进步。

Conclusion: 提出的质量感知模块化框架通过动态路由策略有效应对了图像退化问题，为无人机视频的杂草分割提供了一种高效准确的解决方案，提升了深度学习在农业中的应用水平。

Abstract: This paper addresses the critical need for efficient and accurate weed
segmentation from drone video in precision agriculture. A quality-aware modular
deep-learning framework is proposed that addresses common image degradation by
analyzing quality conditions-such as blur and noise-and routing inputs through
specialized pre-processing and transformer models optimized for each
degradation type. The system first analyzes drone images for noise and blur
using Mean Absolute Deviation and the Laplacian. Data is then dynamically
routed to one of three vision transformer models: a baseline for clean images,
a modified transformer with Fisher Vector encoding for noise reduction, or
another with an unrolled Lucy-Robinson decoder to correct blur. This novel
routing strategy allows the system to outperform existing CNN-based methods in
both segmentation quality and computational efficiency, demonstrating a
significant advancement in deep-learning applications for agriculture.

</details>


### [15] [Generating Synthetic Invoices via Layout-Preserving Content Replacement](https://arxiv.org/abs/2508.03754)
*Bevin V,Ananthakrishnan P V,Ragesh KR,Sanjay M,Vineeth S,Bibin Wilson*

Main category: cs.CV

TL;DR: 本文提出了一种通过OCR、LLM生成内容和修复技术生成高保真合成发票文档的方法。该技术从真实发票中提取内容和布局，替换关键信息为LLM生成的合成内容，然后通过修复技术在新发票图像中精确呈现新文本，输出一个视觉上真实的发票图像及对应结构化数据（JSON），解决发票数据处理中数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在自动发票处理中需要大规模多样数据集，但是在隐私法规和高成本人工标注的限制下，获取足够数据面临挑战。为减轻对真实数据的依赖，提出生成合成文档数据的方案扩大有限数据集的规模，用于训练更准确文档智能模型。

Method: 1. 利用OCR提取原始发票的文本内容和空间布局；2. 使用大型语言模型（LLM）生成上下文真实的关键字段的合成内容，替换所选数据；3. 使用修复技术清除原图上的文本，渲染新文本于原位置，保持原始布局、字体特征不变；4. 生成两个输出：视觉逼真的新发票图像和完全对齐的结构化数据（JSON）。

Result: 该方法能生成既高保真又视觉逼真的合成发票图像及其结构化JSON输出。整个过程自动化且可扩展，能从小型私有数据集放大为大规模多样化语料库，从而解决了数据限制问题。

Conclusion: 提出的合成发票数据处理管道克服了真实数据稀缺的挑战，能创造足够大规模和多样化的数据集，用于提升文档智能模型的鲁棒性和准确性，为自动化票据处理提供支持。

Abstract: The performance of machine learning models for automated invoice processing
is critically dependent on large-scale, diverse datasets. However, the
acquisition of such datasets is often constrained by privacy regulations and
the high cost of manual annotation. To address this, we present a novel
pipeline for generating high-fidelity, synthetic invoice documents and their
corresponding structured data. Our method first utilizes Optical Character
Recognition (OCR) to extract the text content and precise spatial layout from a
source invoice. Select data fields are then replaced with contextually
realistic, synthetic content generated by a large language model (LLM).
Finally, we employ an inpainting technique to erase the original text from the
image and render the new, synthetic text in its place, preserving the exact
layout and font characteristics. This process yields a pair of outputs: a
visually realistic new invoice image and a perfectly aligned structured data
file (JSON) reflecting the synthetic content. Our approach provides a scalable
and automated solution to amplify small, private datasets, enabling the
creation of large, varied corpora for training more robust and accurate
document intelligence models.

</details>


### [16] [Refine-IQA: Multi-Stage Reinforcement Finetuning for Perceptual Image Quality Assessment](https://arxiv.org/abs/2508.03763)
*Ziheng Jia,Jiaying Qian,Zicheng Zhang,Zijian Chen,Xiongkuo Min*

Main category: cs.CV

TL;DR: 本文提出Refine-IQA方法，一个多阶段强化学习微调框架，旨在提升大型多模态模型在图像质量评估（IQA）任务中的表现。通过构建高质量数据集和设计多任务奖励函数来增强模型的低层次视觉质量感知能力，并引入针对‘思考’过程的监督机制，显著提升了模型在质量评分和感知任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于强化微调（RFT）的IQA方法仅监督模型输出的奖励，而忽略了‘思考’过程（质量解释）的监督，这可能导致过程不正确或低效。此外，这些方法在下游IQA任务上直接微调，没有明确提升模型的低层次视觉质量感知能力，限制了性能上限。因此，需要一种新的方法来同时监督模型的‘思考’过程并增强其基础视觉感知能力。

Method: 分为两个阶段：

阶段一：构建Refine-Perception-20K数据集（包含12种主要失真类型、20,907张局部失真图像和超过55,000个RFT样本），设计多任务奖励函数（包括文本描述质量、失真类型识别、失真位置检测和失真严重度预测）来增强模型的低层次视觉质量感知能力。

阶段二：针对质量评分任务，引入了一种基于概率差异的奖励策略，以监督模型的‘思考’过程（即对输入图像质量进行分步推理）。该策略根据模型生成中间步骤的置信度差异设计奖励，从而引导模型生成更准确的推理过程。

Result: Refine-IQA系列模型在视觉质量感知和质量评分任务上均取得了优异的性能。在感知任务（如失真类型识别和位置检测）上有显著提升。评分任务上超过了现有强化学习微调方法（平均SROCC提高0.5%-2%），且模型具有强大的质量解释能力，在质量解释基准测试中也表现突出。

Conclusion: Refine-IQA框架通过分阶段强化学习微调，既提升了模型的低层次视觉感知，又通过‘思考’过程监督显著提高质量评分性能并激活了强大的解释能力。该方法为IQA领域的新范式，未来可扩展到其他视觉评估任务中。

Abstract: Reinforcement fine-tuning (RFT) is a proliferating paradigm for LMM training.
Analogous to high-level reasoning tasks, RFT is similarly applicable to
low-level vision domains, including image quality assessment (IQA). Existing
RFT-based IQA methods typically use rule-based output rewards to verify the
model's rollouts but provide no reward supervision for the "think" process,
leaving its correctness and efficacy uncontrolled. Furthermore, these methods
typically fine-tune directly on downstream IQA tasks without explicitly
enhancing the model's native low-level visual quality perception, which may
constrain its performance upper bound. In response to these gaps, we propose
the multi-stage RFT IQA framework (Refine-IQA). In Stage-1, we build the
Refine-Perception-20K dataset (with 12 main distortions, 20,907
locally-distorted images, and over 55K RFT samples) and design multi-task
reward functions to strengthen the model's visual quality perception. In
Stage-2, targeting the quality scoring task, we introduce a probability
difference reward involved strategy for "think" process supervision. The
resulting Refine-IQA Series Models achieve outstanding performance on both
perception and scoring tasks-and, notably, our paradigm activates a robust
"think" (quality interpreting) capability that also attains exceptional results
on the corresponding quality interpreting benchmark.

</details>


### [17] [4D-PreNet: A Unified Preprocessing Framework for 4D-STEM Data Analysis](https://arxiv.org/abs/2508.03775)
*Mingyu Liu,Zian Mao,Zhu Liu,Haoran Zhang,Jintao Guo,Xiaoya He,Xi Huang,Shufen Chu,Chun Cheng,Jun Ding,Yujun Xie*

Main category: cs.CV

TL;DR: 该研究开发了一种名为4D-PreNet的端到端深度学习框架，用于通用化处理4D-STEM实验数据。它能同步处理去噪、中心漂移校正和椭圆的畸变修正，提升数据质量和实时分析能力。


<details>
  <summary>Details</summary>
Motivation: 传统算法无法适应材料多样性与高吞吐实验需求，普遍存在噪声、束中心漂移和非圆形畸变问题。需要一种无需参数调整和适应材料变化的通用解决方案。

Method: 联合注意力增强的U-Net和ResNet，构建端到端框架。用多样化模拟数据训练模型（涵盖各种噪声强度、漂移幅度和畸变类型）。任务包括去噪、中心校正、畸变校准。模型输出同时处理三个问题。

Result: 模型在去噪环节使MSE降低50%以上；中心定位平均误差小于0.04像素；显著改善衍射图恢复质量。相比传统方法提高可靠性与精度，适应自动表征系统的实时性需求。

Conclusion: 4D-PreNet首次统一处理4D-STEM三大预处理问题，显著提升数据处理效率，适用于不同的实验条件。其无需人工干预特性为实时自动分析提供保障。

Abstract: Automated experimentation with real time data analysis in scanning
transmission electron microscopy (STEM) often require end-to-end framework. The
four-dimensional scanning transmission electron microscopy (4D-STEM) with
high-throughput data acquisition has been constrained by the critical
bottleneck results from data preprocessing. Pervasive noise, beam center drift,
and elliptical distortions during high-throughput acquisition inevitably
corrupt diffraction patterns, systematically biasing quantitative measurements.
Yet, conventional correction algorithms are often material-specific and fail to
provide a robust, generalizable solution. In this work, we present 4D-PreNet,
an end-to-end deep-learning pipeline that integrates attention-enhanced U-Net
and ResNet architectures to simultaneously perform denoising, center
correction, and elliptical distortion calibration. The network is trained on
large, simulated datasets encompassing a wide range of noise levels, drift
magnitudes, and distortion types, enabling it to generalize effectively to
experimental data acquired under varying conditions. Quantitative evaluations
demonstrate that our pipeline reduces mean squared error by up to 50% during
denoising and achieves sub-pixel center localization in the center detection
task, with average errors below 0.04 pixels. The outputs are bench-marked
against traditional algorithms, highlighting improvements in both noise
suppression and restoration of diffraction patterns, thereby facilitating
high-throughput, reliable 4D-STEM real-time analysis for automated
characterization.

</details>


### [18] [HPSv3: Towards Wide-Spectrum Human Preference Score](https://arxiv.org/abs/2508.03789)
*Yuhang Ma,Xiaoshi Wu,Keqiang Sun,Hongsheng Li*

Main category: cs.CV

TL;DR: 本文提出了Human Preference Score v3 (HPSv3)模型，解决了现有的文本到图像生成模型人工评估指标存在的三个问题：数据覆盖有限、特征提取不够优化和损失函数效率低。HPSv3基于一个名为HPDv3的广泛人类偏好数据集，包含108万文本-图像对和117万人工标注的成对比较，同时采用基于VLM的偏好模型和一个称为Chain-of-Human-Preference (CoHP)的质量提升方法。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像生成模型的评估指标在数据覆盖范围、特征提取效率和损失函数设计上不够理想，限制了其在多种实际应用中的适应性。为解决这些问题，我们提出了新的评估标准HPSv3，以期实现更接近人类感知的评估结果。

Method: 构建了HPDv3数据库，这是一个大规模的综合数据集，包含1.08M文本-图像对和1.17M标注的成对比较。训练基于VLM的偏好模型（HPSv3）时采用了专门设计的不确定性感知排序损失用于精细排序。进一步地提出了Chain-of-Human-Preference (CoHP)方法，在不需要外部数据的情况下，通过迭代地使用HPSv3在每个步骤优化图像质量。

Result: 经实验验证，HPSv3展现出在广泛场景下的图像评估鲁棒性。CoHP能够高效地提升图像生成质量，符合人类偏好。相关资源已在HPSv3主页公开。

Conclusion: HPSv3为评估文本到图像生成模型的有效性和人类对齐性提供了强大的依据。CoHP方法则提供了一种在无需额外数据的情况下提升生成图像质量的方案。

Abstract: Evaluating text-to-image generation models requires alignment with human
perception, yet existing human-centric metrics are constrained by limited data
coverage, suboptimal feature extraction, and inefficient loss functions. To
address these challenges, we introduce Human Preference Score v3 (HPSv3). (1)
We release HPDv3, the first wide-spectrum human preference dataset integrating
1.08M text-image pairs and 1.17M annotated pairwise comparisons from
state-of-the-art generative models and low to high-quality real-world images.
(2) We introduce a VLM-based preference model trained using an
uncertainty-aware ranking loss for fine-grained ranking. Besides, we propose
Chain-of-Human-Preference (CoHP), an iterative image refinement method that
enhances quality without extra data, using HPSv3 to select the best image at
each step. Extensive experiments demonstrate that HPSv3 serves as a robust
metric for wide-spectrum image evaluation, and CoHP offers an efficient and
human-aligned approach to improve image generation quality. The code and
dataset are available at the HPSv3 Homepage.

</details>


### [19] [RiemanLine: Riemannian Manifold Representation of 3D Lines for Factor Graph Optimization](https://arxiv.org/abs/2508.04335)
*Yanyan Li,Ze Yang,Keisuke Tateno,Federico Tombari Liang Zhao,Gim Hee Lee*

Main category: cs.CV

TL;DR: 提出了一种名为RiemanLine的最小化参数化方法，用于统一表示3D线段，包括独立线和并行线组。该方法在黎曼流形上构建，能够有效利用人造环境中常见的结构规则性，减少参数维度并提升姿态估计和线段重建的精度与稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有的3D线段参数化方法通常处理独立线段，而忽视了人造环境中普遍存在的结构性规则（如平行线集合）。这些规则具有降低参数数量、提升优化效率的潜力。当前方法在涉及平行线时存在参数冗余和显式约束的问题。

Method: 1. 解耦表示：将每个线段分解为全局（共享消失方向，位于单位球面S^2上）和局部（尺度化法向量，约束于正交子空间）分量；
2. 压缩参数化：n条平行线的参数从4n（正交形式）降至2n+2，隐式嵌入平行性；
3. 优化框架：在基于流形的捆绑调整中联合优化全局方向对齐（流形优化）和局部重投影。

Result: 在ICL-NUIM、TartanAir和合成数据集上：
- 显著提升相机位姿估计和线段重建精度
- 参数维度降低
- 收敛稳定性提高

Conclusion: RiemanLine通过流形上的统一表示有效利用了结构性规则，解决了现有多线参数化的冗余问题，为视觉SLAM等任务提供了更高效的几何约束处理方案。

Abstract: Minimal parametrization of 3D lines plays a critical role in camera
localization and structural mapping. Existing representations in robotics and
computer vision predominantly handle independent lines, overlooking structural
regularities such as sets of parallel lines that are pervasive in man-made
environments. This paper introduces \textbf{RiemanLine}, a unified minimal
representation for 3D lines formulated on Riemannian manifolds that jointly
accommodates both individual lines and parallel-line groups. Our key idea is to
decouple each line landmark into global and local components: a shared
vanishing direction optimized on the unit sphere $\mathcal{S}^2$, and scaled
normal vectors constrained on orthogonal subspaces, enabling compact encoding
of structural regularities. For $n$ parallel lines, the proposed representation
reduces the parameter space from $4n$ (orthonormal form) to $2n+2$, naturally
embedding parallelism without explicit constraints. We further integrate this
parameterization into a factor graph framework, allowing global direction
alignment and local reprojection optimization within a unified manifold-based
bundle adjustment. Extensive experiments on ICL-NUIM, TartanAir, and synthetic
benchmarks demonstrate that our method achieves significantly more accurate
pose estimation and line reconstruction, while reducing parameter
dimensionality and improving convergence stability.

</details>


### [20] [Deep learning framework for crater detection and identification on the Moon and Mars](https://arxiv.org/abs/2508.03920)
*Yihan Ma,Zeyang Yu,Rohitash Chandra*

Main category: cs.CV

TL;DR: 本文开发了一个基于卷积神经网络的双阶段框架，用于自动识别和定位火星与月球上的陨石坑。该框架在第一阶段利用经典CNN、ResNet-50和YOLO模型进行陨石坑识别，在第二阶段通过YOLO模型进行陨石坑定位。结果表明，YOLO模型在整体检测性能上表现最均衡，而ResNet-50在识别大型陨石坑时具有高精度优势。


<details>
  <summary>Details</summary>
Motivation: 陨石坑作为行星表面的重要地貌特征，其空间分布与形貌特征对研究行星表面成分、地质历史及撞击过程具有关键意义。近年来深度学习模型的发展为自动化陨石坑检测提供了新的技术途径，因此本研究旨在应用先进深度学习模型提升陨石坑检测识别效能。

Method: 1. 构建双阶段检测框架：第一阶段采用经典CNN、ResNet-50和YOLO模型进行陨石坑特征识别；第二阶段基于YOLO模型实现陨石坑精确定位。2. 使用火星与月球选定区域的遥感数据作为实验样本。3. 通过对比各模型在不同尺寸陨石坑上的识别精度评估性能。

Result: 1. YOLO模型在整体陨石坑检测任务中表现最均衡（兼顾精度与召回率）；2. ResNet-50对大型陨石坑的识别精度最高（尤其适用于直径>1km的陨石坑）；3. 框架成功生成了选定区域的陨石坑类型分布报告。

Conclusion: 双阶段深度学习框架能有效整合不同模型的优势：YOLO适用于通用检测场景，而ResNet模型在特定尺度陨石坑识别中具有精度优势。该方法为行星地质研究提供了可靠的自动化陨石坑分析工具。未来可考虑引入Transformer等新型架构以提升复杂地貌的识别能力。

Abstract: Impact craters are among the most prominent geomorphological features on
planetary surfaces and are of substantial significance in planetary science
research. Their spatial distribution and morphological characteristics provide
critical information on planetary surface composition, geological history, and
impact processes. In recent years, the rapid advancement of deep learning
models has fostered significant interest in automated crater detection. In this
paper, we apply advancements in deep learning models for impact crater
detection and identification. We use novel models, including Convolutional
Neural Networks (CNNs) and variants such as YOLO and ResNet. We present a
framework that features a two-stage approach where the first stage features
crater identification using simple classic CNN, ResNet-50 and YOLO. In the
second stage, our framework employs YOLO-based detection for crater
localisation. Therefore, we detect and identify different types of craters and
present a summary report with remote sensing data for a selected region. We
consider selected regions for craters and identification from Mars and the Moon
based on remote sensing data. Our results indicate that YOLO demonstrates the
most balanced crater detection performance, while ResNet-50 excels in
identifying large craters with high precision.

</details>


### [21] [OmniDepth: Bridging Monocular and Stereo Reasoning with Latent Alignment](https://arxiv.org/abs/2508.04611)
*Tongfan Guan,Jiaxin Guo,Chen Wang,Yun-Hui Liu*

Main category: cs.CV

TL;DR: OmniDepth提出了一种统一的框架，通过迭代双向对齐单目和双目深度估计的潜在表示，结合两者的优势。


<details>
  <summary>Details</summary>
Motivation: 单目方法能够捕捉丰富的上下文先验但缺乏几何精度，而双目方法利用极线几何却在反射或无纹理表面等模糊区域表现不佳。现有方法未能有效结合两者的优势。

Method: OmniDepth采用新颖的交叉注意力对齐机制，在双目推理过程中动态同步单目上下文线索与双目假设表示。通过双向对齐，在解决双目模糊性的同时利用立体几何优化单目深度，整个流程在一个网络中完成。

Result: 实验表明，OmniDepth在Middlebury和ETH3D数据集上实现了零样本泛化错误率降低40%以上，且在透明和反射表面的长期挑战上取得显著改进。

Conclusion: 通过统一多视图几何与单目上下文，OmniDepth克服了单一模态的局限性，实现了鲁棒的3D感知。

Abstract: Monocular and stereo depth estimation offer complementary strengths:
monocular methods capture rich contextual priors but lack geometric precision,
while stereo approaches leverage epipolar geometry yet struggle with
ambiguities such as reflective or textureless surfaces. Despite post-hoc
synergies, these paradigms remain largely disjoint in practice. We introduce
OmniDepth, a unified framework that bridges both through iterative
bidirectional alignment of their latent representations. At its core, a novel
cross-attentive alignment mechanism dynamically synchronizes monocular
contextual cues with stereo hypothesis representations during stereo reasoning.
This mutual alignment resolves stereo ambiguities (e.g., specular surfaces) by
injecting monocular structure priors while refining monocular depth with stereo
geometry within a single network. Extensive experiments demonstrate
state-of-the-art results: \textbf{OmniDepth reduces zero-shot generalization
error by $\!>\!40\%$ on Middlebury and ETH3D}, while addressing longstanding
failures on transparent and reflective surfaces. By harmonizing multi-view
geometry with monocular context, OmniDepth enables robust 3D perception that
transcends modality-specific limitations. Codes available at
https://github.com/aeolusguan/OmniDepth.

</details>


### [22] [Point-Based Shape Representation Generation with a Correspondence-Preserving Diffusion Model](https://arxiv.org/abs/2508.03925)
*Shen Zhu,Yinzhu Jin,Ifrah Zawar,P. Thomas Fletcher*

Main category: cs.CV

TL;DR: 本文提出了一种扩散模型，用于生成具有点对应关系的点基形状表示。该模型能够在生成形状时保留训练数据中的点对应关系，并通过实验证明其在生成海马形状表示上的有效性。


<details>
  <summary>Details</summary>
Motivation: 传统的统计形状模型广泛考虑了点的对应关系，但当前的深度学习方法专注于无序点云，而忽视了点的对应性。这导致现有方法无法生成具有点对应关系的形状。本文旨在解决这一问题。

Method: 1. 设计了一个扩散模型，用于生成具有点对应关系的点基形状。
2. 使用来自OASIS-3数据库的具有点对应关系的形状表示作为训练数据。

Result: 1. 该模型在生成海马形状表示上表现出高真实性，优于现有方法。
2. 通过下游任务验证模型应用：
   a) 条件生成健康与阿尔茨海默病（AD）受试者的形状。
   b) 通过反事实生成预测疾病进展的形态变化。

Conclusion: 本文提出的模型能够生成具有点对应关系的真实点基形状，并在生成质量和应用潜力上均优于现有方法，为基于点对应关系的生成任务提供了有力工具。

Abstract: We propose a diffusion model designed to generate point-based shape
representations with correspondences. Traditional statistical shape models have
considered point correspondences extensively, but current deep learning methods
do not take them into account, focusing on unordered point clouds instead.
Current deep generative models for point clouds do not address generating
shapes with point correspondences between generated shapes. This work aims to
formulate a diffusion model that is capable of generating realistic point-based
shape representations, which preserve point correspondences that are present in
the training data. Using shape representation data with correspondences derived
from Open Access Series of Imaging Studies 3 (OASIS-3), we demonstrate that our
correspondence-preserving model effectively generates point-based hippocampal
shape representations that are highly realistic compared to existing methods.
We further demonstrate the applications of our generative model by downstream
tasks, such as conditional generation of healthy and AD subjects and predicting
morphological changes of disease progression by counterfactual generation.

</details>


### [23] [Policy to Assist Iteratively Local Segmentation: Optimising Modality and Location Selection for Prostate Cancer Localisation](https://arxiv.org/abs/2508.03953)
*Xiangcen Wu,Shaheer U. Saeed,Yipei Wang,Ester Bonmati Coll,Yipeng Hu*

Main category: cs.CV

TL;DR: 本文提出了一种基于策略网络的推荐系统，通过动态选择最佳成像模态和关键图像区域来优化前列腺癌分割任务。该系统模仿放射科医生的工作流程，迭代地分析和分割肿瘤区域，实验表明其在1325张多参数MRI数据集上超越了标准分割网络，并可能为放射科医生提供交互式辅助。


<details>
  <summary>Details</summary>
Motivation: 放射科医生在阅读医学图像时会混合使用多种策略（如单模态/多模态分析、局部区域检查），但现有机器学习模型通常无法灵活地整合这些策略。因此，作者希望开发一个能动态选择最佳检查模态和区域的系统，通过模仿医生流程提升分割性能，尤其针对复杂病理。

Method: 1. 训练策略网络：根据当前分割结果动态推荐最优成像模态（如T2加权、ADC等）和需重点关注的图像区域；
2. 预训练分割网络：负责处理策略网络选定的模态组合和区域，输出局部肿瘤分割结果；
3. 迭代优化：以当前分割结果为输入，策略网络再次推荐新模态/区域供分割网络分析，反复执行直至完成全图分割；
4. 端到端训练：策略网络通过强化学习或梯度反向传播进行训练，目标是最大化最终分割精度。

Result: 1. 在1325例前列腺癌患者的多参数MRI数据上，该方法显著提升了肿瘤分割精度（尤其对复杂病例），超越标准端到端分割模型；
2. 策略网络自发学习到与临床指南（如PI-RADS）相似或全新的检查策略，表明其具有发现更优诊断路径的潜力；
3. 通过减少冗余图像分析区域，提升了标注效率。

Conclusion: 该研究首次将动态决策策略引入医学图像分割，其核心贡献在于：1）证明策略驱动迭代分割对复杂任务的优越性；2）揭示AI模型可能自主发现超越人类指南的诊断逻辑；3）为构建人机协作的智能影像分析系统提供了可行框架。未来可拓展至多病种、多中心验证。

Abstract: Radiologists often mix medical image reading strategies, including inspection
of individual modalities and local image regions, using information at
different locations from different images independently as well as
concurrently. In this paper, we propose a recommend system to assist machine
learning-based segmentation models, by suggesting appropriate image portions
along with the best modality, such that prostate cancer segmentation
performance can be maximised. Our approach trains a policy network that assists
tumor localisation, by recommending both the optimal imaging modality and the
specific sections of interest for review. During training, a pre-trained
segmentation network mimics radiologist inspection on individual or variable
combinations of these imaging modalities and their sections - selected by the
policy network. Taking the locally segmented regions as an input for the next
step, this dynamic decision making process iterates until all cancers are best
localised. We validate our method using a data set of 1325 labelled
multiparametric MRI images from prostate cancer patients, demonstrating its
potential to improve annotation efficiency and segmentation accuracy,
especially when challenging pathology is present. Experimental results show
that our approach can surpass standard segmentation networks. Perhaps more
interestingly, our trained agent independently developed its own optimal
strategy, which may or may not be consistent with current radiologist
guidelines such as PI-RADS. This observation also suggests a promising
interactive application, in which the proposed policy networks assist human
radiologists.

</details>


### [24] [Scaling Up Audio-Synchronized Visual Animation: An Efficient Training Paradigm](https://arxiv.org/abs/2508.03955)
*Lin Zhang,Zefan Cai,Yufan Zhou,Shentong Mo,Jinhong Lin,Cheng-En Wu,Yibing Wei,Yijing Zhang,Ruiyi Zhang,Wen Xiao,Tong Sun,Junjie Hu,Pedro Morgado*

Main category: cs.CV

TL;DR: 提出了一种新的两步训练范式，利用大量有噪声的视频扩展音频同步视觉动画，大幅降低对高质量人工标注视频的依赖，并在多类基准上实现良好泛化。


<details>
  <summary>Details</summary>
Motivation: 现有音频同步视觉动画方法过于依赖特定类别的高质量标注视频，导致难以扩展到开放世界中的多样音频-视频类别。

Method: 分成两个训练阶段：1）利用自动筛选的大型有噪声视频进行预训练，学习多样但不完美的音视频对齐；2）在少量手工精选的高质量样本上微调。通过多特征条件与窗口注意力机制增强同步效果，并基于预训练文本-视频生成器和音频编码器，仅引入1.9%可训练参数学会音频条件能力。

Result: 在自建的AVSync48基准（48个类别，多样性为先前3倍）上验证，该方法将人工标注需求降低10倍以上，同时能泛化到许多开放类别。

Conclusion: 该训练范式大幅减少对人工标注视频的依赖，通过使用丰富但带噪声的数据预训练结合少量高质量数据微调，高效扩展了音频同步动画的类别覆盖范围。

Abstract: Recent advances in audio-synchronized visual animation enable control of
video content using audios from specific classes. However, existing methods
rely heavily on expensive manual curation of high-quality, class-specific
training videos, posing challenges to scaling up to diverse audio-video classes
in the open world. In this work, we propose an efficient two-stage training
paradigm to scale up audio-synchronized visual animation using abundant but
noisy videos. In stage one, we automatically curate large-scale videos for
pretraining, allowing the model to learn diverse but imperfect audio-video
alignments. In stage two, we finetune the model on manually curated
high-quality examples, but only at a small scale, significantly reducing the
required human effort. We further enhance synchronization by allowing each
frame to access rich audio context via multi-feature conditioning and window
attention. To efficiently train the model, we leverage pretrained text-to-video
generator and audio encoders, introducing only 1.9\% additional trainable
parameters to learn audio-conditioning capability without compromising the
generator's prior knowledge. For evaluation, we introduce AVSync48, a benchmark
with videos from 48 classes, which is 3$\times$ more diverse than previous
benchmarks. Extensive experiments show that our method significantly reduces
reliance on manual curation by over 10$\times$, while generalizing to many open
classes.

</details>


### [25] [RAVID: Retrieval-Augmented Visual Detection: A Knowledge-Driven Approach for AI-Generated Image Identification](https://arxiv.org/abs/2508.03967)
*Mamadou Keita,Wassim Hamidouche,Hessen Bougueffa Eutamene,Abdelmalik Taleb-Ahmed,Abdenour Hadid*

Main category: cs.CV

TL;DR: RAVID是一个利用视觉检索增强生成（RAG）的AI生成图像检测框架，首次在视觉领域应用RAG以提升检测性能。该方法通过动态检索相关图像增强输入，结合微调的RAVID CLIP编码器和视觉语言模型（VLM），在UniversalFakeDetect基准测试中达到93.85%的平均准确率，且在图像退化条件下鲁棒性优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有AI生成图像检测方法主要依赖低级伪影和模型特定特征，导致泛化性和鲁棒性不足；而传统RAG方法集中于文本领域，未充分利用视觉知识。RAVID旨在弥补这一差距，首次将检索增强技术应用于视觉内容验证。

Method: 1. 使用类别相关提示微调CLIP图像编码器（RAVID CLIP）提升表征学习；2. 给定查询图像，通过RAVID CLIP生成嵌入向量并检索相关图像；3. 将查询图像与检索结果融合，输入视觉语言模型（如Qwen-VL或Openflamingo）进行分类，整个流程实现端到端检测。

Result: 在覆盖19种生成模型的UniversalFakeDetect基准测试中，RAVID以93.85%平均准确率刷新SOTA。在鲁棒性测试中，对高斯模糊和JPEG压缩的退化图像仍保持80.27%平均准确率，显著优于当前最佳模型C2P-CLIP（63.44%）。

Conclusion: RAVID通过检索增强策略有效解决了生成图像检测的泛化和鲁棒性问题，首次验证了视觉RAG在内容验证任务的价值，为抵御不断进化的AI生成技术提供了新方向。

Abstract: In this paper, we introduce RAVID, the first framework for AI-generated image
detection that leverages visual retrieval-augmented generation (RAG). While RAG
methods have shown promise in mitigating factual inaccuracies in foundation
models, they have primarily focused on text, leaving visual knowledge
underexplored. Meanwhile, existing detection methods, which struggle with
generalization and robustness, often rely on low-level artifacts and
model-specific features, limiting their adaptability. To address this, RAVID
dynamically retrieves relevant images to enhance detection. Our approach
utilizes a fine-tuned CLIP image encoder, RAVID CLIP, enhanced with
category-related prompts to improve representation learning. We further
integrate a vision-language model (VLM) to fuse retrieved images with the
query, enriching the input and improving accuracy. Given a query image, RAVID
generates an embedding using RAVID CLIP, retrieves the most relevant images
from a database, and combines these with the query image to form an enriched
input for a VLM (e.g., Qwen-VL or Openflamingo). Experiments on the
UniversalFakeDetect benchmark, which covers 19 generative models, show that
RAVID achieves state-of-the-art performance with an average accuracy of 93.85%.
RAVID also outperforms traditional methods in terms of robustness, maintaining
high accuracy even under image degradations such as Gaussian blur and JPEG
compression. Specifically, RAVID achieves an average accuracy of 80.27% under
degradation conditions, compared to 63.44% for the state-of-the-art model
C2P-CLIP, demonstrating consistent improvements in both Gaussian blur and JPEG
compression scenarios. The code will be publicly available upon acceptance.

</details>


### [26] [Investigating the Impact of Large-Scale Pre-training on Nutritional Content Estimation from 2D Images](https://arxiv.org/abs/2508.03996)
*Michele Andrade,Guilherme A. L. Silva,Valéria Santos,Gladston Moreira,Eduardo Luz*

Main category: cs.CV

TL;DR: 本文研究了大型预训练数据集对仅使用2D图像进行营养估计的深度学习模型性能的影响。通过在Nutrition5k数据集上进行广泛实验，发现基于JFT-300M（专有数据集）预训练的模型显著优于基于公开数据集（ImageNet、COYO）的模型。值得注意的是，尽管COYO数据集规模庞大，但其预训练模型在营养估计任务上的表现甚至不如ImageNet预训练模型，这挑战了预训练数据规模越大越好的假设。研究强调了预训练数据集特性（规模、领域相关性和质量）对迁移学习效果的关键作用。


<details>
  <summary>Details</summary>
Motivation: 当前通过2D图像估算食物营养成分的方法面临两大挑战：一是仅凭2D图像难以准确估计食物的体积和质量（缺乏深度信息）；二是现有最优方法大多依赖专有数据集进行大规模预训练，导致研究可复现性受阻。为解决上述问题，本文旨在系统评估公开可用的大型预训练数据集对营养估计任务的有效性，为社区提供可复现的基准。

Method: 1) 实验设计：在统一实验框架下，对比不同预训练数据源（ImageNet公开集、COYO公开集）与专有集（JFT-300M）对营养估计模型性能的影响。
2) 模型架构：选用Vision Transformer (ViT)作为主干网络，并以CNN基线模型（InceptionV2、ResNet-50）和基于JFT-300M的SOTA方法作为参照。
3) 实施流程：在Nutrition5k数据集（含高精度营养标注的真实食物图像）上对所有模型进行端到端微调。
4) 评估指标：采用平均绝对误差（MAE）和平均绝对百分比误差（MAE%）进行量化评估。

Result: 1) 性能差异显著：JFT-300M预训练模型的MAE/MAE%值全面优于公开数据集预训练模型（具体数值需参考论文表格）。
2) 反直觉发现：尽管COYO数据集（7亿图像）规模大于ImageNet（1400万图像），但COYO预训练ViT的表现反而更差（MAE高出约5.4%），推翻了"更大规模数据必然带来更好效果"的假设。
3) 关键启示：预训练数据集的领域相关性（如是否包含食物图像）和标注质量比单纯的数据规模更重要。
4) 架构影响：ViT整体优于CNN基线，但预训练数据源的选择比架构差异影响更大。

Conclusion: 1) 即使使用先进的ViT架构，预训练数据源的特性（领域适配性、标注质量）对营养估计性能起决定性作用。
2) 当前公开可用数据集尚不足以支撑达到专有数据集（JFT-300M）水平的营养估计精度，这为数据收集策略提供了重要参考。
3) 该研究揭示了现有公开数据集的局限性，呼吁社区发展更高质量、与任务相关的开放预训练资源以推动公平比较。

Abstract: Estimating the nutritional content of food from images is a critical task
with significant implications for health and dietary monitoring. This is
challenging, especially when relying solely on 2D images, due to the
variability in food presentation, lighting, and the inherent difficulty in
inferring volume and mass without depth information. Furthermore,
reproducibility in this domain is hampered by the reliance of state-of-the-art
methods on proprietary datasets for large-scale pre-training. In this paper, we
investigate the impact of large-scale pre-training datasets on the performance
of deep learning models for nutritional estimation using only 2D images. We
fine-tune and evaluate Vision Transformer (ViT) models pre-trained on two large
public datasets, ImageNet and COYO, comparing their performance against
baseline CNN models (InceptionV2 and ResNet-50) and a state-of-the-art method
pre-trained on the proprietary JFT-300M dataset. We conduct extensive
experiments on the Nutrition5k dataset, a large-scale collection of real-world
food plates with high-precision nutritional annotations. Our evaluation using
Mean Absolute Error (MAE) and Mean Absolute Percentage Error (MAE%) reveals
that models pre-trained on JFT-300M significantly outperform those pre-trained
on public datasets. Unexpectedly, the model pre-trained on the massive COYO
dataset performs worse than the model pre-trained on ImageNet for this specific
regression task, refuting our initial hypothesis. Our analysis provides
quantitative evidence highlighting the critical role of pre-training dataset
characteristics, including scale, domain relevance, and curation quality, for
effective transfer learning in 2D nutritional estimation.

</details>


### [27] [JanusNet: Hierarchical Slice-Block Shuffle and Displacement for Semi-Supervised 3D Multi-Organ Segmentation](https://arxiv.org/abs/2508.03997)
*Zheng Zhang,Tianzhuzi Tan,Guanchun Yin,Bo Zhang,Xiuzhuang Zhou*

Main category: cs.CV

TL;DR: JanusNet是一个用于3D医学图像分割的数据增强框架，它通过全局建模解剖连续性和局部关注难分割区域，解决了现有数据增强方法破坏解剖连续性和对小器官等区域训练不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的数据增强方法（如随机混合体块）破坏了3D医学图像的解剖连续性，导致结构不一致和在困难区域（如小器官）训练不足。为了更好利用人体解剖信息，需要一种在全局和局部都优化的增强方法。

Method: JanusNet包含两个步骤：1) 切片块对齐混洗（Slice-Block Shuffle）：沿随机轴对相同索引的切片块进行对齐混洗，同时保留垂直轴解剖平面结构。2) 置信度引导替换（Confidence-Guided Displacement）：利用预测置信度替换每个切片内的块，强化困难区域的信号。该框架是即插即用的，适用于师生框架。

Result: 在Synapse和AMOS数据集上的实验显示，JanusNet显著优于现有方法。例如，在Synapse数据集上使用仅20%标注数据，实现了4%的DSC（Dice相似系数）提升。

Conclusion: JanusNet通过兼顾全局解剖结构和局部困难区域，有效提升了弱监督3D医学图像分割性能，且易于集成到现有框架中。

Abstract: Limited by the scarcity of training samples and annotations, weakly
supervised medical image segmentation often employs data augmentation to
increase data diversity, while randomly mixing volumetric blocks has
demonstrated strong performance. However, this approach disrupts the inherent
anatomical continuity of 3D medical images along orthogonal axes, leading to
severe structural inconsistencies and insufficient training in challenging
regions, such as small-sized organs, etc. To better comply with and utilize
human anatomical information, we propose JanusNet}, a data augmentation
framework for 3D medical data that globally models anatomical continuity while
locally focusing on hard-to-segment regions. Specifically, our Slice-Block
Shuffle step performs aligned shuffling of same-index slice blocks across
volumes along a random axis, while preserving the anatomical context on planes
perpendicular to the perturbation axis. Concurrently, the Confidence-Guided
Displacement step uses prediction reliability to replace blocks within each
slice, amplifying signals from difficult areas. This dual-stage, axis-aligned
framework is plug-and-play, requiring minimal code changes for most
teacher-student schemes. Extensive experiments on the Synapse and AMOS datasets
demonstrate that JanusNet significantly surpasses state-of-the-art methods,
achieving, for instance, a 4% DSC gain on the Synapse dataset with only 20%
labeled data.

</details>


### [28] [CAD-Judge: Toward Efficient Morphological Grading and Verification for Text-to-CAD Generation](https://arxiv.org/abs/2508.04002)
*Zheyuan Zhou,Jiayi Han,Liang Du,Naiyu Fang,Lemiao Qiu,Shuyou Zhang*

Main category: cs.CV

TL;DR: 提出CAD-Judge，一种可验证的奖励系统，用于高效准确地进行CAD偏好评分和语法验证，通过Compiler-as-a-Judge模块和Compiler-as-a-Review模块优化文本到CAD的流程，并在挑战性数据集上实现最先进性能。


<details>
  <summary>Details</summary>
Motivation: 传统的文本到CAD系统在渲染CAD模型时速度慢，同时使用视觉语言模型（VLM）进行评审成本高且可能导致奖励攻击问题。因此需要一种高效、可靠且成本低的方法来评估和提升CAD模型的生成质量。

Method: 1. 引入Compiler-as-a-Judge（CJM）模块，作为快速直接的奖励信号，通过前景理论最大化生成效用，优化模型对齐。2. 采用一种简单而有效的Agentic CAD生成方法。3. 引入Compiler-as-a-Review（CRM）模块，高效验证生成的CAD模型并允许系统进行相应的改进。

Result: 在多个挑战性CAD数据集上的大量实验表明，该方法在保持高效率的同时，达到了最先进的性能水平。

Conclusion: CAD-Judge系统通过可验证的奖励机制和编译器模块的运用，有效解决了文本到CAD系统中存在的效率低下和奖励攻击问题，为CAD生成提供了更可靠和高效的解决方案。

Abstract: Computer-Aided Design (CAD) models are widely used across industrial design,
simulation, and manufacturing processes. Text-to-CAD systems aim to generate
editable, general-purpose CAD models from textual descriptions, significantly
reducing the complexity and entry barrier associated with traditional CAD
workflows. However, rendering CAD models can be slow, and deploying VLMs to
review CAD models can be expensive and may introduce reward hacking that
degrades the systems. To address these challenges, we propose CAD-Judge, a
novel, verifiable reward system for efficient and effective CAD preference
grading and grammatical validation. We adopt the Compiler-as-a-Judge Module
(CJM) as a fast, direct reward signal, optimizing model alignment by maximizing
generative utility through prospect theory. To further improve the robustness
of Text-to-CAD in the testing phase, we introduce a simple yet effective
agentic CAD generation approach and adopt the Compiler-as-a-Review Module
(CRM), which efficiently verifies the generated CAD models, enabling the system
to refine them accordingly. Extensive experiments on challenging CAD datasets
demonstrate that our method achieves state-of-the-art performance while
maintaining superior efficiency.

</details>


### [29] [$\text{S}^2$Q-VDiT: Accurate Quantized Video Diffusion Transformer with Salient Data and Sparse Token Distillation](https://arxiv.org/abs/2508.04016)
*Weilun Feng,Haotong Qin,Chuanguang Yang,Xiangqi Li,Han Yang,Yuqi Li,Zhulin An,Libo Huang,Michele Magno,Yongjun Xu*

Main category: cs.CV

TL;DR: 为解决视频扩散模型（V-DMs）量化过程中因长token序列导致的高校准方差和学习挑战，提出一个即插即用的后训练量化框架S²Q-VDiT，通过显著数据选择和稀疏token蒸馏提升量化效果，实现高性能、模型压缩与推理加速。


<details>
  <summary>Details</summary>
Motivation: 视频扩散模型参数庞大（高达数十亿），带来计算和内存负担。量化虽可解决，但V-DMs因其时空联合建模导致token序列极长，引入高校准方差和学习挑战。需要解决量化过程中数据校准低效与长序列优化难题。

Method: (a) 提出Hessian感知显著数据选择：结合V-DMs独有的扩散特性和量化需求构建高质量校准数据集；(b) 提出注意力引导的稀疏token蒸馏：利用token注意力分布，对模型输出影响更大的token施加更强的蒸馏权重。在W4A6量化配置下应用该框架。

Result: 实现无损性能下3.9倍模型压缩+1.3倍推理加速；代码即将开源。

Conclusion: S²Q-VDiT有效解决V-DMs量化时的长序列难题，Hessian敏感数据选择与注意力稀疏蒸馏为视频量化领域提供新思路。

Abstract: Diffusion transformers have emerged as the mainstream paradigm for video
generation models. However, the use of up to billions of parameters incurs
significant computational costs. Quantization offers a promising solution by
reducing memory usage and accelerating inference. Nonetheless, we observe that
the joint modeling of spatial and temporal information in video diffusion
models (V-DMs) leads to extremely long token sequences, which introduces high
calibration variance and learning challenges. To address these issues, we
propose \textbf{$\text{S}^2$Q-VDiT}, a post-training quantization framework for
V-DMs that leverages \textbf{S}alient data and \textbf{S}parse token
distillation. During the calibration phase, we identify that quantization
performance is highly sensitive to the choice of calibration data. To mitigate
this, we introduce \textit{Hessian-aware Salient Data Selection}, which
constructs high-quality calibration datasets by considering both diffusion and
quantization characteristics unique to V-DMs. To tackle the learning
challenges, we further analyze the sparse attention patterns inherent in V-DMs.
Based on this observation, we propose \textit{Attention-guided Sparse Token
Distillation}, which exploits token-wise attention distributions to emphasize
tokens that are more influential to the model's output. Under W4A6
quantization, $\text{S}^2$Q-VDiT achieves lossless performance while delivering
$3.9\times$ model compression and $1.3\times$ inference acceleration. Code will
be available at
\href{https://github.com/wlfeng0509/s2q-vdit}{https://github.com/wlfeng0509/s2q-vdit}.

</details>


### [30] [Can Large Multimodal Models Actively Recognize Faulty Inputs? A Systematic Evaluation Framework of Their Input Scrutiny Ability](https://arxiv.org/abs/2508.04017)
*Haiqi Yang,Jinzhe Li,Gengxu Li,Yi Chang,Yuan Wu*

Main category: cs.CV

TL;DR: 该论文介绍了评估多模态模型检测输入错误的框架ISEval，揭示了现有模型在无指导时识别错误的能力不足，尤其是在表面语言错误和条件性缺陷方面，并强调了改进模型主动验证输入的需求。


<details>
  <summary>Details</summary>
Motivation: 尽管大型多模态模型（LMMs）在复杂任务上表现出色，但现有研究主要关注模型被动接受错误输入的问题，忽略了模型能否主动检测错误输入。本研究旨在填补这一空白，评估LMMs在识别有缺陷前提方面的能力。

Method: 研究团队设计了输入审查能力评估框架(ISEval)，该框架包含七类有缺陷前提（如逻辑错误、语言错误、条件性缺陷等）以及三项评估指标。通过对十个先进LMMs进行广泛测试，包括提示模型在不依赖引导的情况下识别输入中的错误。测试中特别关注了模型在文本前提、视觉信息冲突等场景下的表现。

Result: 1. 多数模型无法在无引导下主动检测有缺陷的文本前提，严重依赖显式提示来识别错误。
2. 模型识别能力因错误类型而异：模型在识别逻辑谬误时表现较好，但难以捕捉表面语言错误（如拼写、语法）和某些条件性缺陷（如违反常识的条件）。
3. 模型对模态信息的信任度差异显著：Gemini 2.5 pro 和 Claude Sonnet 4 能平衡视觉与文本信息处理；而aya-vision-8b在模态冲突时过度依赖文字，忽略视觉证据。

Conclusion: 该研究揭示了当前LMMs在主动验证输入有效性方面的显著局限，特别是对语言细节和隐含条件缺陷的敏感性不足。结果表明需要开发更鲁棒的机制来提升模型对输入错误的自我审查能力，避免基于无效前提的推理。同时，框架ISEval为后续研究提供了标准化评测工具。

Abstract: Large Multimodal Models (LMMs) have witnessed remarkable growth, showcasing
formidable capabilities in handling intricate multimodal tasks with exceptional
performance. Recent research has underscored the inclination of large language
models to passively accept defective inputs, often resulting in futile
reasoning on invalid prompts. However, the same critical question of whether
LMMs can actively detect and scrutinize erroneous inputs still remains
unexplored. To address this gap, we introduce the Input Scrutiny Ability
Evaluation Framework (ISEval), which encompasses seven categories of flawed
premises and three evaluation metrics. Our extensive evaluation of ten advanced
LMMs has identified key findings. Most models struggle to actively detect
flawed textual premises without guidance, which reflects a strong reliance on
explicit prompts for premise error identification. Error type affects
performance: models excel at identifying logical fallacies but struggle with
surface-level linguistic errors and certain conditional flaws. Modality trust
varies-Gemini 2.5 pro and Claude Sonnet 4 balance visual and textual info,
while aya-vision-8b over-rely on text in conflicts. These insights underscore
the urgent need to enhance LMMs' proactive verification of input validity and
shed novel insights into mitigating the problem. The code is available at
https://github.com/MLGroupJLU/LMM_ISEval.

</details>


### [31] [Prototype-Driven Structure Synergy Network for Remote Sensing Images Segmentation](https://arxiv.org/abs/2508.04022)
*Junyi Wang,Jinjiang Li,Guodong Fan,Yakun Ju,Xiang Fang,Alex C. Kot*

Main category: cs.CV

TL;DR: 本文提出了一个原型驱动的结构协同网络PDSSNet，旨在解决遥感图像语义分割中由于类内差异大和类间相似性高导致的地物分割不完整问题。该网络通过三个核心模块（APEM、SSCM、CSAM）协同工作，结合不变类别语义和可变空间结构，提升分割精度。实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 遥感图像分割中，完整获取地物的挑战在于高类内差异（同类地物形态多变）和高类间相似性（不同类别外观相似）。传统方法难以统一类表示和区分相似特征，现有类引导方法则受限于原型粗糙且忽略目标结构信息。因此需设计新方法同时利用类别语义与空间结构信息。

Method: PDSSNet网络框架：
1. 自适应原型提取（APEM）：通过编码真实标签获取无偏的类原型（作为语义基准）。
2. 语义-结构协同（SSCM）：分层次处理——先建立全局语义认知，再用结构信息约束/细化语义表征（例如：先识别树木区域再细化轮廓）。
3. 通道相似性调整（CSAM）：动态调整机制，增强类间可区分特征（如：根据梯度调节特征通道权重，强调区分农田/水域的关键通道）。

Result: 在多个遥感数据集（如Potsdam、Vaihingen等）上验证，PDSSNet在分割完整性（IoU）、边界精度等指标超过现有技术（如DeepLabV3+、SegFormer）1-3%的mIoU。可视化结果亦显示其对复杂场景（如密集建筑群/植被覆盖区域）的分割更完整。

Conclusion: 同时建模类别原型（不变语义）和空间结构（可变细节）是提升分割完整性的关键。PDSSNet通过协同机制统一语义与结构信息，有效缓解了类内差异/类间相似性问题，为遥感图像分析提供更可靠的分割结果。

Abstract: In the semantic segmentation of remote sensing images, acquiring complete
ground objects is critical for achieving precise analysis. However, this task
is severely hindered by two major challenges: high intra-class variance and
high inter-class similarity. Traditional methods often yield incomplete
segmentation results due to their inability to effectively unify class
representations and distinguish between similar features. Even emerging
class-guided approaches are limited by coarse class prototype representations
and a neglect of target structural information.
  Therefore, this paper proposes a Prototype-Driven Structure Synergy Network
(PDSSNet). The design of this network is based on a core concept, a complete
ground object is jointly defined by its invariant class semantics and its
variant spatial structure. To implement this, we have designed three key
modules. First, the Adaptive Prototype Extraction Module (APEM) ensures
semantic accuracy from the source by encoding the ground truth to extract
unbiased class prototypes. Subsequently, the designed Semantic-Structure
Coordination Module (SSCM) follows a hierarchical semantics-first,
structure-second principle. This involves first establishing a global semantic
cognition, then leveraging structural information to constrain and refine the
semantic representation, thereby ensuring the integrity of class information.
Finally, the Channel Similarity Adjustment Module (CSAM) employs a dynamic
step-size adjustment mechanism to focus on discriminative features between
classes.
  Extensive experiments demonstrate that PDSSNet outperforms state-of-the-art
methods. The source code is available at
https://github.com/wangjunyi-1/PDSSNet.

</details>


### [32] [Dual Prompt Learning for Adapting Vision-Language Models to Downstream Image-Text Retrieval](https://arxiv.org/abs/2508.04028)
*Yifan Wang,Tao Wang,Chenwei Tang,Caiyang Yu,Zhengqing Zang,Mengmi Zhang,Shudong Huang,Jiancheng Lv*

Main category: cs.CV

TL;DR: 论文提出了一种名为DCAR的双提示学习框架，结合了联合类别-属性重加权，旨在提升下游图文检索（ITR）任务中CLIP模型的表现。该框架动态调整文本和视觉两个维度的提示向量，增强细粒度表示学习。此外，还构建了一个新的细分类别描述检索数据集（FDRD）用于验证方法有效性，并在其上达到了SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 现有提示学习在图像分类等任务中表现出色，但在下游ITR任务中面临挑战，尤其是在区分细粒度属性和相似子类别方面存在困难。

Method: 1. 属性级别：基于图文互信息关联动态更新属性描述的权重；2. 类别级别：引入多角度负样本和类别匹配加权策略来学习子类别区分。整体框架通过双提示学习联合优化属性和类别特征。

Result: 提出的DCAR方法在自建数据集FDRD（包含1500+细粒度类别和23万图文对）上进行了实验，结果显著超越了现有基线模型，达到了最先进性能。

Conclusion: DCAR框架有效解决了ITR任务中细粒度属性与相似子类别识别的难点，联合优化策略增强了模型表示能力，所构建的FDRD数据集为下游ITR研究提供了新标准。

Abstract: Recently, prompt learning has demonstrated remarkable success in adapting
pre-trained Vision-Language Models (VLMs) to various downstream tasks such as
image classification. However, its application to the downstream Image-Text
Retrieval (ITR) task is more challenging. We find that the challenge lies in
discriminating both fine-grained attributes and similar subcategories of the
downstream data. To address this challenge, we propose Dual prompt Learning
with Joint Category-Attribute Reweighting (DCAR), a novel dual-prompt learning
framework to achieve precise image-text matching. The framework dynamically
adjusts prompt vectors from both semantic and visual dimensions to improve the
performance of CLIP on the downstream ITR task. Based on the prompt paradigm,
DCAR jointly optimizes attribute and class features to enhance fine-grained
representation learning. Specifically, (1) at the attribute level, it
dynamically updates the weights of attribute descriptions based on text-image
mutual information correlation; (2) at the category level, it introduces
negative samples from multiple perspectives with category-matching weighting to
learn subcategory distinctions. To validate our method, we construct the
Fine-class Described Retrieval Dataset (FDRD), which serves as a challenging
benchmark for ITR in downstream data domains. It covers over 1,500 downstream
fine categories and 230,000 image-caption pairs with detailed attribute
annotations. Extensive experiments on FDRD demonstrate that DCAR achieves
state-of-the-art performance over existing baselines.

</details>


### [33] [Radar-Based NLoS Pedestrian Localization for Darting-Out Scenarios Near Parked Vehicles with Camera-Assisted Point Cloud Interpretation](https://arxiv.org/abs/2508.04033)
*Hee-Yeun Kim,Byeonggyu Park,Byonghyok Choi,Hansang Cho,Byungkwan Kim,Soomok Lee,Mingu Jeon,Seung-Woo Seo,Seong-Woo Kim*

Main category: cs.CV

TL;DR: 提出了一种非视距（NLoS）行人定位框架，利用单目相机图像和二维雷达点云（PCD）数据，通过图像分割检测停放车辆并估计深度，再结合雷达PCD精确定位，解决城市道路因路边停车导致的盲区问题。实验表明该方法可提高行人早期检测能力，增强道路安全性。


<details>
  <summary>Details</summary>
Motivation: 城市环境中路边停车形成的非视距盲区对道路安全构成威胁，尤其是行人从停放车辆间突然出现的情况。现有方法依赖预设空间信息或简单墙体反射假设，缺乏实时适应性。由于停放车辆位置动态变化，基于卫星地图等预设信息的方案难以反映实时路况，导致传感器误判。因此，需开发能动态感知障碍物并精确定位NLoS行人的方法。

Method: 1. 用图像分割模型检测停放车辆；2. 通过单目深度估计推测车辆的空间关系；3. 融合二维雷达点云（PCD）数据对空间信息进行精细化校准；4. 实现NLoS行人定位。框架首先利用相机获取环境语义信息并粗略估计深度，再结合雷达的穿透特性获取障碍物后方精确点云，从而构建动态空间模型。

Result: 在真实城市道路环境中进行实验验证，结果表明：1. 较传统方案显著提升盲区行人早期检测率；2. 实现遮挡场景下行人位置的动态精准定位；3. 有效克服预设地图信息滞后性问题，适应实时路况变化。框架部署成本较低（单目相机+2D雷达），具备实用价值。

Conclusion: 通过融合视觉语义理解与雷达穿透感知能力，构建了动态空间推理框架，突破现有NLoS检测方案对预设信息的依赖限制。该方案为复杂城市场景的安全感知提供新思路，核心贡献在于：1. 实现临时空间障碍物（停放车辆）的实时检测与建模；2. 提出视觉-雷达跨模态校准机制；3. 验证系统在真实场景提升道路安全的有效性。未来可扩展至多传感器协同或自动驾驶实时决策系统。

Abstract: The presence of Non-Line-of-Sight (NLoS) blind spots resulting from roadside
parking in urban environments poses a significant challenge to road safety,
particularly due to the sudden emergence of pedestrians. mmWave technology
leverages diffraction and reflection to observe NLoS regions, and recent
studies have demonstrated its potential for detecting obscured objects.
However, existing approaches predominantly rely on predefined spatial
information or assume simple wall reflections, thereby limiting their
generalizability and practical applicability. A particular challenge arises in
scenarios where pedestrians suddenly appear from between parked vehicles, as
these parked vehicles act as temporary spatial obstructions. Furthermore, since
parked vehicles are dynamic and may relocate over time, spatial information
obtained from satellite maps or other predefined sources may not accurately
reflect real-time road conditions, leading to erroneous sensor interpretations.
To address this limitation, we propose an NLoS pedestrian localization
framework that integrates monocular camera image with 2D radar point cloud
(PCD) data. The proposed method initially detects parked vehicles through image
segmentation, estimates depth to infer approximate spatial characteristics, and
subsequently refines this information using 2D radar PCD to achieve precise
spatial inference. Experimental evaluations conducted in real-world urban road
environments demonstrate that the proposed approach enhances early pedestrian
detection and contributes to improved road safety. Supplementary materials are
available at https://hiyeun.github.io/NLoS/.

</details>


### [34] [ForestFormer3D: A Unified Framework for End-to-End Segmentation of Forest LiDAR 3D Point Clouds](https://arxiv.org/abs/2506.16991)
*Binbin Xiang,Maciej Wielgosz,Stefano Puliti,Kamil Král,Martin Krůček,Azim Missarov,Rasmus Astrup*

Main category: cs.CV

TL;DR: 介绍了ForestFormer3D，一个用于森林LiDAR 3D点云中个体树和语义分割的端到端框架，该框架在FOR-instanceV2数据集上取得了最先进的性能，并在未见数据集上表现出良好的泛化能力。数据集和代码已公开。


<details>
  <summary>Details</summary>
Motivation: 当前方法在复杂多变的自然森林环境中处理LiDAR 3D点云分割（包括个体树分割和语义分割）时存在困难。因此，需要一种新的、统一的端到端框架来提高分割精度。

Method: 提出了ForestFormer3D框架，该框架融合了三项创新：1) ISA引导的查询点选择；2) 推理过程中基于分数的块合并策略；3) 训练中采用一对多关联机制。这些组件共同工作，以实现高精度的点云分割。

Result: 1. 在新提出的FOR-instanceV2数据集（涵盖多样森林类型和区域）上，实现了最先进的个体树分割性能。2. 在未见测试集（Wytham woods和LAUTx）上表现出良好的泛化能力，证明了模型在不同森林条件和传感器模式下的鲁棒性。

Conclusion: ForestFormer3D为森林点云分割提供了一个强大且通用的解决方案，其数据集和代码开源促进了相关研究的发展。

Abstract: The segmentation of forest LiDAR 3D point clouds, including both individual
tree and semantic segmentation, is fundamental for advancing forest management
and ecological research. However, current approaches often struggle with the
complexity and variability of natural forest environments. We present
ForestFormer3D, a new unified and end-to-end framework designed for precise
individual tree and semantic segmentation. ForestFormer3D incorporates
ISA-guided query point selection, a score-based block merging strategy during
inference, and a one-to-many association mechanism for effective training. By
combining these new components, our model achieves state-of-the-art performance
for individual tree segmentation on the newly introduced FOR-instanceV2
dataset, which spans diverse forest types and regions. Additionally,
ForestFormer3D generalizes well to unseen test sets (Wytham woods and LAUTx),
showcasing its robustness across different forest conditions and sensor
modalities. The FOR-instanceV2 dataset and the ForestFormer3D code are publicly
available at https://bxiang233.github.io/FF3D/.

</details>


### [35] [CORE-ReID V2: Advancing the Domain Adaptation for Object Re-Identification with Optimized Training and Ensemble Fusion](https://arxiv.org/abs/2508.04036)
*Trinh Quoc Nguyen,Oky Dicky Ardiansyah Prima,Syahid Al Irfan,Hindriyanto Dwi Purnomo,Radius Tanone*

Main category: cs.CV

TL;DR: 本文提出CORE-ReID V2框架，它在CORE-ReID基础上改进，用于解决行人重识别（ReID）和车辆重识别中的无监督域自适应（UDA）问题，并适用于物体重识别。在预训练阶段使用CycleGAN合成多样数据以弥合不同域间的图像特征差异，在微调阶段采用集成融合机制（ECAB和SECAB）增强特征表示并减少伪标签的歧义。实验结果表明该框架在多个数据集上的mAP和Rank-k准确率优于现有方法，且支持轻量级骨干网络。


<details>
  <summary>Details</summary>
Motivation: 当前的UDA方法在ReID任务中面临特征表示不充分以及伪标签歧义的问题，这限制了模型在目标域上的性能。为此，作者提出CORE-ReID V2，通过改进特征融合机制和提高伪标签质量来提升域自适应效果。

Method: 框架分为两个阶段：1. 预训练：使用CycleGAN生成合成数据，减少源域和目标域之间的图像特征差异。2. 微调：设计一个集成融合机制，包括ECAB（高效通道注意力块）和SECAB（简化高效通道注意力块），增强局部和全局特征表示，同时减少目标样本伪标签的歧义。该方法支持轻量级骨干网络（如ResNet18，ResNet34）。

Result: 在多个UDA行人重识别和车辆重识别数据集上，CORE-ReID V2在平均精度均值（mAP）和Rank-k准确率（Top-1, Top-5, Top-10）上均优于现有方法，达到了最先进的性能。

Conclusion: CORE-ReID V2不仅推动了基于UDA的物体重识别的发展，还为后续研究提供了坚实的基础。框架的代码和模型已开源。

Abstract: This study presents CORE-ReID V2, an enhanced framework building upon
CORE-ReID. The new framework extends its predecessor by addressing Unsupervised
Domain Adaptation (UDA) challenges in Person ReID and Vehicle ReID, with
further applicability to Object ReID. During pre-training, CycleGAN is employed
to synthesize diverse data, bridging image characteristic gaps across different
domains. In the fine-tuning, an advanced ensemble fusion mechanism, consisting
of the Efficient Channel Attention Block (ECAB) and the Simplified Efficient
Channel Attention Block (SECAB), enhances both local and global feature
representations while reducing ambiguity in pseudo-labels for target samples.
Experimental results on widely used UDA Person ReID and Vehicle ReID datasets
demonstrate that the proposed framework outperforms state-of-the-art methods,
achieving top performance in Mean Average Precision (mAP) and Rank-k Accuracy
(Top-1, Top-5, Top-10). Moreover, the framework supports lightweight backbones
such as ResNet18 and ResNet34, ensuring both scalability and efficiency. Our
work not only pushes the boundaries of UDA-based Object ReID but also provides
a solid foundation for further research and advancements in this domain. Our
codes and models are available at
https://github.com/TrinhQuocNguyen/CORE-ReID-V2.

</details>


### [36] [SPJFNet: Self-Mining Prior-Guided Joint Frequency Enhancement for Ultra-Efficient Dark Image Restoration](https://arxiv.org/abs/2508.04041)
*Tongshun Zhang,Pingling Liu,Zijian Zhang,Qiuzhan Zhou*

Main category: cs.CV

TL;DR: 本研究提出了一种高效的自挖掘先验引导的联合频率增强网络（SPJFNet），解决了现有暗图像修复方法中的计算瓶颈问题。通过自挖掘指导模块（SMGM）避免了外部先验的依赖和纠错开销，并利用双频指导框架（DFGF）对高低频分别处理，大幅降低了计算复杂度和参数数量，同时提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有暗图像修复方法存在三个主要问题：1）依赖外部先验带来的计算负担和纠错成本；2）复杂多阶段增强流程中的冗余操作；3）频域方法中对所有频率分量不加区分处理导致的高计算需求。这些问题导致现有方法效率低下。

Method: 作者提出了SPJFNet网络，包括三个关键设计：1）自挖掘指导模块（SMGM）：从网络内部生成轻量级的内在指导信息，避免外部先验依赖和纠错开销；2）通过无损小波分解和基于傅里叶的联合优势频率增强，将多级操作链重构压缩为单一高效操作，减少参数；3）双频指导框架（DFGF）：解耦高低频处理，部署专门的高频分支（小波域增强）和低频分支（傅里叶域恢复），显著降低计算复杂度。

Result: 在多个基准测试上进行的严格评估表明，SPJFNet不仅在性能上超越了现有最先进方法，还显著提高了效率，大幅降低了模型复杂度和计算开销。

Conclusion: SPJFNet通过消除外部先验依赖、简化流程并针对性处理高低频分量，有效解决了暗图像修复中的效率瓶颈问题，实现了性能和效率的双重提升。

Abstract: Current dark image restoration methods suffer from severe efficiency
bottlenecks, primarily stemming from: (1) computational burden and error
correction costs associated with reliance on external priors (manual or
cross-modal); (2) redundant operations in complex multi-stage enhancement
pipelines; and (3) indiscriminate processing across frequency components in
frequency-domain methods, leading to excessive global computational demands. To
address these challenges, we propose an Efficient Self-Mining Prior-Guided
Joint Frequency Enhancement Network (SPJFNet). Specifically, we first introduce
a Self-Mining Guidance Module (SMGM) that generates lightweight endogenous
guidance directly from the network, eliminating dependence on external priors
and thereby bypassing error correction overhead while improving inference
speed. Second, through meticulous analysis of different frequency domain
characteristics, we reconstruct and compress multi-level operation chains into
a single efficient operation via lossless wavelet decomposition and joint
Fourier-based advantageous frequency enhancement, significantly reducing
parameters. Building upon this foundation, we propose a Dual-Frequency Guidance
Framework (DFGF) that strategically deploys specialized high/low frequency
branches (wavelet-domain high-frequency enhancement and Fourier-domain
low-frequency restoration), decoupling frequency processing to substantially
reduce computational complexity. Rigorous evaluation across multiple benchmarks
demonstrates that SPJFNet not only surpasses state-of-the-art performance but
also achieves significant efficiency improvements, substantially reducing model
complexity and computational overhead. Code is available at
https://github.com/bywlzts/SPJFNet.

</details>


### [37] [VisualTrans: A Benchmark for Real-World Visual Transformation Reasoning](https://arxiv.org/abs/2508.04043)
*Yuheng Ji,Yipu Wang,Yuyang Liu,Xiaoshuai Hao,Yue Liu,Yuting Zhao,Huaihai Lyu,Xiaolong Zheng*

Main category: cs.CV

TL;DR: VisualTrans基准测试是首个专门为真实世界人机交互场景中的视觉变换推理(VTR)设计的综合基准，包含12个任务，评估空间、过程和数量三个推理维度，通过472个高质量问答对测试模型，发现现有模型在静态空间任务中表现良好，但在动态多步推理场景中存在明显不足。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试存在模拟到现实的差距、任务复杂度有限以及推理覆盖不完整等问题，限制了它们在现实场景的实用性。因此，需要一个新的基准测试来推动VTR研究的发展。

Method: 使用第一人称操作视频构建数据集，流程包括任务选择、图像对提取、利用大型多模态模型自动化元数据标注、结构化问题生成和人工验证。评估了多种先进视觉语言模型在空间、过程和数量推理维度的表现。

Result: 当前最先进的视觉语言模型在静态空间任务中表现强劲，但在动态多步推理场景(如中间状态识别和变换序列规划)中表现出显著缺陷，凸显了现有方法在时间建模和因果推理方面的不足。

Conclusion: VisualTrans为VTR研究提供了首个真实世界交互场景的评估基准，为未来研究指明了方向：需要开发具有更强动态场景建模和因果推理能力的多模态模型。

Abstract: Visual transformation reasoning (VTR) is a vital cognitive capability that
empowers intelligent agents to understand dynamic scenes, model causal
relationships, and predict future states, and thereby guiding actions and
laying the foundation for advanced intelligent systems. However, existing
benchmarks suffer from a sim-to-real gap, limited task complexity, and
incomplete reasoning coverage, limiting their practical use in real-world
scenarios. To address these limitations, we introduce VisualTrans, the first
comprehensive benchmark specifically designed for VTR in real-world
human-object interaction scenarios. VisualTrans encompasses 12 semantically
diverse manipulation tasks and systematically evaluates three essential
reasoning dimensions - spatial, procedural, and quantitative - through 6
well-defined subtask types. The benchmark features 472 high-quality
question-answer pairs in various formats, including multiple-choice, open-ended
counting, and target enumeration. We introduce a scalable data construction
pipeline built upon first-person manipulation videos, which integrates task
selection, image pair extraction, automated metadata annotation with large
multimodal models, and structured question generation. Human verification
ensures the final benchmark is both high-quality and interpretable. Evaluations
of various state-of-the-art vision-language models show strong performance in
static spatial tasks. However, they reveal notable shortcomings in dynamic,
multi-step reasoning scenarios, particularly in areas like intermediate state
recognition and transformation sequence planning. These findings highlight
fundamental weaknesses in temporal modeling and causal reasoning, providing
clear directions for future research aimed at developing more capable and
generalizable VTR systems. The dataset and code are available at
https://github.com/WangYipu2002/VisualTrans.

</details>


### [38] [Iterative pseudo-labeling based adaptive copy-paste supervision for semi-supervised tumor segmentation](https://arxiv.org/abs/2508.04044)
*Qiangguo Jin,Hui Cui,Junbo Wang,Changming Sun,Yimiao He,Ping Xuan,Linlin Wang,Cong Cong,Leyi Wei,Ran Su*

Main category: cs.CV

TL;DR: 论文提出了一种名为IPA-CP的半监督学习方法，用于解决CT扫描中肿瘤分割的挑战，特别是在存在多个小肿瘤的情况下。该方法通过双向不确定性驱动的自适应数据增强和迭代伪标签改进策略，结合平均教师架构，在多个数据集上实现了优于当前先进方法的分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有的半监督学习方法在分割大器官方面成果显著，但在处理肿瘤（特别是数量多或体积小的肿瘤）分割时表现不足。此外，对带标签和无标签数据的数据增强策略潜力尚未充分挖掘。因此，作者旨在开发一种新方法来解决这些挑战。

Method: IPA-CP包含两个关键部分：1) 双向不确定性驱动的自适应增强机制（使用平均教师模型中预测的肿瘤不确定性来指导肿瘤区域的复制粘贴和背景扰动）；2) 迭代伪标签转换策略（在训练过程中迭代修正无标签数据的伪标签以提高其可靠性）。

Result: 在内部和公共数据集上的实验表明，IPA-CP在肿瘤分割任务上显著超过现有最先进半监督学习方法。消融研究验证了所提技术组件的有效性（特别是自适应增强和伪标签迭代策略）。

Conclusion: IPA-CP为小肿瘤和多样化肿瘤分布场景提供了一种有效的半监督解决方案。其核心创新（自适应数据增强及伪标签优化策略）可泛化至其他医学图像任务。

Abstract: Semi-supervised learning (SSL) has attracted considerable attention in
medical image processing. The latest SSL methods use a combination of
consistency regularization and pseudo-labeling to achieve remarkable success.
However, most existing SSL studies focus on segmenting large organs, neglecting
the challenging scenarios where there are numerous tumors or tumors of small
volume. Furthermore, the extensive capabilities of data augmentation
strategies, particularly in the context of both labeled and unlabeled data,
have yet to be thoroughly investigated. To tackle these challenges, we
introduce a straightforward yet effective approach, termed iterative
pseudo-labeling based adaptive copy-paste supervision (IPA-CP), for tumor
segmentation in CT scans. IPA-CP incorporates a two-way uncertainty based
adaptive augmentation mechanism, aiming to inject tumor uncertainties present
in the mean teacher architecture into adaptive augmentation. Additionally,
IPA-CP employs an iterative pseudo-label transition strategy to generate more
robust and informative pseudo labels for the unlabeled samples. Extensive
experiments on both in-house and public datasets show that our framework
outperforms state-of-the-art SSL methods in medical image segmentation.
Ablation study results demonstrate the effectiveness of our technical
contributions.

</details>


### [39] [Motion is the Choreographer: Learning Latent Pose Dynamics for Seamless Sign Language Generation](https://arxiv.org/abs/2508.04049)
*Jiayi He,Xu Wang,Shengeng Tang,Yaxiong Wang,Lechao Cheng,Dan Guo*

Main category: cs.CV

TL;DR: 本文提出了一种新的手语视频生成范式，通过两阶段合成框架将运动语义与身份信息解耦，从而减少数据需求并提高泛化能力。


<details>
  <summary>Details</summary>
Motivation: 手语视频生成需要大量特定身份的数据，且泛化能力差。为了解决这些问题，作者通过解耦运动语义和身份信息，构建一个独立于身份的多模态运动词典，从而实现高效、高质量且可灵活个性化的手语视频生成。

Method: 方法包括两个关键步骤：1) 构建一个独立于身份的多模态运动词典，每个词条存储为与身份无关的姿势、手势和3D网格序列，且每个手势只需一次录制。2) 离散到连续的运动合成阶段：通过检索词典序列生成连贯运动轨迹，随后通过身份感知神经渲染生成任意身份的真实感视频。

Result: 该方案不仅在生成质量上表现优异，还实现了前所未有的身份个性化灵活性。

Conclusion: 通过将运动建模为可移植的“编舞层”，并将其与渲染层解耦，该方法提供了一种高效、灵活的手语视频生成方案，显著降低数据需求并提高泛化能力。

Abstract: Sign language video generation requires producing natural signing motions
with realistic appearances under precise semantic control, yet faces two
critical challenges: excessive signer-specific data requirements and poor
generalization. We propose a new paradigm for sign language video generation
that decouples motion semantics from signer identity through a two-phase
synthesis framework. First, we construct a signer-independent multimodal motion
lexicon, where each gloss is stored as identity-agnostic pose, gesture, and 3D
mesh sequences, requiring only one recording per sign. This compact
representation enables our second key innovation: a discrete-to-continuous
motion synthesis stage that transforms retrieved gloss sequences into
temporally coherent motion trajectories, followed by identity-aware neural
rendering to produce photorealistic videos of arbitrary signers. Unlike prior
work constrained by signer-specific datasets, our method treats motion as a
first-class citizen: the learned latent pose dynamics serve as a portable
"choreography layer" that can be visually realized through different human
appearances. Extensive experiments demonstrate that disentangling motion from
identity is not just viable but advantageous - enabling both high-quality
synthesis and unprecedented flexibility in signer personalization.

</details>


### [40] [DOMR: Establishing Cross-View Segmentation via Dense Object Matching](https://arxiv.org/abs/2508.04050)
*Jitong Liao,Yulu Gao,Shaofei Huang,Jialin Gao,Jie Lei,Ronghua Liang,Si Liu*

Main category: cs.CV

TL;DR: 该论文介绍了DOMR框架，用于解决跨视角（自我中心视角和外部中心视角）的密集目标对应匹配问题。该方法的核心是DOM模块，该模块通过整合视觉、空间和语义信息，并考虑对象间的位置和语义关系，来改进跨视角目标匹配性能。通过结合一个掩码细化头，进一步提升了预测掩码的准确性和完整性。在Ego-Exo4D基准上取得最先进结果。


<details>
  <summary>Details</summary>
Motivation: 解决跨视角（自我中心视角和外部中心视角）目标对应匹配的难题，该任务在视觉理解中至关重要但非常具有挑战性。现有方法在匹配个体目标掩码时效果有限，未充分利用对象之间的关系信息。

Method: 提出密集目标匹配与精炼（DOMR）框架。框架的核心是密集目标匹配器（DOM）模块：通过联合建模多个对象的视觉、空间和语义关系来建立密集目标对应。具体包括：1）提案生成模块生成目标候选；2）密集匹配模块联合编码视觉、空间和语义线索，显式构建对象间关系以实现密集目标匹配。此外，结合一个掩码细化头，以优化预测掩码的完整性和准确性。

Result: 在Ego-Exo4D基准测试上，取得了SOTA结果：Ego->Exo方向mIoU为49.7%，Exo->Ego方向为55.2%，相比之前的最优方法分别提升了5.8%和4.3%。

Conclusion: 实验结果验证了通过整合视觉、空间、语义信息以及对象间关系的DOM模块和掩码细化头的DOMR框架在处理跨视角目标对应匹配任务上的有效性。

Abstract: Cross-view object correspondence involves matching objects between egocentric
(first-person) and exocentric (third-person) views. It is a critical yet
challenging task for visual understanding. In this work, we propose the Dense
Object Matching and Refinement (DOMR) framework to establish dense object
correspondences across views. The framework centers around the Dense Object
Matcher (DOM) module, which jointly models multiple objects. Unlike methods
that directly match individual object masks to image features, DOM leverages
both positional and semantic relationships among objects to find
correspondences. DOM integrates a proposal generation module with a dense
matching module that jointly encodes visual, spatial, and semantic cues,
explicitly constructing inter-object relationships to achieve dense matching
among objects. Furthermore, we combine DOM with a mask refinement head designed
to improve the completeness and accuracy of the predicted masks, forming the
complete DOMR framework. Extensive evaluations on the Ego-Exo4D benchmark
demonstrate that our approach achieves state-of-the-art performance with a mean
IoU of 49.7% on Ego$\to$Exo and 55.2% on Exo$\to$Ego. These results outperform
those of previous methods by 5.8% and 4.3%, respectively, validating the
effectiveness of our integrated approach for cross-view understanding.

</details>


### [41] [Towards Globally Predictable k-Space Interpolation: A White-box Transformer Approach](https://arxiv.org/abs/2508.04051)
*Chen Luo,Qiyu Jin,Taofeng Xie,Xuemei Wang,Huayu Wang,Congcong Liu,Liming Tang,Guoqing Chen,Zhuo-Xu Cui,Dong Liang*

Main category: cs.CV

TL;DR: 提出了一种名为GPI-WT的基于Transformer的磁共振成像（MRI）k空间数据插补框架，该框架结合了结构化低秩（SLR）模型与深度学习，通过全局可预测的插值和自注意力机制提高插值精度与可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有k空间插值方法（包括基于CNN的深度学习）主要利用数据的局部特性而忽略了全局依赖性。Transformers在自然语言和图像处理中处理长程依赖的成功经验启发其用于k空间插值，但缺乏可解释性。本文旨在设计一种可解释的Transformer框架（GPI-WT）以同时解决全局依赖建模和可解释性问题。

Method: 1. 提出全局可预测插值（GPI）理论作为结构化低秩（SLR）模型的新形式，将全局湮灭滤波器参数化；2. 利用SLR模型的次梯度导出可学习的自注意力机制；3. 通过展开SLR优化算法构建可解释的Transformer级联网络（GPI-WT）。

Result: 实验表明，GPI-WT在k空间插值精度上显著优于当前最优方法，同时提供更强的可解释性。

Conclusion: 结合SLR模型与Transformer架构的GPI-WT框架有效利用了k空间全局结构，在保证高精度插值的同时实现了可解释性，为MRI加速提供了新方案。

Abstract: Interpolating missing data in k-space is essential for accelerating imaging.
However, existing methods, including convolutional neural network-based deep
learning, primarily exploit local predictability while overlooking the inherent
global dependencies in k-space. Recently, Transformers have demonstrated
remarkable success in natural language processing and image analysis due to
their ability to capture long-range dependencies. This inspires the use of
Transformers for k-space interpolation to better exploit its global structure.
However, their lack of interpretability raises concerns regarding the
reliability of interpolated data. To address this limitation, we propose
GPI-WT, a white-box Transformer framework based on Globally Predictable
Interpolation (GPI) for k-space. Specifically, we formulate GPI from the
perspective of annihilation as a novel k-space structured low-rank (SLR) model.
The global annihilation filters in the SLR model are treated as learnable
parameters, and the subgradients of the SLR model naturally induce a learnable
attention mechanism. By unfolding the subgradient-based optimization algorithm
of SLR into a cascaded network, we construct the first white-box Transformer
specifically designed for accelerated MRI. Experimental results demonstrate
that the proposed method significantly outperforms state-of-the-art approaches
in k-space interpolation accuracy while providing superior interpretability.

</details>


### [42] [Uni-DocDiff: A Unified Document Restoration Model Based on Diffusion](https://arxiv.org/abs/2508.04055)
*Fangmin Zhao,Weichao Zeng,Zhenhang Li,Dongbao Yang,Binbin Li,Xiaojun Bi,Yu Zhou*

Main category: cs.CV

TL;DR: Uni-DocDiff是一个基于扩散模型的统一且高度可扩展的文档恢复模型，通过可学习的任务提示和先验池机制，有效处理多种文档修复任务，性能媲美甚至超越专门的模型。


<details>
  <summary>Details</summary>
Motivation: 现有文档恢复方法通常为每个任务设计独立模型，导致系统复杂；而现有统一方法受限于人工设计的提示和预处理，未能充分利用任务间的协同效应。

Method: 提出了可学习的任务提示设计确保任务可扩展性；引入了先验池机制（结合局部高频和全局低频特征）及先验融合模块（自适应选择任务相关先验），构建基于扩散的统一模型。

Result: 实验表明Uni-DocDiff在多个任务上性能媲美或超越专用模型，且能无缝适应新任务。

Conclusion: Uni-DocDiff通过统一框架有效解决了文档恢复任务，实现了高扩展性和性能。

Abstract: Removing various degradations from damaged documents greatly benefits
digitization, downstream document analysis, and readability. Previous methods
often treat each restoration task independently with dedicated models, leading
to a cumbersome and highly complex document processing system. Although recent
studies attempt to unify multiple tasks, they often suffer from limited
scalability due to handcrafted prompts and heavy preprocessing, and fail to
fully exploit inter-task synergy within a shared architecture. To address the
aforementioned challenges, we propose Uni-DocDiff, a Unified and highly
scalable Document restoration model based on Diffusion. Uni-DocDiff develops a
learnable task prompt design, ensuring exceptional scalability across diverse
tasks. To further enhance its multi-task capabilities and address potential
task interference, we devise a novel \textbf{Prior \textbf{P}ool}, a simple yet
comprehensive mechanism that combines both local high-frequency features and
global low-frequency features. Additionally, we design the \textbf{Prior
\textbf{F}usion \textbf{M}odule (PFM)}, which enables the model to adaptively
select the most relevant prior information for each specific task. Extensive
experiments show that the versatile Uni-DocDiff achieves performance comparable
or even superior performance compared with task-specific expert models, and
simultaneously holds the task scalability for seamless adaptation to new tasks.

</details>


### [43] [TCSAFormer: Efficient Vision Transformer with Token Compression and Sparse Attention for Medical Image Segmentation](https://arxiv.org/abs/2508.04058)
*Zunhui Xia,Hongxing Li,Libin Lan*

Main category: cs.CV

TL;DR: 提出TCSAFormer网络，通过压缩注意力模块和双分支前馈网络解决Transformer在医学图像分割中的计算复杂度和局部特征捕获不足问题，在多个数据集上实现了高效准确的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于Transformer的医学图像分割方法存在计算复杂度高（随输入序列二次增长）和标准前馈网络（FFN）模块难以有效捕获局部信息和多尺度特征的局限性。为此，需要设计一种高效且能增强局部上下文建模的分割网络。

Method: 1. 压缩注意力（CA）模块：结合令牌压缩和像素级稀疏注意力，为每个查询动态筛选最重要的键值对（先剪枝全局无关令牌，再合并冗余令牌），显著降低计算量并提升关系建模能力；2. 双分支前馈网络（DBFFN）：替换标准FFN，包含双分支结构来同时捕获局部上下文特征和多尺度信息，强化特征表示。使用三个医学图像分割数据集（ISIC-2018, CVC-ClinicDB, Synapse）评估。

Result: 在ISIC-2018、CVC-ClinicDB和Synapse三个公开数据集上的实验表明，TCSAFormer性能优于现有SOTA方法，同时保持较低计算开销，达到效率和精度最优平衡。

Conclusion: TCSAFormer有效解决了Transformer在医学图像分割中的两大核心问题（高计算复杂度和局部特征建模不足），通过压缩注意力机制和双分支FFN设计，实现了高效且高精度的分割性能。

Abstract: In recent years, transformer-based methods have achieved remarkable progress
in medical image segmentation due to their superior ability to capture
long-range dependencies. However, these methods typically suffer from two major
limitations. First, their computational complexity scales quadratically with
the input sequences. Second, the feed-forward network (FFN) modules in vanilla
Transformers typically rely on fully connected layers, which limits models'
ability to capture local contextual information and multiscale features
critical for precise semantic segmentation. To address these issues, we propose
an efficient medical image segmentation network, named TCSAFormer. The proposed
TCSAFormer adopts two key ideas. First, it incorporates a Compressed Attention
(CA) module, which combines token compression and pixel-level sparse attention
to dynamically focus on the most relevant key-value pairs for each query. This
is achieved by pruning globally irrelevant tokens and merging redundant ones,
significantly reducing computational complexity while enhancing the model's
ability to capture relationships between tokens. Second, it introduces a
Dual-Branch Feed-Forward Network (DBFFN) module as a replacement for the
standard FFN to capture local contextual features and multiscale information,
thereby strengthening the model's feature representation capability. We conduct
extensive experiments on three publicly available medical image segmentation
datasets: ISIC-2018, CVC-ClinicDB, and Synapse, to evaluate the segmentation
performance of TCSAFormer. Experimental results demonstrate that TCSAFormer
achieves superior performance compared to existing state-of-the-art (SOTA)
methods, while maintaining lower computational overhead, thus achieving an
optimal trade-off between efficiency and accuracy.

</details>


### [44] [Beyond the Visible: Benchmarking Occlusion Perception in Multimodal Large Language Models](https://arxiv.org/abs/2508.04059)
*Zhaochen Liu,Kaiwen Gao,Shuyi Liang,Bin Xiao,Limeng Qiao,Lin Ma,Tingting Jiang*

Main category: cs.CV

TL;DR: 作者提出了一个新的VQA基准O-Bench，专门用于评估多模态大型语言模型在遮挡感知任务上的表现。在SA-1B数据集的基础上，通过一种分层合成方法构建了1,365张具有语义连贯遮挡场景的图像，并标注了4,588个问题-答案对组成的五个定制任务。评估了22种代表模型，发现模型在遮挡感知上显著落后于人，即使模型规格增大或使用思维过程也无法完全缓解失败趋势。文章报告了失败模式，并表示O-Bench的公开将为计算机视觉的进一步发展提供评估工具。


<details>
  <summary>Details</summary>
Motivation: 解决模型遮挡感知能力未被充分研究的空白。尽管多模态大规模语言模型（MLLMs）在视觉和文本处理方面展现出了优秀的处理能力，但在遮挡感知任务中的表现仍未得到充分评估。这一核心能力对于提升AI达到‘人类级别的空间理解认知’至关重要。

Method: 1.数据集创建：利用SA-1B图像数据集，采用全新的分层合成手法构建包含1,365张含有语义一致性遮挡特征的图片；2.问题标注：通过半自动工作流标注了4,588对问题答案，涵盖5个特殊任务；3.模型评价：系统性地对22种主流的MLLMs模型进行评估，并与人类基准进行对比研究。

Result: 评估得出：（1）目前的多模态大语言模型在遮挡感知能力上比人类有巨大能力差距；（2）发现模型缩放或优化思维流程（Thinking process）并不能完全消除这一差距；报告了三个典型失败模式：（1）过度保守的倾向；（2）脆弱的完形预测效果；（3）数值任务中明显困难。

Conclusion: 引入O-Bench作为首个基于视觉问题回答任务的遮挡感知基准；提供评估工具以促进模型未来发展，同时为多模态大模型在视觉理解方向的深入探索奠定基础；表明该领域仍有显著进步空间。

Abstract: Occlusion perception, a critical foundation for human-level spatial
understanding, embodies the challenge of integrating visual recognition and
reasoning. Though multimodal large language models (MLLMs) have demonstrated
remarkable capabilities, their performance on occlusion perception remains
under-explored. To address this gap, we introduce O-Bench, the first visual
question answering (VQA) benchmark specifically designed for occlusion
perception. Based on SA-1B, we construct 1,365 images featuring semantically
coherent occlusion scenarios through a novel layered synthesis approach. Upon
this foundation, we annotate 4,588 question-answer pairs in total across five
tailored tasks, employing a reliable, semi-automatic workflow. Our extensive
evaluation of 22 representative MLLMs against the human baseline reveals a
significant performance gap between current MLLMs and humans, which, we find,
cannot be sufficiently bridged by model scaling or thinking process. We further
identify three typical failure patterns, including an overly conservative bias,
a fragile gestalt prediction, and a struggle with quantitative tasks. We
believe O-Bench can not only provide a vital evaluation tool for occlusion
perception, but also inspire the development of MLLMs for better visual
intelligence. Our benchmark will be made publicly available upon paper
publication.

</details>


### [45] [TNet: Terrace Convolutional Decoder Network for Remote Sensing Image Semantic Segmentation](https://arxiv.org/abs/2508.04061)
*Chengqian Dai,Yonghong Guo,Hongzhao Xiang,Yigui Luo*

Main category: cs.CV

TL;DR: 提出了TNet（Terrace Convolutional Decoder Network），一种仅使用卷积和加法操作的简单有效架构，通过在解码阶段逐步将低分辨率特征（富含全局上下文）融合到高分辨率特征（富含局部细节）中，解决了现有方法忽视多分辨率间全局上下文依赖的问题。


<details>
  <summary>Details</summary>
Motivation: 当前遥感图像分割网络（如UNet变体）通常通过引入Transformer或Mamba等模块加强解码器阶段的全局-局部特征交互，但这类增强主要集中在单一尺度内的关系，忽视了跨多个分辨率的全局上下文依赖。

Method: 提出TNet架构，核心思想是在解码阶段逐步将低分辨率特征（富含全局上下文）通过卷积和加法操作融合到高分辨率特征（富含局部细节）中。这种渐进式融合使模型能够学习空间感知的卷积核，自然地在不同阶段融合全局和局部信息。具体实现采用ResNet-18作为编码器（称为TNet-R）。

Result: 在ISPRS Vaihingen（mIoU 85.35%）、ISPRS Potsdam（mIoU 87.05%）和LoveDA（mIoU 52.19%）三个基准数据集上达到有竞争力的性能，同时保持高计算效率。

Conclusion: TNet通过逐步融合多尺度特征的策略有效捕获了跨分辨率的全局上下文依赖，仅用卷积和加法操作即可实现高性能的遥感图像分割，且计算效率优异。

Abstract: In remote sensing, most segmentation networks adopt the UNet architecture,
often incorporating modules such as Transformers or Mamba to enhance
global-local feature interactions within decoder stages. However, these
enhancements typically focus on intra-scale relationships and neglect the
global contextual dependencies across multiple resolutions. To address this
limitation, we introduce the Terrace Convolutional Decoder Network (TNet), a
simple yet effective architecture that leverages only convolution and addition
operations to progressively integrate low-resolution features (rich in global
context) into higher-resolution features (rich in local details) across
decoding stages. This progressive fusion enables the model to learn
spatially-aware convolutional kernels that naturally blend global and local
information in a stage-wise manner. We implement TNet with a ResNet-18 encoder
(TNet-R) and evaluate it on three benchmark datasets. TNet-R achieves
competitive performance with a mean Intersection-over-Union (mIoU) of 85.35\%
on ISPRS Vaihingen, 87.05\% on ISPRS Potsdam, and 52.19\% on LoveDA, while
maintaining high computational efficiency. Code is publicly available.

</details>


### [46] [Bridging Diffusion Models and 3D Representations: A 3D Consistent Super-Resolution Framework](https://arxiv.org/abs/2508.04090)
*Yi-Ting Chen,Ting-Hsuan Liao,Pengsheng Guo,Alexander Schwing,Jia-Bin Huang*

Main category: cs.CV

TL;DR: 提出了一个基于3D高斯分布的场景表示的超分辨率框架3DSR，该框架利用现成的基于扩散的2D超分辨率模型来增强3D场景的视觉质量，并在保持三维一致性的同时产生高分辨率结果。


<details>
  <summary>Details</summary>
Motivation: 现有的超分辨率方法要么（如图像上采样）没有考虑多视图之间的三维一致性，要么（如视频超分辨率）仅试图隐式地纳入三维一致性。这些方法在重建场景时可能无法保持空间连贯性，导致视觉质量下降。

Method: 3DSR使用基于3D高斯分布的场景表示方法（3D Gaussian splatting）来显式地表示三维场景，并利用现成的基于扩散模型的二维超分辨率方法对场景进行超分辨率处理。这种方法不需要额外的微调，通过显式的三维表示确保多视图中的三维一致性，从而提升视觉效果。

Result: 在MipNeRF360和LLFF数据集上进行了评估，结果表明3DSR能够生成视觉上引人注目的高分辨率结果，同时保持三维重建的结构一致性。

Conclusion: 3DSR是一种新颖的三维超分辨率框架，它通过结合3D高斯分布的场景表示和现成的2D超分辨率模型，在不需要额外微调的情况下，有效地提升了三维场景的视觉质量并保持了三维一致性。

Abstract: We propose 3D Super Resolution (3DSR), a novel 3D Gaussian-splatting-based
super-resolution framework that leverages off-the-shelf diffusion-based 2D
super-resolution models. 3DSR encourages 3D consistency across views via the
use of an explicit 3D Gaussian-splatting-based scene representation. This makes
the proposed 3DSR different from prior work, such as image upsampling or the
use of video super-resolution, which either don't consider 3D consistency or
aim to incorporate 3D consistency implicitly. Notably, our method enhances
visual quality without additional fine-tuning, ensuring spatial coherence
within the reconstructed scene. We evaluate 3DSR on MipNeRF360 and LLFF data,
demonstrating that it produces high-resolution results that are visually
compelling, while maintaining structural consistency in 3D reconstructions.
Code will be released.

</details>


### [47] [DET-GS: Depth- and Edge-Aware Regularization for High-Fidelity 3D Gaussian Splatting](https://arxiv.org/abs/2508.04099)
*Zexu Huang,Min Xu,Stuart Perry*

Main category: cs.CV

TL;DR: 提出了一种名为DET-GS的深度和边缘感知正则化框架，用于改善在稀疏视图条件下3D高斯泼溅技术的几何重建准确性和视觉保真度。该方法通过多级几何深度监督和边缘感知正则化，以及在RGB引导下保留边缘的总变差损失，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在稀疏视图条件下的3D高斯泼溅（3DGS）存在几何重建不准确的问题。现有方法依赖的非局部深度正则化难以捕捉细粒度结构，且对深度估计噪声敏感；同时传统平滑方法忽略了语义边界，不加区分地削弱重要边缘和纹理。这些问题限制了重建质量。

Method: 1. 分层几何深度监督框架：自适应地强制执行多级几何一致性，提高结构保真度和对深度噪声的鲁棒性。2. 边缘感知深度正则化：基于Canny边缘检测的语义掩模指导，保留场景边界。3. RGB引导的保留边缘总变差（TV）损失：选择性平滑同质区域，严格保留高频细节和纹理。

Result: 实验表明，DET-GS在几何准确性和视觉保真度上均有显著提高，在稀疏视图新视角合成基准测试中超越了现有最先进方法。

Conclusion: DET-GS通过综合深度和边缘感知的正则化策略，有效解决了稀疏视图条件下3DGS的几何重建挑战，提升了重建质量。

Abstract: 3D Gaussian Splatting (3DGS) represents a significant advancement in the
field of efficient and high-fidelity novel view synthesis. Despite recent
progress, achieving accurate geometric reconstruction under sparse-view
conditions remains a fundamental challenge. Existing methods often rely on
non-local depth regularization, which fails to capture fine-grained structures
and is highly sensitive to depth estimation noise. Furthermore, traditional
smoothing methods neglect semantic boundaries and indiscriminately degrade
essential edges and textures, consequently limiting the overall quality of
reconstruction. In this work, we propose DET-GS, a unified depth and edge-aware
regularization framework for 3D Gaussian Splatting. DET-GS introduces a
hierarchical geometric depth supervision framework that adaptively enforces
multi-level geometric consistency, significantly enhancing structural fidelity
and robustness against depth estimation noise. To preserve scene boundaries, we
design an edge-aware depth regularization guided by semantic masks derived from
Canny edge detection. Furthermore, we introduce an RGB-guided edge-preserving
Total Variation loss that selectively smooths homogeneous regions while
rigorously retaining high-frequency details and textures. Extensive experiments
demonstrate that DET-GS achieves substantial improvements in both geometric
accuracy and visual fidelity, outperforming state-of-the-art (SOTA) methods on
sparse-view novel view synthesis benchmarks.

</details>


### [48] [NEARL-CLIP: Interacted Query Adaptation with Orthogonal Regularization for Medical Vision-Language Understanding](https://arxiv.org/abs/2508.04101)
*Zelin Peng,Yichen Zhao,Yu Huang,Piao Yang,Feilong Tang,Zhengqin Xu,Xiaokang Yang,Wei Shen*

Main category: cs.CV

TL;DR: 为了解决医学图像分析中标注数据有限的问题以及领域差距问题，作者提出了NEARL-CLIP，一个基于跨模态交互的VLM框架。该方法引入了USEformer模块，通过动态生成跨模态查询来增强模态间的交互；同时基于正交技术设计了OCA模块，将新知识分解为两个组成部分，从而促进更纯粹的新信息获取。该方法在高效参数量（仅引入约1.46M参数）的前提下实现了模态间的高效交互，充分释放了VLMs在医学领域的能力。


<details>
  <summary>Details</summary>
Motivation: 在医学图像分析领域，标注数据有限阻碍了模型的开发。虽然视觉语言模型（如CLIP）提供了强大的泛化能力，但其在医学图像上应用存在领域差距问题。现有的解决方案（如提示学习和单向模态交互技术）通常仅向单模态引入领域知识，可能带来性能提升，但常导致模态失准，无法充分利用VLMs的能力。

Method: NEARL-CLIP框架包括两个核心技术：1）USEformer（统一协同嵌入转换器），动态生成跨模态查询，促进模态间交互，增强多模态医学知识；2）OCA（正交交叉注意力适配器），引入正交技术将USEformer产生的新知识解耦为两个部分：真正的新信息和增量知识。通过隔离增量知识的干扰，使得新信息的学习更专注。整个框架采用高效参数设计，仅新增1.46M的参数量。

Result: 虽然原文未提供具体量化结果，但通过USEformer的模态交互机制和OCA对知识解耦的优化，NEARL-CLIP解决了跨医学图像的领域适应问题，并大幅降低了参数需求（仅新增1.46M）。该框架有望提高医学图像多模态任务的整体性能（需实验验证）。

Conclusion: NEARL-CLIP通过跨模态交互框架（USEformer）和正交解耦技术（OCA）弥合了通用VLMs与医学领域图像分析之间的领域差距。该方法在保证参数效率的基础上增强了模态间的知识互补与解耦，提升了多模态医学模型的学习能力，为医疗场景的落地提供了一种可行的解决方案。

Abstract: Computer-aided medical image analysis is crucial for disease diagnosis and
treatment planning, yet limited annotated datasets restrict medical-specific
model development. While vision-language models (VLMs) like CLIP offer strong
generalization capabilities, their direct application to medical imaging
analysis is impeded by a significant domain gap. Existing approaches to bridge
this gap, including prompt learning and one-way modality interaction
techniques, typically focus on introducing domain knowledge to a single
modality. Although this may offer performance gains, it often causes modality
misalignment, thereby failing to unlock the full potential of VLMs. In this
paper, we propose \textbf{NEARL-CLIP} (i\underline{N}teracted qu\underline{E}ry
\underline{A}daptation with o\underline{R}thogona\underline{L} Regularization),
a novel cross-modality interaction VLM-based framework that contains two
contributions: (1) Unified Synergy Embedding Transformer (USEformer), which
dynamically generates cross-modality queries to promote interaction between
modalities, thus fostering the mutual enrichment and enhancement of multi-modal
medical domain knowledge; (2) Orthogonal Cross-Attention Adapter (OCA). OCA
introduces an orthogonality technique to decouple the new knowledge from
USEformer into two distinct components: the truly novel information and the
incremental knowledge. By isolating the learning process from the interference
of incremental knowledge, OCA enables a more focused acquisition of new
information, thereby further facilitating modality interaction and unleashing
the capability of VLMs. Notably, NEARL-CLIP achieves these two contributions in
a parameter-efficient style, which only introduces \textbf{1.46M} learnable
parameters.

</details>


### [49] [AR as an Evaluation Playground: Bridging Metrics and Visual Perception of Computer Vision Models](https://arxiv.org/abs/2508.04102)
*Ashkan Ganj,Yiqin Zhao,Tian Guo*

Main category: cs.CV

TL;DR: 提出ARCADE，一个基于增强现实（AR）的平台，用于简化计算机视觉（CV）模型的人为中心评估。


<details>
  <summary>Details</summary>
Motivation: 人类感知研究能补充CV模型的定性评估，但此类研究通常复杂且难以扩展。AR技术提供了促进CV感知评价的机会，因此开发ARCADE以简化评估流程。

Method: 设计跨平台AR数据采集框架，支持可插拔模型推理的自定义实验协议，并通过AR流传输技术进行用户研究。通过在深度和光照估计两种CV模型上的实验验证平台有效性。

Result: 证实AR任务能有效引导人类对模型质量的感知判断。系统在多种部署环境和研究设置中表现出良好的可用性和性能，展现了作为人本评估平台的灵活性与效果。

Conclusion: ARCADE平台成功利用AR技术简化了CV模型的感知评估流程，为研究者提供高效的人为中心评估工具，未来可通过更多CV任务扩展应用场景。

Abstract: Human perception studies can provide complementary insights to qualitative
evaluation for understanding computer vision (CV) model performance. However,
conducting human perception studies remains a non-trivial task, it often
requires complex, end-to-end system setups that are time-consuming and
difficult to scale. In this paper, we explore the unique opportunity presented
by augmented reality (AR) for helping CV researchers to conduct perceptual
studies. We design ARCADE, an evaluation platform that allows researchers to
easily leverage AR's rich context and interactivity for human-centered CV
evaluation. Specifically, ARCADE supports cross-platform AR data collection,
custom experiment protocols via pluggable model inference, and AR streaming for
user studies. We demonstrate ARCADE using two types of CV models, depth and
lighting estimation and show that AR tasks can be effectively used to elicit
human perceptual judgments of model quality. We also evaluate the systems
usability and performance across different deployment and study settings,
highlighting its flexibility and effectiveness as a human-centered evaluation
platform.

</details>


### [50] [Unlocking the Potential of MLLMs in Referring Expression Segmentation via a Light-weight Mask Decode](https://arxiv.org/abs/2508.04107)
*Jingchao Wang,Zhijian Wu,Dingjiang Huang,Yefeng Zheng,Hong Wang*

Main category: cs.CV

TL;DR: 该论文提出了一种名为MLLMSeg的新型框架，用于参考表达式分割（RES）任务。该框架充分利用MLLM视觉编码器固有的视觉细节特征，无需引入额外视觉编码器（如SAM），并通过一个细节增强和语义一致的特征融合模块（DSFF）将视觉细节特征与LLM的语义特征融合。最后，使用仅34M参数的轻量级掩码解码器实现精确掩码预测。该方法在性能和成本之间取得了更好的平衡，超越了现有的基于SAM和不使用SAM的方法。


<details>
  <summary>Details</summary>
Motivation: 现有的RES方法要么耦合了参数量高达632M的SAM（导致计算成本高），要么采用轻量级但牺牲精度的流程。因此，作者旨在解决性能与成本之间的权衡问题，提出一种既高效又精确的新方法。

Method: 1. 充分利用MLLM视觉编码器固有的视觉细节特征，避免引入额外视觉编码器（如SAM）。2. 设计一个细节增强和语义一致的特征融合模块（DSFF），融合来自视觉编码器的细节相关特征和来自LLM的语义特征。3. 构建一个轻量级掩码解码器（仅34M参数），利用视觉编码器的空间细节特征和LLM的语义特征进行掩码预测。

Result: 大量实验表明，MLLMSeg在性能和成本之间取得了更好的平衡，超越了现有的基于SAM和不使用SAM的竞争对手。

Conclusion: MLLMSeg通过有效利用MLLM内部的特征，无需额外引入SAM这样的模型，成功实现了高效且精确的参考表达式分割。该方法不仅性能优越，而且大幅度降低了参数量和计算成本。

Abstract: Reference Expression Segmentation (RES) aims to segment image regions
specified by referring expressions and has become popular with the rise of
multimodal large models (MLLMs). While MLLMs excel in semantic understanding,
their token-generation paradigm struggles with pixel-level dense prediction.
Existing RES methods either couple MLLMs with the parameter-heavy Segment
Anything Model (SAM) with 632M network parameters or adopt SAM-free lightweight
pipelines that sacrifice accuracy. To address the trade-off between performance
and cost, we specifically propose MLLMSeg, a novel framework that fully
exploits the inherent visual detail features encoded in the MLLM vision encoder
without introducing an extra visual encoder. Besides, we propose a
detail-enhanced and semantic-consistent feature fusion module (DSFF) that fully
integrates the detail-related visual feature with the semantic-related feature
output by the large language model (LLM) of MLLM. Finally, we establish a
light-weight mask decoder with only 34M network parameters that optimally
leverages detailed spatial features from the visual encoder and semantic
features from the LLM to achieve precise mask prediction. Extensive experiments
demonstrate that our method generally surpasses both SAM-based and SAM-free
competitors, striking a better balance between performance and cost. Code is
available at https://github.com/jcwang0602/MLLMSeg.

</details>


### [51] [CLIPVehicle: A Unified Framework for Vision-based Vehicle Search](https://arxiv.org/abs/2508.04120)
*Likai Wang,Ruize Han,Xiangqun Zhang,Wei Feng*

Main category: cs.CV

TL;DR: CLIPVehicle是一个联合检测和重识别的端到端模型，用于车辆搜索。它通过双粒度语义区域对齐模块和多层次车辆识别学习策略解决了检测和重识别任务在目标上的冲突。


<details>
  <summary>Details</summary>
Motivation: 现有的车辆搜索方法通常分两步：先检测所有车辆并存储其图像块，然后对这些图像块进行重识别。这种方法资源消耗大且不够实用。因此，本文提出一种端到端的联合检测和重识别模型来解决这一问题。

Method: 提出CLIPVehicle框架，包括：1）双粒度语义区域对齐模块，利用视觉语言模型（VLMs）进行车辆区分建模；2）多层次车辆识别学习策略，从全局、实例和特征层学习身份表示。此外，构建了一个包含真实数据集（CityFlowVS）和两个合成数据集（SynVS-Day和SynVS-All）的基准测试集。

Result: 在车辆重识别（Re-ID）和行人搜索任务上，该方法均优于现有最先进方法。

Conclusion: 本文提出的CLIPVehicle模型有效解决了车辆检测和重识别在联合学习中的目标冲突问题，并通过多个新数据集验证了其优越性，为后续车辆搜索研究提供了重要参考。

Abstract: Vehicles, as one of the most common and significant objects in the real
world, the researches on which using computer vision technologies have made
remarkable progress, such as vehicle detection, vehicle re-identification, etc.
To search an interested vehicle from the surveillance videos, existing methods
first pre-detect and store all vehicle patches, and then apply vehicle
re-identification models, which is resource-intensive and not very practical.
In this work, we aim to achieve the joint detection and re-identification for
vehicle search. However, the conflicting objectives between detection that
focuses on shared vehicle commonness and re-identification that focuses on
individual vehicle uniqueness make it challenging for a model to learn in an
end-to-end system. For this problem, we propose a new unified framework, namely
CLIPVehicle, which contains a dual-granularity semantic-region alignment module
to leverage the VLMs (Vision-Language Models) for vehicle discrimination
modeling, and a multi-level vehicle identification learning strategy to learn
the identity representation from global, instance and feature levels. We also
construct a new benchmark, including a real-world dataset CityFlowVS, and two
synthetic datasets SynVS-Day and SynVS-All, for vehicle search. Extensive
experimental results demonstrate that our method outperforms the
state-of-the-art methods of both vehicle Re-ID and person search tasks.

</details>


### [52] [Conditional Latent Diffusion Models for Zero-Shot Instance Segmentation](https://arxiv.org/abs/2508.04122)
*Maximilian Ulmer,Wout Boerdijk,Rudolph Triebel,Maximilian Durner*

Main category: cs.CV

TL;DR: 本文提出了OC-DiT，一种用于对象中心预测的扩散模型，并应用于零样本实例分割。通过条件潜在扩散框架，生成实例分割掩码，无需在目标数据上重新训练，即实现了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 解决实例分割任务中需要大量标注数据且缺乏泛化能力的问题，提出一种基于扩散模型的方法，通过合成数据集学习对象实例的特征表示，实现零样本泛化。

Method: 1. 提出了条件潜在扩散框架，利用对象模板和图像特征生成实例掩码；2. 设计了两个模型：粗粒度模型生成初始实例提议，细粒度模型并行精炼所有提议；3. 使用合成数据集训练，包含数千个高质量对象网格。

Result: 模型在多个真实世界基准测试中取得SOTA性能，无需对目标数据重新训练。

Conclusion: 通过条件潜在扩散方法有效解决了实例分割任务中的零样本泛化问题，证明了扩散模型在实例分割领域的潜力。

Abstract: This paper presents OC-DiT, a novel class of diffusion models designed for
object-centric prediction, and applies it to zero-shot instance segmentation.
We propose a conditional latent diffusion framework that generates instance
masks by conditioning the generative process on object templates and image
features within the diffusion model's latent space. This allows our model to
effectively disentangle object instances through the diffusion process, which
is guided by visual object descriptors and localized image cues. Specifically,
we introduce two model variants: a coarse model for generating initial object
instance proposals, and a refinement model that refines all proposals in
parallel. We train these models on a newly created, large-scale synthetic
dataset comprising thousands of high-quality object meshes. Remarkably, our
model achieves state-of-the-art performance on multiple challenging real-world
benchmarks, without requiring any retraining on target data. Through
comprehensive ablation studies, we demonstrate the potential of diffusion
models for instance segmentation tasks.

</details>


### [53] [Excavate the potential of Single-Scale Features: A Decomposition Network for Water-Related Optical Image Enhancement](https://arxiv.org/abs/2508.04123)
*Zheng Cheng,Wenri Wang,Guangyong Chen,Yakun Ju,Yihua Cheng,Zhisong Liu,Yanda Meng,Jintao Song*

Main category: cs.CV

TL;DR: 本文质疑了水下图像增强（UIE）领域普遍采用的多尺度特征提取的必要性，通过实验证明单尺度特征提取可达到甚至超越多尺度方法的性能。为此，提出了一种新型单尺度分解网络（SSD-Net），通过非对称分解机制将图像解耦为洁净层和退化层，并结合CNN与Transformer的优势，有效提升退化解耦能力。


<details>
  <summary>Details</summary>
Motivation: 当前水下图像增强的主流方法依赖多尺度特征融合，但作者通过大量实验发现高质量图像重建并不必然依赖多尺度融合，单尺度特征提取在降低复杂度的同时可达到同等或更优效果。因此，本文旨在深度挖掘单尺度特征的潜力，解决水下图像的颜色失真、模糊和低对比度等问题。

Method: 提出单尺度分解网络（SSD-Net），核心创新为非对称分解机制：将输入图像解耦为洁净层（场景固有信息）和退化层（介质干扰）。网络架构包含两个关键模块：1）并行特征分解块（PFDB），通过高效注意力操作和自适应稀疏Transformer实现双分支特征空间解耦；2）双向特征通信块（BFCB），实现跨层残差交互以挖掘互补特征。该设计结合CNN的局部特征提取能力和Transformer的全局建模优势，在保持特征分解独立性的同时建立动态跨层信息通路。

Result: 实验证明所提出的SSD-Net能够高效分离图像中的固有场景信息和退化干扰，单尺度方法在性能上匹配或超越多尺度方法，同时显著降低模型复杂度。

Conclusion: 单尺度特征提取具备替代复杂多尺度融合的潜力。SSD-Net通过创新性分解机制与CNN-Transformer融合架构，为水下图像增强提供了一种更简单高效的解决方案，在保持高性能的同时降低了计算负担。

Abstract: Underwater image enhancement (UIE) techniques aim to improve visual quality
of images captured in aquatic environments by addressing degradation issues
caused by light absorption and scattering effects, including color distortion,
blurring, and low contrast. Current mainstream solutions predominantly employ
multi-scale feature extraction (MSFE) mechanisms to enhance reconstruction
quality through multi-resolution feature fusion. However, our extensive
experiments demonstrate that high-quality image reconstruction does not
necessarily rely on multi-scale feature fusion. Contrary to popular belief, our
experiments show that single-scale feature extraction alone can match or
surpass the performance of multi-scale methods, significantly reducing
complexity. To comprehensively explore single-scale feature potential in
underwater enhancement, we propose an innovative Single-Scale Decomposition
Network (SSD-Net). This architecture introduces an asymmetrical decomposition
mechanism that disentangles input image into clean layer along with degradation
layer. The former contains scene-intrinsic information and the latter encodes
medium-induced interference. It uniquely combines CNN's local feature
extraction capabilities with Transformer's global modeling strengths through
two core modules: 1) Parallel Feature Decomposition Block (PFDB), implementing
dual-branch feature space decoupling via efficient attention operations and
adaptive sparse transformer; 2) Bidirectional Feature Communication Block
(BFCB), enabling cross-layer residual interactions for complementary feature
mining and fusion. This synergistic design preserves feature decomposition
independence while establishing dynamic cross-layer information pathways,
effectively enhancing degradation decoupling capacity.

</details>


### [54] [Learning Using Privileged Information for Litter Detection](https://arxiv.org/abs/2508.04124)
*Matthias Bartolo,Konstantinos Makantasis,Dylan Seychell*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的垃圾检测方法，首次将特权信息与深度学习目标检测相结合，在提高检测精度的同时保持模型效率。该方法在多个数据集上实现了持续的性能提升，且不增加模型复杂度。


<details>
  <summary>Details</summary>
Motivation: 随着全球垃圾污染问题日益严重，开发有效的自动化垃圾检测工具成为重要挑战。然而，现有方法在检测小型垃圾或被部分遮挡的垃圾时面临困难。

Method: 1. 结合特权信息（privileged information）与目标检测模型；
2. 提出将边界框信息编码为二值掩膜（binary mask）的方法，作为检测模型的输入来优化检测引导；
3. 在SODA、BDW和UAVVaste等五个主流目标检测模型上进行评估。

Result: 1. 在SODA数据集上的评估显示检测精度提升；
2. 在BDW和UAVVaste数据集上的跨数据集评估证实了方法的泛化能力；
3. 所有模型均获得一致的性能提升（包括检测小目标和部分遮挡目标），且不增加模型复杂度或计算负担。

Conclusion: 该方法在保持模型效率的前提下有效提高了垃圾检测精度，尤其提升了小目标和被遮挡目标的检测能力，并展现出良好的泛化性能，为实际应用提供了准确性与效率的平衡解决方案。

Abstract: As litter pollution continues to rise globally, developing automated tools
capable of detecting litter effectively remains a significant challenge. This
study presents a novel approach that combines, for the first time, privileged
information with deep learning object detection to improve litter detection
while maintaining model efficiency. We evaluate our method across five widely
used object detection models, addressing challenges such as detecting small
litter and objects partially obscured by grass or stones. In addition to this,
a key contribution of our work can also be attributed to formulating a means of
encoding bounding box information as a binary mask, which can be fed to the
detection model to refine detection guidance. Through experiments on both
within-dataset evaluation on the renowned SODA dataset and cross-dataset
evaluation on the BDW and UAVVaste litter detection datasets, we demonstrate
consistent performance improvements across all models. Our approach not only
bolsters detection accuracy within the training sets but also generalises well
to other litter detection contexts. Crucially, these improvements are achieved
without increasing model complexity or adding extra layers, ensuring
computational efficiency and scalability. Our results suggest that this
methodology offers a practical solution for litter detection, balancing
accuracy and efficiency in real-world applications.

</details>


### [55] [SVC 2025: the First Multimodal Deception Detection Challenge](https://arxiv.org/abs/2508.04129)
*Xun Lin,Xiaobao Guo,Taorui Wang,Yingjie Ma,Jiajian Huang,Jiayu Zhang,Junzhe Cao,Zitong Yu*

Main category: cs.CV

TL;DR: 提出了SVC 2025多模态欺骗检测挑战赛，旨在评估音视频欺骗检测中的跨域泛化能力。该挑战赛要求参与者开发在多个异构数据集上泛化性能良好的模型，利用音频、视频和文本多模态数据捕捉细微的欺骗线索。共有21个团队提交了最终结果。


<details>
  <summary>Details</summary>
Motivation: 现有的欺骗检测研究主要集中在单领域场景，忽视了领域转移导致的显著性能下降。实际应用中需要模型具有良好的跨域泛化能力，当前缺乏相关的基准测试。

Method: 设计SVC 2025多模态欺骗检测挑战赛作为新基准，任务要求开发模型在多个异构数据集上实现跨域泛化性能。模型必须利用音频、视频和文本等多模态数据捕捉微妙且隐式的欺骗线索。

Result: 竞赛吸引了21个团队提交最终模型结果。挑战赛详情可通过https://sites.google.com/view/svc-mm25获取。

Conclusion: 该挑战赛旨在推动开发更适应性强、可解释且实际可部署的欺骗检测系统，促进多模态学习领域的发展。通过建立跨域泛化评估基准，推动解决现有单领域研究局限性的问题。

Abstract: Deception detection is a critical task in real-world applications such as
security screening, fraud prevention, and credibility assessment. While deep
learning methods have shown promise in surpassing human-level performance,
their effectiveness often depends on the availability of high-quality and
diverse deception samples. Existing research predominantly focuses on
single-domain scenarios, overlooking the significant performance degradation
caused by domain shifts. To address this gap, we present the SVC 2025
Multimodal Deception Detection Challenge, a new benchmark designed to evaluate
cross-domain generalization in audio-visual deception detection. Participants
are required to develop models that not only perform well within individual
domains but also generalize across multiple heterogeneous datasets. By
leveraging multimodal data, including audio, video, and text, this challenge
encourages the design of models capable of capturing subtle and implicit
deceptive cues. Through this benchmark, we aim to foster the development of
more adaptable, explainable, and practically deployable deception detection
systems, advancing the broader field of multimodal learning. By the conclusion
of the workshop competition, a total of 21 teams had submitted their final
results. https://sites.google.com/view/svc-mm25 for more information.

</details>


### [56] [DS$^2$Net: Detail-Semantic Deep Supervision Network for Medical Image Segmentation](https://arxiv.org/abs/2508.04131)
*Zhaohong Huang,Yuxin Zhang,Mingbao Lin,Taojian Zhou,Guorong Cai,Rongrong Ji*

Main category: cs.CV

TL;DR: 该论文提出了一个名为DS²Net的细节-语义深度监督网络，用于医学图像分割。该网络通过细节增强模块（DEM）和语义增强模块（SEM）同时监督低层细节特征和高层语义特征，并采用基于不确定性的监督损失自适应不同尺度的监督强度。实验证明其在多个医学影像基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在医学图像分割中仅单独监督粗粒度语义特征或细粒度细节特征，忽略了这两类特征之间的重要关联。本文主张利用互补特征进行监督，以提高分割性能。

Method: 1. 设计DS²Net网络，包含细节增强模块（DEM）和语义增强模块（SEM）。
2. DEM利用低层特征生成细节掩膜，进行细节特征监督；
3. SEM利用高层特征生成语义掩膜，进行语义特征监督；
4. 提出基于不确定性的监督损失，根据特征不确定性自适应调整监督强度。

Result: 在通过结肠镜、超声和显微镜获取的六个基准测试集上进行实验，DS²Net始终优于现有的医学图像分析方法。

Conclusion: DS²Net通过同时监督细节和语义特征，以及自适应损失函数，实现了医学图像分割的显著提升，验证了多角度深度监督的有效性。

Abstract: Deep Supervision Networks exhibit significant efficacy for the medical
imaging community. Nevertheless, existing work merely supervises either the
coarse-grained semantic features or fine-grained detailed features in
isolation, which compromises the fact that these two types of features hold
vital relationships in medical image analysis. We advocate the powers of
complementary feature supervision for medical image segmentation, by proposing
a Detail-Semantic Deep Supervision Network (DS$^2$Net). DS$^2$Net navigates
both low-level detailed and high-level semantic feature supervision through
Detail Enhance Module (DEM) and Semantic Enhance Module (SEM). DEM and SEM
respectively harness low-level and high-level feature maps to create detail and
semantic masks for enhancing feature supervision. This is a novel shift from
single-view deep supervision to multi-view deep supervision. DS$^2$Net is also
equipped with a novel uncertainty-based supervision loss that adaptively
assigns the supervision strength of features within distinct scales based on
their uncertainty, thus circumventing the sub-optimal heuristic design that
typifies previous works. Through extensive experiments on six benchmarks
captured under either colonoscopy, ultrasound and microscope, we demonstrate
that DS$^2$Net consistently outperforms state-of-the-art methods for medical
image analysis.

</details>


### [57] [UniFGVC: Universal Training-Free Few-Shot Fine-Grained Vision Classification via Attribute-Aware Multimodal Retrieval](https://arxiv.org/abs/2508.04136)
*Hongyu Guo,Kuan Zhu,Xiangzhao Hao,Haiyun Guo,Ming Tang,Jinqiao Wang*

Main category: cs.CV

TL;DR: UniFGVC是一个无需训练的多模态检索框架，用于小样本细粒度视觉分类（FGVC）。它通过生成结构化文本描述来利用多模态大语言模型（MLLM）的开放世界知识，并结合视觉和文本编码器在联合空间中进行检索，在多个基准测试中表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法大多基于预训练的视觉语言模型进行微调，但存在过拟合和泛化能力弱的问题。因此，提出一种无需训练、基于多模态检索的通用框架，以避免微调带来的问题。

Method: 1. 使用提出的CDV-Captioner（通过链式思维提示和视觉相似参考图像减少幻觉）生成结构化文本描述，捕获细粒度属性特征。2. 将每张图像转换为图像-描述对，构建多模态类别模板。3. 利用现成的视觉和文本编码器嵌入查询和模板对。4. 在联合空间中检索最近邻模板完成分类。

Result: 在12个FGVC基准测试上进行了广泛实验，一致优于之前的小样本CLIP方法，甚至优于一些全监督的MLLMs方法。

Conclusion: UniFGVC框架无需训练，具有强大的泛化能力和适应性，通过多模态检索整合视觉和文本信息，显著提高了小样本FGVC的性能。

Abstract: Few-shot fine-grained visual classification (FGVC) aims to leverage limited
data to enable models to discriminate subtly distinct categories. Recent works
mostly finetuned the pre-trained visual language models to achieve performance
gain, yet suffering from overfitting and weak generalization. To deal with
this, we introduce UniFGVC, a universal training-free framework that
reformulates few-shot FGVC as multimodal retrieval. First, we propose the
Category-Discriminative Visual Captioner (CDV-Captioner) to exploit the
open-world knowledge of multimodal large language models (MLLMs) to generate a
structured text description that captures the fine-grained attribute features
distinguishing closely related classes. CDV-Captioner uses chain-of-thought
prompting and visually similar reference images to reduce hallucination and
enhance discrimination of generated captions. Using it we can convert each
image into an image-description pair, enabling more comprehensive feature
representation, and construct the multimodal category templates using few-shot
samples for the subsequent retrieval pipeline. Then, off-the-shelf vision and
text encoders embed query and template pairs, and FGVC is accomplished by
retrieving the nearest template in the joint space. UniFGVC ensures broad
compatibility with diverse MLLMs and encoders, offering reliable generalization
and adaptability across few-shot FGVC scenarios. Extensive experiments on 12
FGVC benchmarks demonstrate its consistent superiority over prior few-shot
CLIP-based methods and even several fully-supervised MLLMs-based approaches.

</details>


### [58] [IDCNet: Guided Video Diffusion for Metric-Consistent RGBD Scene Generation with Precise Camera Control](https://arxiv.org/abs/2508.04147)
*Lijuan Liu,Wenfa Li,Dongbo Zhang,Shuo Wang,Shaohui Jiao*

Main category: cs.CV

TL;DR: 研究人员提出了一种名为IDC-Net（图像-深度一致性网络）的框架，该框架能够在明确的相机轨迹控制下生成RGB-D视频序列。IDC-Net的核心创新在于同步处理RGB图像和深度图的联合学习方法，通过一个几何感知扩散模型加强帧之间的空间和几何对齐，从而实现更精确的相机控制。


<details>
  <summary>Details</summary>
Motivation: 现有方法多将RGB图像和深度图的生成分开处理，这可能导致帧间几何一致性的弱化。为了克服传统方法在相机控制和几何一致性方面的局限，研究人员决定开发一个联合学习模型来加强几何对齐和控制能力。

Method: IDC-Net是一种几何感知的扩散模型，使用一个联合学习框架来同步生成RGB图像和深度图。该方法包括基于相机轨迹控制的RGB-D序列生成框架，并通过一个几何感知transformer块实现细粒度相机控制。数据集层面，研究人员构建了具有相机-图像-深度一致性的数据集，该数据集包含尺度对齐的RGB视频、深度图以及精确的相机位姿，为模型提供高质量的几何监督数据。

Result: 实验证明IDC-Net在生成序列的视觉质量和几何一致性方面优于已有方法，生成的RGB-D序列可无缝用于下游三维场景重建任务，无需额外后处理步骤。

Conclusion: IDC-Net通过联合学习框架整合RGB和深度图生成任务，在增强相机控制和几何一致性方面取得突破。该框架在生成质量及下游任务适用性上展现了显著优势，为三维内容创作领域提供了新解决方案。

Abstract: We present IDC-Net (Image-Depth Consistency Network), a novel framework
designed to generate RGB-D video sequences under explicit camera trajectory
control. Unlike approaches that treat RGB and depth generation separately,
IDC-Net jointly synthesizes both RGB images and corresponding depth maps within
a unified geometry-aware diffusion model. The joint learning framework
strengthens spatial and geometric alignment across frames, enabling more
precise camera control in the generated sequences. To support the training of
this camera-conditioned model and ensure high geometric fidelity, we construct
a camera-image-depth consistent dataset with metric-aligned RGB videos, depth
maps, and accurate camera poses, which provides precise geometric supervision
with notably improved inter-frame geometric consistency. Moreover, we introduce
a geometry-aware transformer block that enables fine-grained camera control,
enhancing control over the generated sequences. Extensive experiments show that
IDC-Net achieves improvements over state-of-the-art approaches in both visual
quality and geometric consistency of generated scene sequences. Notably, the
generated RGB-D sequences can be directly feed for downstream 3D Scene
reconstruction tasks without extra post-processing steps, showcasing the
practical benefits of our joint learning framework. See more at
https://idcnet-scene.github.io.

</details>


### [59] [ICM-Fusion: In-Context Meta-Optimized LoRA Fusion for Multi-Task Adaptation](https://arxiv.org/abs/2508.04153)
*Yihua Shao,Xiaofeng Lin,Xinwei Long,Siyu Chen,Minxi Yan,Yang Liu,Ziyang Yan,Ao Ma,Hao Tang,Jingcai Guo*

Main category: cs.CV

TL;DR: 提出了一个名为ICM-Fusion的新框架，用于解决低秩适应（LoRA）模型在多任务适应中的权重冲突和灾难性遗忘问题。通过结合元学习和上下文适应，动态调整任务向量方向并利用自设计的融合变分自编码器（F-VAE）实现多任务LoRA生成。实验证明该方法在视觉和语言任务上显著降低了多任务损失并在少样本场景中实现任务增强。


<details>
  <summary>Details</summary>
Motivation: 现有的预训练LoRA融合方法在分解权重矩阵时共享相似参数但合并不同参数，会引发权重间冲突并导致灾难性领域遗忘。而增量学习虽然能适应多任务，但难以在少样本场景中获得泛化能力，尤其是当权重数据呈长尾分布时会导致融合权重中的遗忘问题。因此，需要一种新的方法来平衡不同领域的优化方向并减少多任务损失。

Method: 1. 提出ICM-Fusion框架，整合元学习和上下文适应。2. 通过特定任务向量算术（task vector arithmetic）动态平衡领域间的冲突优化方向，具体利用学习到的主流投影技术。3. 在潜在空间中调整任务向量方向以获得最优任务向量方向。4. 设计融合变分自编码器（Fusion VAE, F-VAE）重构融合后的LoRA，实现多任务LoRA生成。

Result: 在两个视觉任务（图像分类和目标检测）和两个语言任务（情感分析和机器翻译）上进行了广泛实验。实验结果显示：1. ICM-Fusion能够适配多种架构模型与不同任务。2. 相较于现有预训练LoRA融合方法，能够显著降低多任务损失。3. 在少样本场景中甚至实现了任务增强（即多任务性能超过单任务模型）。

Conclusion: ICM-Fusion框架通过创新的任务向量算术与F-VAE有效解决了LoRA模型的多任务融合问题，在多个领域和少样本场景都表现出优越性能，为预训练适应模型提供了通用且高效的多任务扩展方案。

Abstract: Enabling multi-task adaptation in pre-trained Low-Rank Adaptation (LoRA)
models is crucial for enhancing their generalization capabilities. Most
existing pre-trained LoRA fusion methods decompose weight matrices, sharing
similar parameters while merging divergent ones. However, this paradigm
inevitably induces inter-weight conflicts and leads to catastrophic domain
forgetting. While incremental learning enables adaptation to multiple tasks, it
struggles to achieve generalization in few-shot scenarios. Consequently, when
the weight data follows a long-tailed distribution, it can lead to forgetting
in the fused weights. To address this issue, we propose In-Context Meta LoRA
Fusion (ICM-Fusion), a novel framework that synergizes meta-learning with
in-context adaptation. The key innovation lies in our task vector arithmetic,
which dynamically balances conflicting optimization directions across domains
through learned manifold projections. ICM-Fusion obtains the optimal task
vector orientation for the fused model in the latent space by adjusting the
orientation of the task vectors. Subsequently, the fused LoRA is reconstructed
by a self-designed Fusion VAE (F-VAE) to realize multi-task LoRA generation. We
have conducted extensive experiments on visual and linguistic tasks, and the
experimental results demonstrate that ICM-Fusion can be adapted to a wide range
of architectural models and applied to various tasks. Compared to the current
pre-trained LoRA fusion method, ICM-Fusion fused LoRA can significantly reduce
the multi-tasking loss and can even achieve task enhancement in few-shot
scenarios.

</details>


### [60] [Audio-Assisted Face Video Restoration with Temporal and Identity Complementary Learning](https://arxiv.org/abs/2508.04161)
*Yuqin Cao,Yixuan Gao,Wei Sun,Xiaohong Liu,Yulun Zhang,Xiongkuo Min*

Main category: cs.CV

TL;DR: 该论文提出了一种通用音频辅助人脸视频修复网络（GAVN），利用身份和时间互补学习来解决多种类型的流媒体视频失真问题。该方法首先在低分辨率空间捕获帧间时间特征以粗粒度恢复帧并节省计算成本，然后在高分辨率空间中借助音频信号和面部标志提取帧内身份特征以恢复更多面部细节，最后整合两种特征生成高质量人脸视频。实验结果表明，GAVN在面部视频压缩伪影去除、去模糊和超分辨率任务上优于现有最先进方法。


<details>
  <summary>Details</summary>
Motivation: 现有的人脸视频修复方法大多忽视了视觉与音频特征之间的内在关联（尤其是嘴部区域），且已有的音频辅助方法仅专注于压缩伪影去除。因此，需要开发一种通用框架来处理多种流媒体视频失真问题，并有效融合多模态信息提升修复效果。

Method: 1. 在低分辨率空间捕获帧间时间特征，进行粗粒度恢复（节省计算成本）；2. 在高分辨率空间结合音频信号和面部标志提取帧内身份特征，优化面部细节；3. 通过重建模块整合时间特征与身份特征，生成最终高质量视频。

Result: GAVN在三个任务（压缩伪影去除、去模糊、超分辨率）上超越现有SOTA方法。实验验证了其多任务泛化能力和多模态特征融合的有效性。

Conclusion: GAVN通过联合学习时间特征和音频辅助的身份特征，实现了多种流媒体视频失真的高效修复，为多模态辅助的视频恢复提供了通用解决方案。

Abstract: Face videos accompanied by audio have become integral to our daily lives,
while they often suffer from complex degradations. Most face video restoration
methods neglect the intrinsic correlations between the visual and audio
features, especially in mouth regions. A few audio-aided face video restoration
methods have been proposed, but they only focus on compression artifact
removal. In this paper, we propose a General Audio-assisted face Video
restoration Network (GAVN) to address various types of streaming video
distortions via identity and temporal complementary learning. Specifically,
GAVN first captures inter-frame temporal features in the low-resolution space
to restore frames coarsely and save computational cost. Then, GAVN extracts
intra-frame identity features in the high-resolution space with the assistance
of audio signals and face landmarks to restore more facial details. Finally,
the reconstruction module integrates temporal features and identity features to
generate high-quality face videos. Experimental results demonstrate that GAVN
outperforms the existing state-of-the-art methods on face video compression
artifact removal, deblurring, and super-resolution. Codes will be released upon
publication.

</details>


### [61] [ToxicTAGS: Decoding Toxic Memes with Rich Tag Annotations](https://arxiv.org/abs/2508.04166)
*Subhankar Swain,Naquee Rizwan,Nayandeep Deb,Vishwajeet Singh Solanki,Vishwa Gangadhar S,Animesh Mukherjee*

Main category: cs.CV

TL;DR: 本论文提出了一个包含6300个带有注释的真实世界表情包数据集，用于改进不良内容的检测。数据集通过两阶段标记（毒性和细粒度分类）并附有社会相关标签增强上下文。作者还提出了一个标签生成模块，该模块通过引入标签显著提升了现有视觉语言模型在检测任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 当前社交媒体上表情包常被用于传播有害内容，但数据可访问性差和数据集构建的高成本阻碍了有效的表情包内容审核系统的发展。因此，需要创建带有足够上下文的标记数据集来支持高效的内容审核。

Method: 第一，收集并两阶段标注6300个真实世界表情包：先二分类（毒性/正常），再细粒度标注毒性表情包（仇恨、危险、攻击性）；第二，为每个表情包添加社会相关辅助元数据标签；第三，设计一个标签生成模块，为无标签的表情包生成社会背景标签；最后，利用这些标签增强视觉语言模型（VLM）在检测任务中的表现。

Result: 包含社会相关标签辅助元数据的数据集成功建立，并且提出的标签生成模块能够生成有效的标签。实验结果表明，结合这些标签可以显著提高当前最先进视觉语言模型（VLM）在检测任务中的性能。

Conclusion: 这项工作通过引入一个独特的多标签表情包数据集和标签生成模块，为多模态在线环境中的内容审核提供了新颖且可扩展的基础框架，极大提升了审核系统的性能。

Abstract: The 2025 Global Risks Report identifies state-based armed conflict and
societal polarisation among the most pressing global threats, with social media
playing a central role in amplifying toxic discourse. Memes, as a widely used
mode of online communication, often serve as vehicles for spreading harmful
content. However, limitations in data accessibility and the high cost of
dataset curation hinder the development of robust meme moderation systems. To
address this challenge, in this work, we introduce a first-of-its-kind dataset
of 6,300 real-world meme-based posts annotated in two stages: (i) binary
classification into toxic and normal, and (ii) fine-grained labelling of toxic
memes as hateful, dangerous, or offensive. A key feature of this dataset is
that it is enriched with auxiliary metadata of socially relevant tags,
enhancing the context of each meme. In addition, we propose a tag generation
module that produces socially grounded tags, because most in-the-wild memes
often do not come with tags. Experimental results show that incorporating these
tags substantially enhances the performance of state-of-the-art VLMs detection
tasks. Our contributions offer a novel and scalable foundation for improved
content moderation in multimodal online environments.

</details>


### [62] [AD-FM: Multimodal LLMs for Anomaly Detection via Multi-Stage Reasoning and Fine-Grained Reward Optimization](https://arxiv.org/abs/2508.04175)
*Jingyi Liao,Yongyi Su,Rong-Cheng Tu,Zhao Jin,Wenhao Sun,Yiting Li,Dacheng Tao,Xun Xu,Xulei Yang*

Main category: cs.CV

TL;DR: 该论文提出了一个综合框架，通过多阶段审慎推理过程和细粒度奖励机制，解决多模态大语言模型（MLLMs）在专业异常检测应用中存在的训练数据利用不足和推理过程监督不足问题，从而显著提升在工业数据集上的异常检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于分组相对策略优化（GRPO）的方法在专业异常检测中存在两个关键限制：当模型产生统一响应时训练数据利用不足，以及对推理过程的监督不足（鼓励立即做出二元决策而缺乏审慎分析）。

Method: 1. 引入多阶段审慎推理过程：指导模型从区域识别到聚焦检查，生成多样化的响应模式以优化GRPO，并实现分析工作流程的结构化监督。
2. 开发细粒度奖励机制：结合分类精度和定位监督，将二元反馈转化为连续信号，区分真正的分析洞察与虚假的正确性。

Result: 在多个工业数据集上的全面评估显示，该方法在将通用视觉语言模型适配到专业异常检测任务时实现了显著的性能提升，以高效利用现有标注实现了更高的准确性，有效弥合了通用MLLM能力与检测细微制造缺陷所需的细粒度视觉判别之间的差距。

Conclusion: 所提出的综合框架通过结合多阶段审慎推理和细粒度奖励机制，解决了MLLMs在专业异常检测领域的适配挑战，显著提升了检测性能，为通用模型在专业领域的精细化应用提供了有效解决方案。

Abstract: While Multimodal Large Language Models (MLLMs) demonstrate remarkable
capabilities across diverse domains, their application to specialized anomaly
detection (AD) remains constrained by domain adaptation challenges. Existing
Group Relative Policy Optimization (GRPO) based approaches suffer from two
critical limitations: inadequate training data utilization when models produce
uniform responses, and insufficient supervision over reasoning processes that
encourage immediate binary decisions without deliberative analysis. We propose
a comprehensive framework addressing these limitations through two synergistic
innovations. First, we introduce a multi-stage deliberative reasoning process
that guides models from region identification to focused examination,
generating diverse response patterns essential for GRPO optimization while
enabling structured supervision over analytical workflows. Second, we develop a
fine-grained reward mechanism incorporating classification accuracy and
localization supervision, transforming binary feedback into continuous signals
that distinguish genuine analytical insight from spurious correctness.
Comprehensive evaluation across multiple industrial datasets demonstrates
substantial performance improvements in adapting general vision-language models
to specialized anomaly detection. Our method achieves superior accuracy with
efficient adaptation of existing annotations, effectively bridging the gap
between general-purpose MLLM capabilities and the fine-grained visual
discrimination required for detecting subtle manufacturing defects and
structural irregularities.

</details>


### [63] [Uncertainty-Aware Spatial Color Correlation for Low-Light Image Enhancement](https://arxiv.org/abs/2508.04176)
*Jin Kuang,Dong Liu,Yukuang Zhang,Shengsheng Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为U2CLLIE的新型低光照图像增强框架，该框架融合了不确定性感知增强和空间-颜色因果相关建模，以解决现有方法在极暗条件下因梯度退化和噪声主导导致的特征不确定性问题和因果推理缺失。关键创新包括不确定性感知双域去噪模块（UaD）和分层因果感知框架，后者通过两个非对称因果相关建模模块（NeCo和AsC）构建分层因果约束，有效提升图像质量。


<details>
  <summary>Details</summary>
Motivation: 现有的低光图像增强方法主要关注架构创新，往往忽视了在极暗条件下特征表示的内在不确定性，这会导致梯度退化和噪声主导，损害模型的可靠性和因果推理能力。为了解决这些问题，本文提出了U2CLLIE框架。

Method: U2CLLIE框架包含两个关键部分：1）不确定性感知双域去噪模块（UaD）：利用高斯引导的自适应频域特征增强（G2AF）方法抑制频域噪声并优化基于熵的特征表示，以解决梯度消失和噪声主导问题；2）分层因果感知框架：首先通过亮度增强网络（LEN）对暗区进行粗亮度提升，然后在编解码阶段，通过邻居相关状态空间模块（NeCo）和自适应空间-颜色校准模块（AsC）协同构建分层因果约束，在特征空间重建并增强邻域结构和颜色一致性。

Result: 广泛的实验证明，U2CLLIE在多个基准数据集上均实现了最先进（SOTA）的性能，在不同场景下表现出鲁棒的性能和强大的泛化能力。

Conclusion: U2CLLIE通过整合不确定性感知增强机制和分层因果建模，有效解决了极暗条件下图像增强的挑战，显著提升图像质量并减少噪声影响，为低光照图像处理提供了新的有效解决方案。

Abstract: Most existing low-light image enhancement approaches primarily focus on
architectural innovations, while often overlooking the intrinsic uncertainty
within feature representations particularly under extremely dark conditions
where degraded gradient and noise dominance severely impair model reliability
and causal reasoning. To address these issues, we propose U2CLLIE, a novel
framework that integrates uncertainty-aware enhancement and spatial-color
causal correlation modeling. From the perspective of entropy-based uncertainty,
our framework introduces two key components: (1) An Uncertainty-Aware
Dual-domain Denoise (UaD) Module, which leverages Gaussian-Guided Adaptive
Frequency Domain Feature Enhancement (G2AF) to suppress frequency-domain noise
and optimize entropy-driven representations. This module enhances spatial
texture extraction and frequency-domain noise suppression/structure refinement,
effectively mitigating gradient vanishing and noise dominance. (2) A
hierarchical causality-aware framework, where a Luminance Enhancement Network
(LEN) first performs coarse brightness enhancement on dark regions. Then,
during the encoder-decoder phase, two asymmetric causal correlation modeling
modules Neighborhood Correlation State Space (NeCo) and Adaptive Spatial-Color
Calibration (AsC) collaboratively construct hierarchical causal constraints.
These modules reconstruct and reinforce neighborhood structure and color
consistency in the feature space. Extensive experiments demonstrate that
U2CLLIE achieves state-of-the-art performance across multiple benchmark
datasets, exhibiting robust performance and strong generalization across
various scenes.

</details>


### [64] [Deeper Inside Deep ViT](https://arxiv.org/abs/2508.04181)
*Sungrae Hong*

Main category: cs.CV

TL;DR: 本文研究了类似ViT-22B的大型视觉模型在本地环境中的训练行为，发现并解决了训练不稳定的问题，证明了ViT-22B在相同参数规模下性能优于ViT。此外，探索了ViT-22B在图像生成任务中的应用，并比较了ViT与ViT-22B作为图像生成结构的优劣。


<details>
  <summary>Details</summary>
Motivation: 大型视觉模型（如ViT-22B）的分析与理解尚未完全转化为实际应用。研究旨在探索该模型在本地训练环境中的表现与训练稳定性，并首次尝试将ViT-22B用于图像生成任务，以评估其作为生成模型的潜力。

Method: 1. 在本地环境中训练ViT-22B模型，观察并记录其训练过程的不稳定性；2. 对模型进行修改以稳定训练；3. 从头开始训练ViT-22B模型，并与ViT模型在相同参数规模下进行性能对比；4. 构建基于ViT的图像生成架构；5. 分别使用ViT和ViT-22B作为图像生成架构，对比分析两者的适应性和效果。

Result: 1. 成功识别并解决了ViT-22B训练不稳定的问题；2. 从头训练的ViT-22B模型在相同参数规模下性能优于ViT；3. 在图像生成任务中，ViT-22B相比标准ViT展现更优的生成能力，成为更合适的图像生成结构。

Conclusion: ViT-22B在本地训练环境中通过模型调整可稳定训练，其性能超越同等参数规模的ViT。在图像生成领域，ViT-22B首次被证明具有潜力，且其结构比标准ViT更适用于生成任务。修正后的ViT-22B为大规模视觉模型的实际应用提供了新的可能性。

Abstract: There have been attempts to create large-scale structures in vision models
similar to LLM, such as ViT-22B. While this research has provided numerous
analyses and insights, our understanding of its practical utility remains
incomplete. Therefore, we examine how this model structure reacts and train in
a local environment. We also highlight the instability in training and make
some model modifications to stabilize it. The ViT-22B model, trained from
scratch, overall outperformed ViT in terms of performance under the same
parameter size. Additionally, we venture into the task of image generation,
which has not been attempted in ViT-22B. We propose an image generation
architecture using ViT and investigate which between ViT and ViT-22B is a more
suitable structure for image generation.

</details>


### [65] [RPCANet++: Deep Interpretable Robust PCA for Sparse Object Segmentation](https://arxiv.org/abs/2508.04190)
*Fengyi Wu,Yimian Dai,Tianfang Zhang,Yixuan Ding,Jian Yang,Ming-Ming Cheng,Zhenming Peng*

Main category: cs.CV

TL;DR: 提出基于深度学习的RPCANet++，通过结构化解剖RPCA，融入记忆增强模块和深度对比先验模块，提升稀疏目标分割的鲁棒性和解释性。


<details>
  <summary>Details</summary>
Motivation: 传统鲁棒主成分分析(RPCA)计算负担大、高度依赖超参数微调、先验条件过于严格难以动态适应。

Method: 将松弛的RPCA模型解构为三个模块：背景逼近模块(BAM)、目标提取模块(OEM)和图像恢复模块(IRM)；在BAM中加入记忆增强模块优化背景信息传递；设计深度对比先验模块增强显著目标检测。

Result: 在多个数据集上的实验表明方法达到SOTA性能；可通过可视化低秩性和定量稀疏性评估解释性强；实现了可靠的分割效果。

Conclusion: 将RPCA的理论优势与深度网络的效率有效结合，为可解释稀疏目标分割设定了新基准。

Abstract: Robust principal component analysis (RPCA) decomposes an observation matrix
into low-rank background and sparse object components. This capability has
enabled its application in tasks ranging from image restoration to
segmentation. However, traditional RPCA models suffer from computational
burdens caused by matrix operations, reliance on finely tuned hyperparameters,
and rigid priors that limit adaptability in dynamic scenarios. To solve these
limitations, we propose RPCANet++, a sparse object segmentation framework that
fuses the interpretability of RPCA with efficient deep architectures. Our
approach unfolds a relaxed RPCA model into a structured network comprising a
Background Approximation Module (BAM), an Object Extraction Module (OEM), and
an Image Restoration Module (IRM). To mitigate inter-stage transmission loss in
the BAM, we introduce a Memory-Augmented Module (MAM) to enhance background
feature preservation, while a Deep Contrast Prior Module (DCPM) leverages
saliency cues to expedite object extraction. Extensive experiments on diverse
datasets demonstrate that RPCANet++ achieves state-of-the-art performance under
various imaging scenarios. We further improve interpretability via visual and
numerical low-rankness and sparsity measurements. By combining the theoretical
strengths of RPCA with the efficiency of deep networks, our approach sets a new
baseline for reliable and interpretable sparse object segmentation. Codes are
available at our Project Webpage https://fengyiwu98.github.io/rpcanetx.

</details>


### [66] [From Learning to Unlearning: Biomedical Security Protection in Multimodal Large Language Models](https://arxiv.org/abs/2508.04192)
*Dunyuan Xu,Xikai Yang,Yaoqian Li,Jinpeng Li,Pheng-Ann Heng*

Main category: cs.CV

TL;DR: MLLMU-Med：第一个用于评估生物医学多模态大语言模型（MLLM）在隐私保护和错误知识移除方面的遗忘能力的基准数据集，通过包含合成私人数据和事实错误的新型数据生成管道建立，并揭示了现有遗忘方法的不足。


<details>
  <summary>Details</summary>
Motivation: 生物医学MLLM的训练数据常包含难以检测的私人信息和错误知识，导致隐私泄露和错误输出。遗忘学习可选择性移除有害知识并保留正常能力，但缺乏评估标准。

Method: 1. 提出新型数据生成管道，合成私人数据和事实错误嵌入训练集；2. 构建首个生物医学MLLM遗忘基准MLLMU-Med，涵盖隐私保护和错误知识移除两大场景；3. 设计遗忘效率评分综合评估遗忘效果；4. 评估五种遗忘方法。

Result: 1. 成功构建MLLMU-Med基准数据集；2. 五种遗忘方法在生物医学MLLM上均表现有限，证明该领域有显著改进空间。

Conclusion: MLLMU-Med填补了生物医学MLLM安全评估空白，为研究遗忘学习提供新途径，现有方法的不足突显领域发展潜力。

Abstract: The security of biomedical Multimodal Large Language Models (MLLMs) has
attracted increasing attention. However, training samples easily contain
private information and incorrect knowledge that are difficult to detect,
potentially leading to privacy leakage or erroneous outputs after deployment.
An intuitive idea is to reprocess the training set to remove unwanted content
and retrain the model from scratch. Yet, this is impractical due to significant
computational costs, especially for large language models. Machine unlearning
has emerged as a solution to this problem, which avoids complete retraining by
selectively removing undesired knowledge derived from harmful samples while
preserving required capabilities on normal cases. However, there exist no
available datasets to evaluate the unlearning quality for security protection
in biomedical MLLMs. To bridge this gap, we propose the first benchmark
Multimodal Large Language Model Unlearning for BioMedicine (MLLMU-Med) built
upon our novel data generation pipeline that effectively integrates synthetic
private data and factual errors into the training set. Our benchmark targets
two key scenarios: 1) Privacy protection, where patient private information is
mistakenly included in the training set, causing models to unintentionally
respond with private data during inference; and 2) Incorrectness removal, where
wrong knowledge derived from unreliable sources is embedded into the dataset,
leading to unsafe model responses. Moreover, we propose a novel Unlearning
Efficiency Score that directly reflects the overall unlearning performance
across different subsets. We evaluate five unlearning approaches on MLLMU-Med
and find that these methods show limited effectiveness in removing harmful
knowledge from biomedical MLLMs, indicating significant room for improvement.
This work establishes a new pathway for further research in this promising
field.

</details>


### [67] [Gather and Trace: Rethinking Video TextVQA from an Instance-oriented Perspective](https://arxiv.org/abs/2508.04197)
*Yan Zhang,Gangyan Zeng,Daiqing Wu,Huawen Shen,Binbin Li,Yu Zhou,Can Ma,Xiaojun Bi*

Main category: cs.CV

TL;DR: 提出了一种名为GAT的新模型，用于视频文本视觉问答（Video TextVQA）任务，通过实例导向的方法解决了现有帧级方法的冗余文本和隐含关系建模问题。模型包含上下文聚合的实例收集模块和实例聚焦的轨迹跟踪模块，在多个数据集上取得了准确性和推理速度的提升。


<details>
  <summary>Details</summary>
Motivation: 现有的视频文本视觉问答方法主要基于帧级处理，存在文本实体冗余和关系建模不明确的问题，导致准确性低且效率不高。本文从实例导向的角度重新思考该任务，旨在通过聚合上下文并追踪实例轨迹来提高性能。

Method: 1. Context-Aggregated Instance Gathering模块：整合视觉外观、布局特征和文本内容，为每个视频文本实例生成统一的文本表示。2. Instance-Focused Trajectory Tracing模块：建立实例间的时空关系以追踪动态文本演化并进行答案推理。

Result: 在多个Video TextVQA数据集上优于现有方法、视频语言预训练方法和视频大型语言模型。准确率比先前的最佳方法提高3.86%，推理速度比视频大型语言模型快10倍。

Conclusion: GAT模型通过实例导向的框架有效解决了Video TextVQA任务中的冗余和关系建模问题，显著提升了准确性和效率。

Abstract: Video text-based visual question answering (Video TextVQA) aims to answer
questions by explicitly reading and reasoning about the text involved in a
video. Most works in this field follow a frame-level framework which suffers
from redundant text entities and implicit relation modeling, resulting in
limitations in both accuracy and efficiency. In this paper, we rethink the
Video TextVQA task from an instance-oriented perspective and propose a novel
model termed GAT (Gather and Trace). First, to obtain accurate reading result
for each video text instance, a context-aggregated instance gathering module is
designed to integrate the visual appearance, layout characteristics, and
textual contents of the related entities into a unified textual representation.
Then, to capture dynamic evolution of text in the video flow, an
instance-focused trajectory tracing module is utilized to establish
spatio-temporal relationships between instances and infer the final answer.
Extensive experiments on several public Video TextVQA datasets validate the
effectiveness and generalization of our framework. GAT outperforms existing
Video TextVQA methods, video-language pretraining methods, and video large
language models in both accuracy and inference speed. Notably, GAT surpasses
the previous state-of-the-art Video TextVQA methods by 3.86\% in accuracy and
achieves ten times of faster inference speed than video large language models.
The source code is available at https://github.com/zhangyan-ucas/GAT.

</details>


### [68] [Bootstrap Deep Spectral Clustering with Optimal Transport](https://arxiv.org/abs/2508.04200)
*Wengang Guo,Wei Ye,Chunchun Chen,Xin Sun,Christian Böhm,Claudia Plant,Susanto Rahardja*

Main category: cs.CV

TL;DR: 提出一种名为BootSC的深度谱聚类模型，通过端到端的单一网络联合学习谱聚类的所有阶段（构建亲和矩阵、嵌入谱特征和执行k-means聚类），并利用最优传输的监督来引导亲和矩阵和簇分配矩阵。引入语义正交化提升嵌入的判别性，在多个数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 谱聚类方法存在两个主要缺点：各阶段优化过程是独立的，以及表示能力有限。为解决这些问题，本文旨在设计一个端到端的深度学习框架，联合训练谱聚类的所有阶段，并提升模型的表达与判别能力。

Method: 1. 通过单一网络以端到端方式联合优化谱聚类全流程（亲和矩阵构建、谱嵌入学习及k-means聚类）；2. 利用最优运输理论推导的监督信号，同时引导亲和矩阵和簇分配矩阵的学习；3. 引入保持语义一致性的正交表示技术，强制谱嵌入空间正交化以提升特征判别能力。

Result: 在ImageNet-Dogs等数据集上取得显著性能突破，相较第二名方法提升16% NMI（归一化互信息）指标，代码已开源。

Conclusion: BootSC统一了谱聚类各阶段的优化过程，引入最优运输监督和正交正则化技术，显著改进聚类判别能力，实现了当前最佳性能，为谱聚类方法提供新的深度学习方法框架。

Abstract: Spectral clustering is a leading clustering method. Two of its major
shortcomings are the disjoint optimization process and the limited
representation capacity. To address these issues, we propose a deep spectral
clustering model (named BootSC), which jointly learns all stages of spectral
clustering -- affinity matrix construction, spectral embedding, and $k$-means
clustering -- using a single network in an end-to-end manner. BootSC leverages
effective and efficient optimal-transport-derived supervision to bootstrap the
affinity matrix and the cluster assignment matrix. Moreover, a
semantically-consistent orthogonal re-parameterization technique is introduced
to orthogonalize spectral embeddings, significantly enhancing the
discrimination capability. Experimental results indicate that BootSC achieves
state-of-the-art clustering performance. For example, it accomplishes a notable
16\% NMI improvement over the runner-up method on the challenging ImageNet-Dogs
dataset. Our code is available at https://github.com/spdj2271/BootSC.

</details>


### [69] [ViFP: A Framework for Visual False Positive Detection to Enhance Reasoning Reliability in VLMs](https://arxiv.org/abs/2508.04201)
*Ben Zhang,LuLu Yu,Lei Gao,Jing Liu,QuanJiang Guo,Hui Gao*

Main category: cs.CV

TL;DR: 文章提出了ViFP框架，用于增强视觉语言模型在推理时的可靠性，通过检测误报、构建有效的多轮问答路径以及动态分析推理路径一致性，提高了答案准确率和推理正确性。并通过可靠性评估指标VoC对模型进行定量评估。实验表明ViFP在多个数据集上提升效果明显。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言推理模型存在误报推理，即在答案正确但推理路径错误的情况下生成答案。已有的方法需要特定数据集和强化学习，训练成本高且泛化能力有限。

Method: ViFP框架在三大视觉推理维度构建基于子问题模板的推理路径，进而设计针对误报的改进模块：检测误报推理路径，并引入自适应机制；提出VoC作为可靠性评估指标，综合答案准确率和误报率。主要步骤：构建子问题模板、通过多轮问答生成推理路径、动态分析一致性检测误报、通过自适应链式思维处理误报与正确样本。

Result: 在A-OKVQA、OKVQA和FVQA等数据集上对多个闭源视觉语言模型测试，ViFP提升了推理性能。在A-OKVQA上准确率提高5.4%，优于此前最优方法4.3%，同时显著降低误报案例。

Conclusion: ViFP框架提升了视觉语言模型的推理可靠性和答案准确性，其构建的多维度子问题模板及适应性推理纠正机制具有良好泛化效果，同时VoC指标可为模型可靠性提供有效评价工具。

Abstract: In visual-language model (VLM) reasoning, false positive(FP) reasoning occurs
when a model generates a correct answer but follows an incorrect reasoning
path. Existing methods based on specific multi-step reasoning datasets and
reinforcement learning strategies, leading to high training costs and limited
generalization. In this work, we propose ViFP, a general framework for
enhancing visual reasoning reliability. It improves both answer accuracy and
reasoning soundness by detecting FPs. ViFP tackles the limitations of dataset
dependency and poor generalization by constructing sub-question templates
grounded in the core dimensions of visual reasoning, such as object
localization, characteristic description, and object discovery. ViFP then
builds effective reasoning paths via multi-turn QA to improve reasoning
accuracy. Meanwhile, ViFP dynamically analyzes the consistency of reasoning
path to identify potential FPs, and introduces a targeted chain-of-thought
(CoT) mechanism that adaptively guides both FP and non-FP samples. Thereby
reducing logical errors in the reasoning path while preserving accuracy.
Finally, we introduce a reliability evaluation metric-VoC, which integrates
answer accuracy and the FP rate, providing a quantitative tool to assess
whether a VLM not only answers correctly, but also reasons reliably. Our
experiments on closed-source VLMs show that ViFP consistently improves
performance across three datasets: A-OKVQA, OKVQA, and FVQA. On A-OKVQA, ViFP
improves accuracy by up to 5.4%, surpassing the previous state-of-the-art by
4.3%, and significantly reduces the number of FPs, validating its benefits in
enhancing reasoning reliability.

</details>


### [70] [Small Lesions-aware Bidirectional Multimodal Multiscale Fusion Network for Lung Disease Classification](https://arxiv.org/abs/2508.04205)
*Jianxun Yu,Ruiquan Ge,Zhipeng Wang,Cheng Yang,Chenyu Lin,Xianjun Fu,Jikui Liu,Ahmed Elazab,Changmiao Wang*

Main category: cs.CV

TL;DR: 提出了一种名为MMCAF-Net的多模态多尺度交叉注意力融合网络，用于解决医学影像与电子健康记录数据维度差异导致的对齐和融合挑战，并在Lung-PET-CT-Dx数据集上取得了超越现有方法的诊断准确率。


<details>
  <summary>Details</summary>
Motivation: 医学疾病诊断中存在小病灶误诊等问题，而多模态深度学习在这方面展现了潜力。但医学影像和电子健康记录数据之间的维度差异给有效对齐和融合带来了挑战。

Method: 提出了MMCAF-Net模型，包含两个关键模块：1) 特征金字塔结构结合高效的3D多尺度卷积注意力模块，用于从3D医学影像中提取病灶特征；2) 多尺度交叉注意力模块，解决维度不一致问题并增强多模态特征融合。

Result: 在Lung-PET-CT-Dx数据集上进行评估，结果显示该模型显著提升了诊断准确率，超越了当前最先进的方法。

Conclusion: MMCAF-Net通过创新的多尺度特征提取和交叉注意力机制有效融合了多模态医疗数据，提升了医学疾病诊断的准确性。

Abstract: The diagnosis of medical diseases faces challenges such as the misdiagnosis
of small lesions. Deep learning, particularly multimodal approaches, has shown
great potential in the field of medical disease diagnosis. However, the
differences in dimensionality between medical imaging and electronic health
record data present challenges for effective alignment and fusion. To address
these issues, we propose the Multimodal Multiscale Cross-Attention Fusion
Network (MMCAF-Net). This model employs a feature pyramid structure combined
with an efficient 3D multi-scale convolutional attention module to extract
lesion-specific features from 3D medical images. To further enhance multimodal
data integration, MMCAF-Net incorporates a multi-scale cross-attention module,
which resolves dimensional inconsistencies, enabling more effective feature
fusion. We evaluated MMCAF-Net on the Lung-PET-CT-Dx dataset, and the results
showed a significant improvement in diagnostic accuracy, surpassing current
state-of-the-art methods. The code is available at
https://github.com/yjx1234/MMCAF-Net

</details>


### [71] [What Holds Back Open-Vocabulary Segmentation?](https://arxiv.org/abs/2508.04211)
*Josip Šarić,Ivan Martinović,Matej Kristan,Siniša Šegvić*

Main category: cs.CV

TL;DR: 这篇论文针对开放词汇分割模型性能停滞的问题，提出了使用真实信息（oracle）来分析瓶颈的新方法，并通过实验揭示了失败原因，为未来研究指明了方向。


<details>
  <summary>Details</summary>
Motivation: 现有的开放词汇分割模型在训练税外概念识别上存在性能瓶颈，且近两年性能停滞，需要深入分析其原因。

Method: 提出了一种新的oracle分析方法，利用真实信息识别并解耦开放词汇分割模型中的性能瓶颈，设计验证实验进行实证分析。

Result: 实验揭示了开放词汇模型失败的关键原因，提供了对模型失效的深入理解。

Conclusion: 该方法为开放词汇分割的未来研究提供了重要启示，指明了潜在的突破方向，有望打破性能瓶颈。

Abstract: Standard segmentation setups are unable to deliver models that can recognize
concepts outside the training taxonomy. Open-vocabulary approaches promise to
close this gap through language-image pretraining on billions of image-caption
pairs. Unfortunately, we observe that the promise is not delivered due to
several bottlenecks that have caused the performance to plateau for almost two
years. This paper proposes novel oracle components that identify and decouple
these bottlenecks by taking advantage of the groundtruth information. The
presented validation experiments deliver important empirical findings that
provide a deeper insight into the failures of open-vocabulary models and
suggest prominent approaches to unlock the future research.

</details>


### [72] [SplitGaussian: Reconstructing Dynamic Scenes via Visual Geometry Decomposition](https://arxiv.org/abs/2508.04224)
*Jiahui Li,Shengeng Tang,Jingxuan He,Gang Huang,Zhangye Wang,Yantao Pan,Lechao Cheng*

Main category: cs.CV

TL;DR: 针对单目视频中动态场景重建的挑战，本文提出SplitGaussian框架，通过分离静态与动态组件来解决现有方法中几何与外观耦合导致的运动泄漏和闪烁问题，提高重建质量与时间一致性。


<details>
  <summary>Details</summary>
Motivation: 现有基于高斯喷涂的动态场景重建方法往往将静态和动态元素融合在统一表示中，导致运动泄漏（motion leakage）、几何扭曲和时域闪烁。核心问题在于跨时间的几何与外观建模耦合，影响稳定性和可解释性。

Method: 1. 提出SplitGaussian框架，将场景表示显式分解为静态与动态组件；
2. 解耦运动建模与背景几何，仅允许动态分支随时间形变；
3. 支持视角和时间依赖的外观精细化（如反射/光照变化）；
4. 通过分离设计避免静态区域受运动伪影干扰，同时提升收敛速度。

Result: 实验表明：
- 在渲染质量、几何稳定性和运动分离能力上超越现有SOTA方法；
- 显著减少运动伪影（如静态区域扭曲）和时域闪烁；
- 分离结构加速训练收敛。

Conclusion: 显式解耦静态/动态表示是解决动态场景重建问题的关键，SplitGaussian通过独立建模运动和背景几何实现更高保真度与时间一致性，为单目视频的4D重建提供新方向。

Abstract: Reconstructing dynamic 3D scenes from monocular video remains fundamentally
challenging due to the need to jointly infer motion, structure, and appearance
from limited observations. Existing dynamic scene reconstruction methods based
on Gaussian Splatting often entangle static and dynamic elements in a shared
representation, leading to motion leakage, geometric distortions, and temporal
flickering. We identify that the root cause lies in the coupled modeling of
geometry and appearance across time, which hampers both stability and
interpretability. To address this, we propose \textbf{SplitGaussian}, a novel
framework that explicitly decomposes scene representations into static and
dynamic components. By decoupling motion modeling from background geometry and
allowing only the dynamic branch to deform over time, our method prevents
motion artifacts in static regions while supporting view- and time-dependent
appearance refinement. This disentangled design not only enhances temporal
consistency and reconstruction fidelity but also accelerates convergence.
Extensive experiments demonstrate that SplitGaussian outperforms prior
state-of-the-art methods in rendering quality, geometric stability, and motion
separation.

</details>


### [73] [Continual Learning for VLMs: A Survey and Taxonomy Beyond Forgetting](https://arxiv.org/abs/2508.04227)
*Yuyang Liu,Qiuhe Hong,Linlan Huang,Alexandra Gomez-Villa,Dipam Goswami,Xialei Liu,Joost van de Weijer,Yonghong Tian*

Main category: cs.CV

TL;DR: 首个针对视觉语言模型持续学习（VLM-CL）的系统综述，识别了VLM在持续学习中的核心挑战（跨模态特征漂移、参数干扰、零样本能力腐蚀），提出了解决方案的分类法（多模态重播策略、跨模态正则化、参数高效适配），并指出评估基准的不足与未来方向。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型（VLM）在大规模预训练中表现出色，但在非平稳数据上的持续学习能力存在严重退化（灾难性遗忘）。与传统单模态持续学习不同，VLM面临跨模态特征漂移、共享架构参数干扰和零样本能力腐蚀等特有挑战，需要针对性研究。

Method: 1. 识别VLM持续学习中三大失效模式：跨模态特征漂移、参数干扰、零样本能力退化；2. 提出面向挑战的分类法：多模态重播（显式/隐式记忆应对特征漂移）、跨模态正则化（保护模态对齐）、参数高效适配（模块化/低秩更新避免干扰）；3. 分析现有评估指标/数据集缺陷，呼吁开发针对VLM遗忘特性和组合泛化的新基准。

Result: 构建首个VLM持续学习系统性分类框架和资源库（包括解决方案分类法、挑战分析、评估指标总结），指出当前领域在评估基准上的关键不足，并提出未来方向（如持续预训练、组合零样本学习）。

Conclusion: 综述填补了VLM持续学习领域的系统性研究空白，为开发终身视觉语言系统提供诊断性参考。资源库整合了分类工具和问题分析，推动领域发展。未来需加强评估协议设计，尤其是针对VLM特有的组合泛化和零样本能力遗忘的度量。

Abstract: Vision-language models (VLMs) have achieved impressive performance across
diverse multimodal tasks by leveraging large-scale pre-training. However,
enabling them to learn continually from non-stationary data remains a major
challenge, as their cross-modal alignment and generalization capabilities are
particularly vulnerable to catastrophic forgetting. Unlike traditional unimodal
continual learning (CL), VLMs face unique challenges such as cross-modal
feature drift, parameter interference due to shared architectures, and
zero-shot capability erosion. This survey offers the first focused and
systematic review of continual learning for VLMs (VLM-CL). We begin by
identifying the three core failure modes that degrade performance in VLM-CL.
Based on these, we propose a challenge-driven taxonomy that maps solutions to
their target problems: (1) \textit{Multi-Modal Replay Strategies} address
cross-modal drift through explicit or implicit memory mechanisms; (2)
\textit{Cross-Modal Regularization} preserves modality alignment during
updates; and (3) \textit{Parameter-Efficient Adaptation} mitigates parameter
interference with modular or low-rank updates. We further analyze current
evaluation protocols, datasets, and metrics, highlighting the need for better
benchmarks that capture VLM-specific forgetting and compositional
generalization. Finally, we outline open problems and future directions,
including continual pre-training and compositional zero-shot learning. This
survey aims to serve as a comprehensive and diagnostic reference for
researchers developing lifelong vision-language systems. All resources are
available at:
https://github.com/YuyangSunshine/Awesome-Continual-learning-of-Vision-Language-Models.

</details>


### [74] [LayerT2V: Interactive Multi-Object Trajectory Layering for Video Generation](https://arxiv.org/abs/2508.04228)
*Kangrui Cen,Baixuan Zhao,Yi Xin,Siqi Luo,Guangtao Zhai,Xiaohong Liu*

Main category: cs.CV

TL;DR: 提出LayerT2V方法，通过分层合成背景和前景对象来生成视频，解决了多对象运动轨迹控制问题，尤其在轨迹交叉时避免语义冲突，提高了多对象视频生成的性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型在控制多对象运动轨迹时存在困难，尤其在轨迹交叉场景下出现语义冲突。当前社区模型和数据集主要针对单对象运动，难以生成复杂的多对象场景。

Method: 分层生成视频：将背景和前景对象分别生成，并将每个对象置于独立的“层”上，然后组合成完整视频。这种方法可灵活整合多个独立元素，避免交叉区域冲突。

Result: 在多个指标上超越SOTA：mIoU提升1.4倍，AP50提升4.5倍。能够生成复杂多对象交叉轨迹场景。

Conclusion: LayerT2V是首个分层组合生成视频的方法，显著提高多对象运动场景的生成效果和控制能力，为复杂视频生成提供了新方向。

Abstract: Controlling object motion trajectories in Text-to-Video (T2V) generation is a
challenging and relatively under-explored area, particularly in scenarios
involving multiple moving objects. Most community models and datasets in the
T2V domain are designed for single-object motion, limiting the performance of
current generative models in multi-object tasks. Additionally, existing motion
control methods in T2V either lack support for multi-object motion scenes or
experience severe performance degradation when object trajectories intersect,
primarily due to the semantic conflicts in colliding regions. To address these
limitations, we introduce LayerT2V, the first approach for generating video by
compositing background and foreground objects layer by layer. This layered
generation enables flexible integration of multiple independent elements within
a video, positioning each element on a distinct "layer" and thus facilitating
coherent multi-object synthesis while enhancing control over the generation
process. Extensive experiments demonstrate the superiority of LayerT2V in
generating complex multi-object scenarios, showcasing 1.4x and 4.5x
improvements in mIoU and AP50 metrics over state-of-the-art (SOTA) methods.
Project page and code are available at https://kr-panghu.github.io/LayerT2V/ .

</details>


### [75] [Intention Enhanced Diffusion Model for Multimodal Pedestrian Trajectory Prediction](https://arxiv.org/abs/2508.04229)
*Yu Liu,Zhijie Liu,Xiao Ren,You-Fu Li,He Kong*

Main category: cs.CV

TL;DR: 提出了一种基于扩散模型的多模态行人轨迹预测方法，该方法通过分解行人运动意图为横向和纵向分量，并引入意图识别模块，从而提高了轨迹预测的可解释性和精准性。在ETH和UCY基准测试中表现出竞争力。


<details>
  <summary>Details</summary>
Motivation: 现有的基于扩散模型的行人轨迹预测方法未充分考虑行人运动的潜在意图（如方向性变化），这限制了预测模型的精准度和可解释性。为了解决这一问题，本文设计了一种新的扩散模型框架，将行人运动意图分解为横向和纵向运动分量，并引入专门模块捕捉这些意图，从而提升性能。

Method: 1. 提出一种扩散模型基础的多模态轨迹预测框架。2. 引入行人意图识别模块：将行人运动意图分解为横向（如左右转向意图）和纵向（如加速、减速意图）分量。3. 设计高效的引导机制，将意图信息整合到扩散过程的每一步，增强生成轨迹的可解释性。4. 在ETH和UCY数据集上进行模型训练和评估。

Result: 在ETH和UCY标准行人轨迹预测数据集上进行实验验证，与现有领先方法对比表明，所提出的运动意图增强方法在预测精度方面具有竞争性能，并能生成可解释的多样化预测轨迹。

Conclusion: 通过将分解的行人运动意图引入扩散模型框架提升预测性能是可行的。实验表明引入意图认知模块和引导机制能更有效捕捉行人运动的不确定性，进而产生更精准、可解释的预测结果，为后续规划模块提供了参考。

Abstract: Predicting pedestrian motion trajectories is critical for path planning and
motion control of autonomous vehicles. However, accurately forecasting crowd
trajectories remains a challenging task due to the inherently multimodal and
uncertain nature of human motion. Recent diffusion-based models have shown
promising results in capturing the stochasticity of pedestrian behavior for
trajectory prediction. However, few diffusion-based approaches explicitly
incorporate the underlying motion intentions of pedestrians, which can limit
the interpretability and precision of prediction models. In this work, we
propose a diffusion-based multimodal trajectory prediction model that
incorporates pedestrians' motion intentions into the prediction framework. The
motion intentions are decomposed into lateral and longitudinal components, and
a pedestrian intention recognition module is introduced to enable the model to
effectively capture these intentions. Furthermore, we adopt an efficient
guidance mechanism that facilitates the generation of interpretable
trajectories. The proposed framework is evaluated on two widely used human
trajectory prediction benchmarks, ETH and UCY, on which it is compared against
state-of-the-art methods. The experimental results demonstrate that our method
achieves competitive performance.

</details>


### [76] [DocVCE: Diffusion-based Visual Counterfactual Explanations for Document Image Classification](https://arxiv.org/abs/2508.04233)
*Saifullah Saifullah,Stefan Agne,Andreas Dengel,Sheraz Ahmed*

Main category: cs.CV

TL;DR: 本文提出了一种名为DocVCE的新方法，用于生成文档图像的对抗性解释，以增强黑匣子AI文档处理系统的透明度和可靠性。该方法利用潜扩散模型和分层块级细化来创建可视化对抗解释，从而提供对文档分类模型决策的深入洞察。


<details>
  <summary>Details</summary>
Motivation: 由于AI驱动的文档处理系统在高风险应用中的广泛使用，其决策过程缺乏透明性可能导致严重问题。现有的特征重要性图解释方法难以理解且无法展示模型学习的全局特征，因此需要一种更有效的解释方法。

Method: 首先，DocVCE利用潜扩散模型结合分类器引导生成符合分布的视觉对抗示例；其次，通过分层块级细化搜索与原始目标图像最接近的细化对抗样本。

Result: 在RVL-CDIP、Tobacco3482和DocLayNet三个数据集上，对ResNet、ConvNeXt和DiT三种模型进行了严格评估，结果表明该方法在有效性、接近性和真实性方面表现优异。

Conclusion: DocVCE首次将生成对抗解释引入文档图像分析领域，通过可视化对抗解释提供了对模型决策的有意义洞察，从而增强了文档分类模型的透明性和可靠性。

Abstract: As black-box AI-driven decision-making systems become increasingly widespread
in modern document processing workflows, improving their transparency and
reliability has become critical, especially in high-stakes applications where
biases or spurious correlations in decision-making could lead to serious
consequences. One vital component often found in such document processing
workflows is document image classification, which, despite its widespread use,
remains difficult to explain. While some recent works have attempted to explain
the decisions of document image classification models through
feature-importance maps, these maps are often difficult to interpret and fail
to provide insights into the global features learned by the model. In this
paper, we aim to bridge this research gap by introducing generative document
counterfactuals that provide meaningful insights into the model's
decision-making through actionable explanations. In particular, we propose
DocVCE, a novel approach that leverages latent diffusion models in combination
with classifier guidance to first generate plausible in-distribution visual
counterfactual explanations, and then performs hierarchical patch-wise
refinement to search for a refined counterfactual that is closest to the target
factual image. We demonstrate the effectiveness of our approach through a
rigorous qualitative and quantitative assessment on 3 different document
classification datasets -- RVL-CDIP, Tobacco3482, and DocLayNet -- and 3
different models -- ResNet, ConvNeXt, and DiT -- using well-established
evaluation criteria such as validity, closeness, and realism. To the best of
the authors' knowledge, this is the first work to explore generative
counterfactual explanations in document image analysis.

</details>


### [77] [A machine learning approach for image classification in synthetic aperture RADAR](https://arxiv.org/abs/2508.04234)
*Romina Gaburro,Patrick Healy,Shraddha Naidu,Clifford Nolan*

Main category: cs.CV

TL;DR: 摘要讨论了使用卷积神经网络（CNNs）处理合成孔径雷达（SAR）数据的问题，旨在识别和分类地面物体形状，并通过单散射近似方法对模拟SAR数据和重建图像进行分类效果比较。实验表明在模拟和现实场景（哨兵1号卫星的冰分类）中都能达到75%以上的分类准确率，并探讨了不同天线高度获取数据对分类能力的影响。


<details>
  <summary>Details</summary>
Motivation: 合成孔径雷达（SAR）在目标识别领域具有独特优势（如全天候工作能力），但传统方法在处理复杂SAR数据时效果有限。本文旨在探索深度学习（特别是CNNs）在利用SAR数据进行几何形状分类和环境分类（如区分冰的类型）方面的有效性和鲁棒性。

Method: 1.使用单散射近似的数学模型生成模拟SAR数据。2.分别构建基于模拟原始SAR数据的CNN分类模型和基于重建图像（如后向投影图像）的CNN模型，并进行比较实验。3.在现实卫星（Sentinel-1）获得的SAR数据上进行冰类型分类实验。4.额外分析SAR数据采集时不同天线高度对分类精度的影响。

Result: 1.在模拟目标形状分类任务上，原始SAR数据和重建图像上的CNN均表现良好（准确率≥75%）。2.在Sentinel-1卫星获取的现实SAR数据上对冰类型进行识别，同样达到75%以上的高准确率。3.实验发现原始SAR数据比重建图像更适合CNNs分类，推测是因为原始数据保留了更完整的散射特征。4.发现天线高度会影响分类效果，需在后续工程实践中优化硬件布局。

Conclusion: 验证了CNNs处理两类SAR任务的可行性：几何形状分类（模拟物体）和材料分类（真实冰层）。原始数据更优的结果表明：CNNs能绕过图像重建步骤，直接从原始相位信息中提取特征。未来工作可优化模拟数据生成、改进网络结构，并将方法扩展到移动平台数据采集系统的高度调节优化。

Abstract: We consider the problem in Synthetic Aperture RADAR (SAR) of identifying and
classifying objects located on the ground by means of Convolutional Neural
Networks (CNNs). Specifically, we adopt a single scattering approximation to
classify the shape of the object using both simulated SAR data and
reconstructed images from this data, and we compare the success of these
approaches. We then identify ice types in real SAR imagery from the satellite
Sentinel-1. In both experiments we achieve a promising high classification
accuracy ($\geq$75\%). Our results demonstrate the effectiveness of CNNs in
using SAR data for both geometric and environmental classification tasks. Our
investigation also explores the effect of SAR data acquisition at different
antenna heights on our ability to classify objects successfully.

</details>


### [78] [FrEVL: Leveraging Frozen Pretrained Embeddings for Efficient Vision-Language Understanding](https://arxiv.org/abs/2508.04469)
*Emmanuelle Bourigault,Pauline Bourigault*

Main category: cs.CV

TL;DR: 该论文提出了一个名为 FrEVL 的框架，研究冻结预训练嵌入是否能在视觉语言理解任务中有效使用。结果表明冻结嵌入在判别任务上能达到 SOTA 性能的 85-95%，同时减少了可训练参数量和计算需求。当预训练目标与下游任务对齐良好时，冻结嵌入特别高效，该框架在速度和能耗上有显著优势。


<details>
  <summary>Details</summary>
Motivation: 目前的视觉语言模型因高计算资源需求限制了实际部署。本文旨在探索冻结的预训练嵌入在保持性能的同时，降低计算开销的可能性。通过 FrEVL 框架验证冻结嵌入的可用性与效率。

Method: 设计实验分析冻结预训练嵌入在多个标准视觉语言任务上的表现，包括与下游任务的匹配度测试。建立性能评估指标，并对比传统微调方法在计算量、可训练参数量等方面的差异。

Result: 实验显示冻结嵌入在判别任务达到 SOTA 性能的 85%-95%（仅需680万个可训练参数）。框架的端到端计算加速2.3倍，能耗降低52%。同时发现性能高度依赖预训练目标与下游任务的匹配性。

Conclusion: FrEVL 证明在预训练目标与任务匹配的情况下，冻结嵌入是高性能、高能效的替代方案，适用于输入可预计算资源受限场景。该研究为工业部署提供了实用指导，并将开源工具以推动高效多模态研究。

Abstract: The deployment of vision-language models remains constrained by substantial
computational requirements. We present \textbf{FrEVL}, a framework exploring
whether frozen pretrained embeddings can support effective vision-language
understanding. Our analysis reveals that frozen embeddings contain rich
information for discriminative tasks, achieving 85\% to 95\% of
state-of-the-art performance on standard benchmarks with only 68.4M trainable
parameters. This performance dichotomy reveals a critical insight: frozen
embedding effectiveness depends on alignment between pretraining objectives and
downstream task requirements. When accounting for end-to-end computation
including embedding extraction, FrEVL provides $2.3\times$ speedup with 52\%
lower energy consumption, making it suitable for scenarios with pre-computable
inputs or when deployment constraints outweigh marginal performance gains. Our
evaluation provides practitioners with guidance on when frozen embedding
approaches represent viable alternatives to full model deployment. We will
release our complete implementation and evaluation framework to facilitate
further research into efficient multi-modal understanding.

</details>


### [79] [PIS3R: Very Large Parallax Image Stitching via Deep 3D Reconstruction](https://arxiv.org/abs/2508.04236)
*Muhua Zhu,Xinhao Jin,Chengbo Wang,Yongcong Zhang,Yifei Xue,Tie Ji,Yizhen Lao*

Main category: cs.CV

TL;DR: 提出了一种名为PIS3R的图像拼接方法，通过深度三维重建技术解决大视差图像拼接问题，包括利用视觉几何变换器进行相机参数估计和三维重建、重投影生成初始拼接图，以及基于点条件的图像扩散模块细化结果，实验表明其在大视差场景下的性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当三维场景存在深度变化且相机基线较大时，视差效应会导致现有图像拼接方法难以处理。本文旨在解决大视差下的图像拼接挑战，确保几何完整性以满足下游三维视觉任务需求。

Method: 1. 使用视觉几何变换器从输入图像中估计相机内外参并重建稠密三维点云；2. 将点云重投影至参考视角生成初始拼接图；3. 设计点条件图像扩散模块修复初始图中的空洞/噪声，输出优化结果。

Result: 所提PIS3R方法对大视差具有强鲁棒性，能完整保留像素的几何信息，适用于SfM等三维任务；在定性与定量实验中均超越现有方法。

Conclusion: 基于深度三维重建的PIS3R框架有效解决了大视差图像拼接问题，其重建-重投影-细化的流程兼顾几何精度与视觉效果，为下游三维分析提供可直接使用的数据基础。

Abstract: Image stitching aim to align two images taken from different viewpoints into
one seamless, wider image. However, when the 3D scene contains depth variations
and the camera baseline is significant, noticeable parallax occurs-meaning the
relative positions of scene elements differ substantially between views. Most
existing stitching methods struggle to handle such images with large parallax
effectively. To address this challenge, in this paper, we propose an image
stitching solution called PIS3R that is robust to very large parallax based on
the novel concept of deep 3D reconstruction. First, we apply visual geometry
grounded transformer to two input images with very large parallax to obtain
both intrinsic and extrinsic parameters, as well as the dense 3D scene
reconstruction. Subsequently, we reproject reconstructed dense point cloud onto
a designated reference view using the recovered camera parameters, achieving
pixel-wise alignment and generating an initial stitched image. Finally, to
further address potential artifacts such as holes or noise in the initial
stitching, we propose a point-conditioned image diffusion module to obtain the
refined result.Compared with existing methods, our solution is very large
parallax tolerant and also provides results that fully preserve the geometric
integrity of all pixels in the 3D photogrammetric context, enabling direct
applicability to downstream 3D vision tasks such as SfM. Experimental results
demonstrate that the proposed algorithm provides accurate stitching results for
images with very large parallax, and outperforms the existing methods
qualitatively and quantitatively.

</details>


### [80] [From eye to AI: studying rodent social behavior in the era of machine Learning](https://arxiv.org/abs/2508.04255)
*Giuseppe Chindemi,Camilla Bellone,Benoit Girard*

Main category: cs.CV

TL;DR: 本文综述了人工智能和机器学习在啮齿类动物社会行为研究中的应用，探讨了传统方法的局限性和现代计算方法的优势，以及整合过程中面临的挑战，并提出了实用的解决方案。


<details>
  <summary>Details</summary>
Motivation: 啮齿类动物社会行为的研究正从依赖人工观察转向结合人工智能和机器学习的计算化方法。传统方法存在偏差且难以捕捉社会互动的复杂性，而现代方法则能提供多层次的行为见解，这对社会神经科学尤为重要。然而，AI的整合也带来了挑战。

Method: 文章通过梳理分析啮齿类动物社会行为的主要步骤和可用工具，比较了传统与现代方法的优缺点，并针对常见阻碍提出了实际解决方案。

Result: 该研究指出了整合AI在啮齿动物社会行为研究中的潜力，同时也明确了如数据需求、算法透明度和伦理等挑战。

Conclusion: 本文旨在指导年轻研究者采用AI方法，并促进专家间关于这些工具在科研中不断变化的需求的讨论。文中强调了实际经验在工具开发中的重要性，并指出未来的发展方向应包括更多的实验验证和标准化共享平台的建设。

Abstract: The study of rodent social behavior has shifted in the last years from
relying on direct human observation to more nuanced approaches integrating
computational methods in artificial intelligence (AI) and machine learning.
While conventional approaches introduce bias and can fail to capture the
complexity of rodent social interactions, modern approaches bridging computer
vision, ethology and neuroscience provide more multifaceted insights into
behavior which are particularly relevant to social neuroscience. Despite these
benefits, the integration of AI into social behavior research also poses
several challenges. Here we discuss the main steps involved and the tools
available for analyzing rodent social behavior, examining their advantages and
limitations. Additionally, we suggest practical solutions to address common
hurdles, aiming to guide young researchers in adopting these methods and to
stimulate further discussion among experts regarding the evolving requirements
of these tools in scientific applications.

</details>


### [81] [Segment Any Vehicle: Semantic and Visual Context Driven SAM and A Benchmark](https://arxiv.org/abs/2508.04260)
*Xiao Wang,Ziwen Wang,Wentao Wu,Anjie Wang,Jiashu Wu,Yantao Pan,Chenglong Li*

Main category: cs.CV

TL;DR: 提出了一种名为SAV的新框架，用于车辆部件分割。该框架包括一个SAM基础编码器-解码器、车辆部件知识图谱和上下文样本检索编码模块，同时引入了一个名为VehicleSeg10K的大规模数据集。该方法解决了SAM模型不能直接用于车辆部件分割的问题，通过结构化知识图谱和上下文检索来提升分割性能。


<details>
  <summary>Details</summary>
Motivation: 由于SAM在文本提示分割功能上尚未开放，且其生成的掩码区域缺乏语义标签，无法直接用于结构化、细粒度的车辆部件分割任务。因此需要设计一种能够结合车辆部件结构化知识和上下文信息的方法。

Method: 1. 使用基于SAM的编码器解码器作为基础架构。2. 构建车辆部件知识图谱，显式建模部件之间的空间和几何关系。3. 设计上下文样本检索编码模块，通过从训练数据中检索相似实例提供上下文先验。4. 训练和评估在大规模数据集VehicleSeg10K上进行。

Result: 在VehicleSeg10K数据集以及其他两个数据集上进行了全面实验，并评估了多个基准方法，为未来的研究提供了基础和比较依据。具体性能指标在摘要中未提及，但论文接受后将发布数据集和代码。

Conclusion: SAV框架通过结合车辆部件知识图谱和上下文检索，有效改善了车辆部件分割效果，解决了SAM不能直接应用于细粒度分割任务的问题。同时，新构建的数据集将为该领域提供重要资源。

Abstract: With the rapid advancement of autonomous driving, vehicle perception,
particularly detection and segmentation, has placed increasingly higher demands
on algorithmic performance. Pre-trained large segmentation models, especially
Segment Anything Model (SAM), have sparked significant interest and inspired
new research directions in artificial intelligence. However, SAM cannot be
directly applied to the fine-grained task of vehicle part segmentation, as its
text-prompted segmentation functionality is not publicly accessible, and the
mask regions generated by its default mode lack semantic labels, limiting its
utility in structured, category-specific segmentation tasks. To address these
limitations, we propose SAV, a novel framework comprising three core
components: a SAM-based encoder-decoder, a vehicle part knowledge graph, and a
context sample retrieval encoding module. The knowledge graph explicitly models
the spatial and geometric relationships among vehicle parts through a
structured ontology, effectively encoding prior structural knowledge.
Meanwhile, the context retrieval module enhances segmentation by identifying
and leveraging visually similar vehicle instances from training data, providing
rich contextual priors for improved generalization. Furthermore, we introduce a
new large-scale benchmark dataset for vehicle part segmentation, named
VehicleSeg10K, which contains 11,665 high-quality pixel-level annotations
across diverse scenes and viewpoints. We conduct comprehensive experiments on
this dataset and two other datasets, benchmarking multiple representative
baselines to establish a solid foundation for future research and comparison. %
Both the dataset and source code of this paper will be released upon
acceptance. Both the dataset and source code of this paper will be released on
https://github.com/Event-AHU/SAV

</details>


### [82] [Analyzing and Mitigating Object Hallucination: A Training Bias Perspective](https://arxiv.org/abs/2508.04567)
*Yifan Li,Kun Zhou,Wayne Xin Zhao,Lei Fang,Ji-Rong Wen*

Main category: cs.CV

TL;DR: 提出了POPEv2基准和Obliviate方法，系统研究训练数据在大型视觉语言模型（LVLM）幻觉现象中的作用，并提出高效的反向学习方案缓解问题。


<details>
  <summary>Details</summary>
Motivation: 尽管大规模训练数据提升了多模态能力，但LVLMs仍存在与视觉输入不一致的幻觉问题。希望通过分析训练数据在幻觉中的作用，找到解决方案。

Method: 1) 构建包含原始训练图片掩码反事实版本的POPEv2基准
2) 分析训练偏差现象与语言建模头（LM head）的关系
3) 提出Obliviate方法：仅微调LM Head（约2%参数），利用训练数据中真实标签与模型输出的差异作为偏差代理进行反向学习

Result: 1) 发现LVLMs在训练数据上存在偏差导致的明显幻觉现象
2) Obliviate在判别和生成任务中均显著减少幻觉
3) 在2B-72B模型规模、不同训练数据量下均保持高效性和强扩展性
4) 能泛化至非物体级幻觉类型

Conclusion: 训练数据偏差是LVLM幻觉的重要诱因；通过局部参数的反向学习可高效缓解幻觉问题。POPEv2基准与Obliviate方案为后续研究提供新工具。

Abstract: As scaling up training data has significantly improved the general multimodal
capabilities of Large Vision-Language Models (LVLMs), they still suffer from
the hallucination issue, generating text that is inconsistent with the visual
input. This phenomenon motivates us to systematically investigate the role of
training data in hallucination. We introduce a new benchmark, POPEv2, which
consists of counterfactual images collected from the training data of LVLMs
with certain objects masked. Through comprehensive evaluation on POPEv2, we
find that current LVLMs suffer from training bias: they fail to fully leverage
their training data and hallucinate more frequently on images seen during
training. Specifically, they perform poorly on counterfactual images, often
incorrectly answering ``Yes'' to questions about masked objects. To understand
this issue, we conduct probing experiments on the models' internal components,
revealing that this training bias is primarily located in the language modeling
(LM) head. Based on these findings, we propose Obliviate, an efficient and
lightweight unlearning method designed to mitigate object hallucination via
training bias unlearning. Obliviate identifies the discrepancy between
ground-truth labels and model outputs on the training data as a proxy for bias
and adopts a parameter- and data-efficient fine-tuning strategy that only
updates the LM head. Extensive experiments demonstrate the effectiveness of our
approach. While only reusing the training data and updating approximately 2\%
of the parameters, Obliviate significantly reduces hallucination across both
discriminative and generative tasks. Furthermore, it demonstrates strong
scalability with respect to both model size (2B to 72B) and training data
volume, and exhibits promising generalization to hallucination types beyond
object-level hallucination. Our code and data will be publicly released.

</details>


### [83] [Revisiting Continual Semantic Segmentation with Pre-trained Vision Models](https://arxiv.org/abs/2508.04267)
*Duzhen Zhang,Yong Ren,Wei Cong,Junhao Zheng,Qiaoyi Su,Shuncheng Jia,Zhong-Zhi Li,Xuanle Zhao,Ye Bai,Feilong Chen,Qi Tian,Tielin Zhang*

Main category: cs.CV

TL;DR: 论文重新评估了直接微调（DFT）在连续语义分割（CSS）任务中的表现，挑战了此前认为DFT会因为灾难性遗忘而表现不佳的假设。研究发现，预训练视觉模型（PVMs）在DFT下仍能保留大部分先前学到的知识，遗忘主要源于分类器的偏移而非主干网络的退化。基于此，作者提出了DFT*，一种简单的DFT增强方法，通过冻结主干和已学分类器以及预分配未来分类器来提升性能。实验表明DFT*在多个设置和数据集上超越了现有的SOTA方法，且具有更高的效率。


<details>
  <summary>Details</summary>
Motivation: 现有研究普遍认为直接微调（DFT）在连续语义分割（CSS）任务中会因灾难性遗忘而表现不佳，因此被视作性能下限。然而，本文作者通过系统实验发现在两种主流预训练视觉模型（PVMs）和八个CSS设置下，DFT的表现远超预期，PVMs在DFT中表现出内在的抗遗忘特性。基于这一观察，作者旨在重新评估DFT的有效性，并探索其改进空间，从而设计出更简单高效的方法。

Method: 首先，论文通过详细的探测分析（probing analysis）重新检验了DFT在Pascal VOC 2012和ADE20K数据集上的表现，使用ResNet101和Swin-B作为主干网络。发现了PVMs在DFT下遗忘较小的特性，指出主要问题在于分类器的偏移（drift）而非主干表示退化。据此，作者提出了DFT*：1）冻结PVM主干；2）冻结已学习的分类器；3）为未来步骤预分配分类器权重。DFT*仅需微调当前阶段的新分类器和解码器少量参数，大大减少了计算负担。

Result: 实验结果显示：1）DFT在多个CSS设置下明显优于现有方法，突破了此前对其为性能下限的认知；2）DFT*进一步提升了性能，在16个SOTA方法中表现领先，最高超过第二名1.84%～9.17% mIoU；3）DFT*比SOTA方法减少可训练参数最高达99%，训练时间缩短最高达90%以上。这些结果验证了PVMs在DFT框架中的抗遗忘性以及所提方法的有效性。

Conclusion: 本文挑战了CSS领域对DFT的传统认知，证明PVMs在直接微调下具有内在抗遗忘性。作者发现分类器偏移是主要遗忘源，进而提出DFT*——一种只需简单修正的强基线方法。这不仅重新定义了CSS任务的性能下限，也为未来设计更高效方法提供了新思路：应更多关注分类器稳定性而非复杂遗忘抑制结构。

Abstract: Continual Semantic Segmentation (CSS) seeks to incrementally learn to segment
novel classes while preserving knowledge of previously encountered ones. Recent
advancements in CSS have been largely driven by the adoption of Pre-trained
Vision Models (PVMs) as backbones. Among existing strategies, Direct
Fine-Tuning (DFT), which sequentially fine-tunes the model across classes,
remains the most straightforward approach. Prior work often regards DFT as a
performance lower bound due to its presumed vulnerability to severe
catastrophic forgetting, leading to the development of numerous complex
mitigation techniques. However, we contend that this prevailing assumption is
flawed. In this paper, we systematically revisit forgetting in DFT across two
standard benchmarks, Pascal VOC 2012 and ADE20K, under eight CSS settings using
two representative PVM backbones: ResNet101 and Swin-B. Through a detailed
probing analysis, our findings reveal that existing methods significantly
underestimate the inherent anti-forgetting capabilities of PVMs. Even under
DFT, PVMs retain previously learned knowledge with minimal forgetting. Further
investigation of the feature space indicates that the observed forgetting
primarily arises from the classifier's drift away from the PVM, rather than
from degradation of the backbone representations. Based on this insight, we
propose DFT*, a simple yet effective enhancement to DFT that incorporates
strategies such as freezing the PVM backbone and previously learned
classifiers, as well as pre-allocating future classifiers. Extensive
experiments show that DFT* consistently achieves competitive or superior
performance compared to sixteen state-of-the-art CSS methods, while requiring
substantially fewer trainable parameters and less training time.

</details>


### [84] [PKSS-Align: Robust Point Cloud Registration on Pre-Kendall Shape Space](https://arxiv.org/abs/2508.04286)
*Chenlei Lv,Hui Huang*

Main category: cs.CV

TL;DR: 提出了一个鲁棒的点云配准方法PKSS-Align，能够在相似变换、非均匀密度、随机噪声和部分缺陷等影响下有效工作。该方法在Pre-Kendall形状空间（PKSS）上测量点云间基于形状特征的相似性，该度量对欧几里得坐标系统中的各种表示具有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 点云配准任务对相似变换（平移、缩放、旋转）、噪声点和不完整的几何结构敏感，尤其是非均匀尺度和点云的部分缺陷会增加陷入局部最优的概率。为了解决这些问题，提出了PKSS-Align方法。

Method: 在Pre-Kendall形状空间（PKSS）上，使用基于形状特征的相似性度量，该度量不需要点对点或点对面的对应关系，并作为流形度量对欧几里得坐标系中的各种表示具有鲁棒性。通过该度量，直接为受多种影响（包括相似变换、非均匀密度、噪声和缺陷）的点云生成变换矩阵。不需要数据训练和复杂的特征编码，通过简单的并行加速提高效率和实用性。

Result: 实验表明，该方法在效率与可行性方面取得了显著改进，并优于相关state-of-the-art方法。

Conclusion: PKSS-Align是一种高效且鲁棒的点云配准方法，能够同时处理多种干扰因素，无需训练和复杂特征提取，通过基于形状的度量在PKSS空间实现有效配准。适用于存在非均匀密度、噪声和部分缺陷的点云数据。

Abstract: Point cloud registration is a classical topic in the field of 3D Vision and
Computer Graphics. Generally, the implementation of registration is typically
sensitive to similarity transformations (translation, scaling, and rotation),
noisy points, and incomplete geometric structures. Especially, the non-uniform
scales and defective parts of point clouds increase probability of struck local
optima in registration task. In this paper, we propose a robust point cloud
registration PKSS-Align that can handle various influences, including
similarity transformations, non-uniform densities, random noisy points, and
defective parts. The proposed method measures shape feature-based similarity
between point clouds on the Pre-Kendall shape space (PKSS),
\textcolor{black}{which is a shape measurement-based scheme and doesn't require
point-to-point or point-to-plane metric.} The employed measurement can be
regarded as the manifold metric that is robust to various representations in
the Euclidean coordinate system. Benefited from the measurement, the
transformation matrix can be directly generated for point clouds with mentioned
influences at the same time. The proposed method does not require data training
and complex feature encoding. Based on a simple parallel acceleration, it can
achieve significant improvement for efficiency and feasibility in practice.
Experiments demonstrate that our method outperforms the relevant
state-of-the-art methods.

</details>


### [85] [MuGS: Multi-Baseline Generalizable Gaussian Splatting Reconstruction](https://arxiv.org/abs/2508.04297)
*Yaopeng Lou,Liao Shen,Tianqi Liu,Jiaqi Li,Zihao Huang,Huiqiang Sun,Zhiguo Cao*

Main category: cs.CV

TL;DR: 提出Multi-Baseline Gaussian Splatting (MuRF)方法，用于处理不同基线设置（包括稀疏输入视图中大小基线）的新视角合成。通过结合MVS和MDE特征增强泛化重建，提出投影采样机制进行深度融合以构建精细概率体积，引入参考视图损失改进几何和优化效率，利用3D高斯表示加速训练推理并提升渲染质量。在多个数据集上达到SOTA，并展示在LLFF和Mip-NeRF 360上的零样本性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理多样基线（如稀疏视图中大小基线差异）的新视角合成时存在挑战，需要一种能泛化到不同基线设置的高效方法。

Method: 1. 结合MVS和MDE特征增强特征表示；2. 提出投影采样机制进行深度融合，构建概率体积指导特征图回归；3. 引入参考视图损失改进几何和优化效率；4. 使用3D高斯表示加速训练推理。

Result: 在DTU、RealEstate10K等数据集上取得SOTA；在LLFF和Mip-NeRF 360上展示零样本性能。

Conclusion: MuRF通过融合多源特征、创新深度融合机制和参考视图损失，有效处理多样基线设置的新视角合成，在质量和效率上均有提升，并具备良好泛化能力。

Abstract: We present Multi-Baseline Gaussian Splatting (MuRF), a generalized
feed-forward approach for novel view synthesis that effectively handles diverse
baseline settings, including sparse input views with both small and large
baselines. Specifically, we integrate features from Multi-View Stereo (MVS) and
Monocular Depth Estimation (MDE) to enhance feature representations for
generalizable reconstruction. Next, We propose a projection-and-sampling
mechanism for deep depth fusion, which constructs a fine probability volume to
guide the regression of the feature map. Furthermore, We introduce a
reference-view loss to improve geometry and optimization efficiency. We
leverage 3D Gaussian representations to accelerate training and inference time
while enhancing rendering quality. MuRF achieves state-of-the-art performance
across multiple baseline settings and diverse scenarios ranging from simple
objects (DTU) to complex indoor and outdoor scenes (RealEstate10K). We also
demonstrate promising zero-shot performance on the LLFF and Mip-NeRF 360
datasets.

</details>


### [86] [Length Matters: Length-Aware Transformer for Temporal Sentence Grounding](https://arxiv.org/abs/2508.04299)
*Yifan Wang,Ziyi Liu,Xiaolong Sun,Jiawei Wang,Hongmin Liu*

Main category: cs.CV

TL;DR: 本文介绍了一种名为LATR（Length-Aware Transformer）的模型，旨在解决时序句法定位（TSG）任务中DETR架构查询重叠的问题。通过利用视频描述对的长度先验，LATR为不同长度的视频段落分配不同的查询组别，并通过长度分类任务使各查询专门化，从而提升了性能并在三大公开基准上取得领先结果。


<details>
  <summary>Details</summary>
Motivation: 现有的基于DETR的TSG模型因缺乏明确监督，导致学习的查询角色重叠，产生冗余预测。为提升TSG性能，作者认为应引导每个查询专注于其指定的角色（特别是基于不同时长段落），从而减少冗余并增强模型的区分能力。

Method: LATR（Length-Aware Transformer）结构将全部查询分成三组，分别负责短、中、长三个时间段的预测。模型引入额外的长度分类分支，在训练中抑制长度不匹配查询的预测结果，强化各查询组的专业性。此方法将视频文本对的长度先验显式融入TSG优化过程。

Result: 在三大公开基准测试上，LATR均达到最先进（SOTA）性能。消融实验验证了模型各组件的有效性，并证实了长度先验在TSG任务中的关键性作用。

Conclusion: 显式利用视频描述对的长度先验能优化DETR模型的查询分工，减少冗余预测并提升TSG精度。LATR的模块化设计可推广到其他基于查询的时空定位任务中。

Abstract: Temporal sentence grounding (TSG) is a highly challenging task aiming to
localize the temporal segment within an untrimmed video corresponding to a
given natural language description. Benefiting from the design of learnable
queries, the DETR-based models have achieved substantial advancements in the
TSG task. However, the absence of explicit supervision often causes the learned
queries to overlap in roles, leading to redundant predictions. Therefore, we
propose to improve TSG by making each query fulfill its designated role,
leveraging the length priors of the video-description pairs. In this paper, we
introduce the Length-Aware Transformer (LATR) for TSG, which assigns different
queries to handle predictions based on varying temporal lengths. Specifically,
we divide all queries into three groups, responsible for segments with short,
middle, and long temporal durations, respectively. During training, an
additional length classification task is introduced. Predictions from queries
with mismatched lengths are suppressed, guiding each query to specialize in its
designated function. Extensive experiments demonstrate the effectiveness of our
LATR, achieving state-of-the-art performance on three public benchmarks.
Furthermore, the ablation studies validate the contribution of each component
of our method and the critical role of incorporating length priors into the TSG
task.

</details>


### [87] [A Foundation Model for DAS Signal Recognition and Visual Prompt Tuning of the Pre-trained Model for Downstream Tasks](https://arxiv.org/abs/2508.04316)
*Kun Gui,Hongliang Ren,Shang Shi,Jin Lu,Changqiu Yu,Quanjun Cao,Guomin Gu,Qi Xuan*

Main category: cs.CV

TL;DR: 该研究提出了一种名为MAEPD的基于掩码自编码器的分布式声学传感（DAS）信号识别基础模型。该模型通过自监督掩码重建任务在包含多个DAS信号领域的63.5万个样本上进行预训练，并采用视觉提示调整方法进行下游微调。实验证明该方法在参数微调量极小（0.322%）的情况下，分类准确率超过传统全微调方法，且训练时间减少45%，展示出良好的泛化性和效率。


<details>
  <summary>Details</summary>
Motivation: DAS技术在多领域应用中面临数据分布差异导致的跨域泛化能力不足问题，以及标记训练数据稀缺的挑战。为解决这些问题，需建立一个能适应多领域信号识别的通用基础模型。

Method: 1. 基于掩码自编码器构建MAEPD基础模型，在63.5万样本的多领域DAS信号（包括步态时空信号、周界安防2D图像、管道泄漏时频图、公开数据集等）上进行自监督预训练；
2. 在下游任务中采用视觉提示调整：冻结预训练骨干参数，仅在Transformer编码器层插入少量可学习视觉提示向量进行微调。

Result: 1. 在步态识别任务中，仅微调0.322%参数的VPT-Deep方法达到96.94%准确率（较传统全微调提升0.61%），训练时间减少45%；
2. 在管道泄漏检测任务中同样展现强劲性能；
3. 证明MAEPD具备泛化性、高效性和可扩展性。

Conclusion: MAEPD通过自监督预训练和视觉提示调整，有效解决DAS领域信号识别模型泛化受限问题，为跨域迁移提供新范式。极小参数量的下游微调在保证模型性能的同时大幅提升训练效率。

Abstract: Distributed Acoustic Sensing (DAS) technology finds growing applications
across various domains. However, data distribution disparities due to
heterogeneous sensing environments pose challenges for data-driven artificial
intelligence (AI) models, limiting cross-domain generalization and facing a
shortage of labeled training data. To address these issues, this study proposes
a foundational model for DAS signal recognition based on a Masked Autoencoder,
named MAEPD. The MAEPD model is pretrained on a dataset of 635,860 samples,
encompassing DAS gait spatiotemporal signals, 2D GASF images for perimeter
security, 2D time-frequency images for pipeline leakage, and open-dataset
signals including whale vocalizations and seismic activities, using a
self-supervised mask reconstruction task to capture deep semantic features of
DAS signals. Visual Prompt Tuning (VPT) is employed for downstream recognition
tasks. This method freezes the pretrained backbone parameters and fine-tunes
only a small set of learnable visual prompt vectors inserted into the
Transformer encoder layers. Experiments on the NVIDIA GeForce RTX 4080 Super
platform validate MAEPD using indoor gait recognition as a downstream task. The
VPT-Deep approach achieves a classification accuracy of 96.94% with just 0.322%
of parameters fine-tuned, surpassing the traditional Full Fine Tuning (FFT)
method by 0.61% and reducing training time by 45%. The model also exhibits
robust performance in pipeline leakage detection, confirming the generality,
efficiency, and scalability of MAEPD as a foundational model. This approach
offers a novel paradigm for addressing the limited generalization of signal
recognition models in the DAS domain.

</details>


### [88] [TempFlow-GRPO: When Timing Matters for GRPO in Flow Models](https://arxiv.org/abs/2508.04324)
*Xiaoxuan He,Siming Fu,Yuke Zhao,Wanli Li,Jian Yang,Dacheng Yin,Fengyun Rao,Bo Zhang*

Main category: cs.CV

TL;DR: 论文TempFlow-GRPO通过解决现有流模型中奖励优化面临的信用分配问题，提出了一个带时间感知的强化学习对齐框架。


<details>
  <summary>Details</summary>
Motivation: 流模型在文本到图像生成中表现出色，但在结合强化学习优化人类偏好时存在不足。主要障碍是现有方法假设所有生成时间步的决策贡献相同，这导致了稀疏奖励下信用分配不足和探索效率低。

Method: 引入TempFlow-GRPO框架，包括两项创新：轨迹分支机制在关键时间点引入噪声以提供中间过程奖励，无需额外奖励模型；噪声感知权重方案根据时间步的重要性调节优化权重，关注早期关键阶段同时稳定后期生成。

Result: 方法在人类偏好对齐和多个文本到图像生成基准上取得了最先进的结果，实现了更高效的学习过程。

Conclusion: 模型通过赋予时间感知的优化策略，更好地捕获了生成模型的时序动态，提升了强化学习优化的性能。

Abstract: Recent flow matching models for text-to-image generation have achieved
remarkable quality, yet their integration with reinforcement learning for human
preference alignment remains suboptimal, hindering fine-grained reward-based
optimization. We observe that the key impediment to effective GRPO training of
flow models is the temporal uniformity assumption in existing approaches:
sparse terminal rewards with uniform credit assignment fail to capture the
varying criticality of decisions across generation timesteps, resulting in
inefficient exploration and suboptimal convergence. To remedy this shortcoming,
we introduce \textbf{TempFlow-GRPO} (Temporal Flow GRPO), a principled GRPO
framework that captures and exploits the temporal structure inherent in
flow-based generation. TempFlow-GRPO introduces two key innovations: (i) a
trajectory branching mechanism that provides process rewards by concentrating
stochasticity at designated branching points, enabling precise credit
assignment without requiring specialized intermediate reward models; and (ii) a
noise-aware weighting scheme that modulates policy optimization according to
the intrinsic exploration potential of each timestep, prioritizing learning
during high-impact early stages while ensuring stable refinement in later
phases. These innovations endow the model with temporally-aware optimization
that respects the underlying generative dynamics, leading to state-of-the-art
performance in human preference alignment and standard text-to-image
benchmarks.

</details>


### [89] [RotatedMVPS: Multi-view Photometric Stereo with Rotated Natural Light](https://arxiv.org/abs/2508.04366)
*Songyun Yang,Yufei Han,Jilong Zhang,Kongming Liang,Peng Yu,Zhaowei Qu,Heng Guo*

Main category: cs.CV

TL;DR: 提出RotatedMVPS方法，利用旋转台实现自然光下的形状和反射率恢复，通过光一致性减少环境光复杂性，并整合单视图方法的数据先验提升精度。


<details>
  <summary>Details</summary>
Motivation: 现有MVPS方法需暗室环境控制光照或忽略反射率/光照恢复，限制其在自然光照和反向渲染任务的应用。

Method: 1) 通过物体旋转确保不同姿态的光照一致性，减少环境光未知性；2) 整合现成单视图光测立体法的数据先验到MVPS框架，优化形状/反射率恢复。

Result: 在合成与真实数据集上验证了方法在自然光照下恢复高保真形状和反射率的有效性。

Conclusion: RotatedMVPS为自然光照下物体重建提供了实用解决方案，同时支持反向渲染下游任务。

Abstract: Multiview photometric stereo (MVPS) seeks to recover high-fidelity surface
shapes and reflectances from images captured under varying views and
illuminations. However, existing MVPS methods often require controlled darkroom
settings for varying illuminations or overlook the recovery of reflectances and
illuminations properties, limiting their applicability in natural illumination
scenarios and downstream inverse rendering tasks. In this paper, we propose
RotatedMVPS to solve shape and reflectance recovery under rotated natural
light, achievable with a practical rotation stage. By ensuring light
consistency across different camera and object poses, our method reduces the
unknowns associated with complex environment light. Furthermore, we integrate
data priors from off-the-shelf learning-based single-view photometric stereo
methods into our MVPS framework, significantly enhancing the accuracy of shape
and reflectance recovery. Experimental results on both synthetic and real-world
datasets demonstrate the effectiveness of our approach.

</details>


### [90] [TSPO: Temporal Sampling Policy Optimization for Long-form Video Language Understanding](https://arxiv.org/abs/2508.04369)
*Canhui Tang,Zifan Han,Hongbo Sun,Sanping Zhou,Xuchong Zhang,Xin Wei,Ye Yuan,Jinglin Xu,Hao Sun*

Main category: cs.CV

TL;DR: 提出了一种名为TSPO（时间采样策略优化）的强化学习框架，用于解决多模态大语言模型在长视频理解中的关键帧采样问题，该框架训练事件感知的时间代理，并引入基于规则的高效奖励机制，显著提升了长视频理解性能。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在长视频处理中存在局限性：由于模型上下文限制和训练成本，只能进行稀疏帧采样。现有方法（如均匀采样或非训练型关键帧搜索）可能遗漏关键事件或受限于预训练模型的事件理解能力，而训练型方法则因非连续性和不可导性面临挑战。

Method: 1) 提出可训练的事件感知时间代理，通过事件-查询相关性进行概率化关键帧选择；2) 设计TSPO强化学习范式，将关键帧选择与语言生成联合建模，利用基于规则的高效奖励实现端到端优化；3) 提出长视频训练数据构建流程，包括综合时序数据和"大海捞针"式视频数据；4) 引入基于规则的回答准确性和时序定位奖励优化采样策略。

Result: 在多个长视频理解基准测试中达到最先进性能，同时展现出在不同前沿视频MLLM上的可迁移能力。

Conclusion: TSPO通过强化学习解决了长视频理解的关键帧采样问题，其端到端的联合优化框架显著提升了事件捕捉效率和模型性能，并具有普适性潜力。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated significant
progress in vision-language tasks, yet they still face challenges when
processing long-duration video inputs. The limitation arises from MLLMs'
context limit and training costs, necessitating sparse frame sampling before
feeding videos into MLLMs. Existing video MLLMs adopt training-free uniform
sampling or keyframe search, which may miss critical events or be constrained
by the pre-trained models' event understanding capabilities. Meanwhile,
building a training-based method remains challenging due to the unsupervised
and non-differentiable nature of sparse frame sampling. To address these
problems, we propose Temporal Sampling Policy Optimization (TSPO), advancing
MLLMs' long-form video-language understanding via reinforcement learning.
Specifically, we first propose a trainable event-aware temporal agent, which
captures event-query correlation for performing probabilistic keyframe
selection. Then, we propose the TSPO reinforcement learning paradigm, which
models keyframe selection and language generation as a joint decision-making
process, enabling end-to-end group relative optimization with efficient
rule-based rewards. Furthermore, for the TSPO's training, we propose a long
video training data construction pipeline with comprehensive temporal data and
video Needle-in-a-Haystack data. Finally, we incorporate rule-based answering
accuracy and temporal locating reward mechanisms to optimize the temporal
sampling policy. Comprehensive experiments show that our TSPO achieves
state-of-the-art performance across multiple long video understanding
benchmarks, and shows transferable ability across different cutting-edge
Video-MLLMs.

</details>


### [91] [VisionTS++: Cross-Modal Time Series Foundation Model with Continual Pre-trained Visual Backbones](https://arxiv.org/abs/2508.04379)
*Lefei Shen,Mouxiang Chen,Xu Liu,Han Fu,Xiaoxue Ren,Jianling Sun,Zhuo Li,Chenghao Liu*

Main category: cs.CV

TL;DR: VisionTS++是一种基于视觉模型的时间序列基础模型，通过持续预训练解决从视觉到时间序列的三个关键差异，包括数据模态差距、多变量预测差距和概率预测差距，并在多个基准测试中取得最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 尽管预训练的视觉模型在时间序列预测中表现出潜力，但由于数据模态差异、多变量处理能力不足以及缺乏概率预测支持，其跨模态迁移面临挑战。需要设计新方法来弥合这些差距。

Method: 1）使用视觉模型过滤高质量时间序列数据以减小模态差距；2）通过彩色多变量转换将时间序列转化为多子图RGB图像以捕获变量间依赖；3）采用并行重建头生成多分位数预测以实现灵活的概率分布估计。

Result: 在分布内和分布外时间序列预测基准测试中，VisionTS++平均比现有最佳模型减少6%-44%的MSE误差，并在12项概率预测任务中的9项排名第一。

Conclusion: VisionTS++建立了跨模态知识迁移的新范式，通过三项创新有效解决了视觉模型应用于时间序列的三大挑战，推动了通用时间序列基础模型的发展。

Abstract: Recent studies have revealed that vision models pre-trained on images can
perform well in time series forecasting by reformulating forecasting as an
image reconstruction task, suggesting their potential as universal time series
foundation models. However, effective cross-modal transfer from vision to time
series remains challenging due to three key discrepancies: (1) data-modality
gap between structured, bounded image data and unbounded, heterogeneous time
series; (2) multivariate-forecasting gap between standard RGB
three-channel-based vision models and the need to model time series with
arbitrary numbers of variates; and (3) probabilistic-forecasting gap between
the deterministic output formats of most vision models and the requirement for
uncertainty-aware probabilistic predictions. To bridge these gaps, we propose
VisionTS++, a vision-model-based TSFM that performs continual pre-training on
large-scale time series datasets, including 3 innovations: (1) a
vision-model-based filtering mechanism to identify high-quality time series
data, thereby mitigating modality gap and improving pre-training stability, (2)
a colorized multivariate conversion method that transforms multivariate time
series into multi-subfigure RGB images, capturing complex inter-variate
dependencies; and (3) a multi-quantile forecasting approach using parallel
reconstruction heads to generate forecasts of different quantile levels, thus
more flexibly approximating arbitrary output distributions without restrictive
prior distributional assumptions. Evaluated on both in-distribution and
out-of-distribution TSF benchmarks, \model achieves SOTA results, outperforming
specialized TSFMs by 6%-44% in MSE reduction and ranking first in 9 out of 12
probabilistic forecasting settings. Our work establishes a new paradigm for
cross-modal knowledge transfer, advancing the development of universal TSFMs.

</details>


### [92] [ProtoN: Prototype Node Graph Neural Network for Unconstrained Multi-Impression Ear Recognition](https://arxiv.org/abs/2508.04381)
*Santhoshkumar Peddi,Sadhvik Bathini,Arun Balasubramanian,Monalisa Sarma,Debasis Samanta*

Main category: cs.CV

TL;DR: 论文提出ProtoN，一个用于少样本耳部识别的图神经网络框架。该框架通过图结构处理身份的多张图像，利用原型节点表达身份级信息，并通过双路径消息传递机制优化特征。结合跨图原型对齐策略和混合损失函数，该模型在五个基准数据集上实现了当前最优性能。


<details>
  <summary>Details</summary>
Motivation: 耳部生物特征识别作为一种稳定且非接触的身份识别方法，面临两个主要挑战：标注数据稀缺和类内差异大。现有方法通常在单个样本上提取特征，难以捕捉一致且具有区分性的表示。因此，需要一种能够综合分析身份多张图像的框架来解决这些问题。

Method: 1. **构建身份图**：构建一个图结构，其中每个身份的不同耳部图像样本（impression）作为节点，同时为每个身份创建可学习的原型节点（prototype node）表示身份整体特征。
2. **Prototype GNN层（PGNN）**：设计一种双路径消息传递机制，一方面原型节点聚合各样本节点的特征以提取身份级信息；另一方面样本节点接收原型的特征进行增强（例如：impression节点可更新为原型的函数，如相加或平均）。
3. **跨图原型对齐**：在训练过程中将来自不同身份的原型节点组成跨图结构，对它们增加约束以提高类间可分性（如将不同身份的原型节点对齐到特征空间中距离较远的位置）。
4. **混合损失函数**：结合两项损失：①基于小样本学习场景的元学习损失函数（每个训练迭代形成新的小样本分类任务）；②全局分类损失函数（将所有身份样本纳入一个分类任务），以优化嵌入空间结构。

Result: 在五个耳部识别基准数据集上进行了实验，ProtoN取得了SOTA性能：Rank-1识别准确率最高达99.60%，等误率（EER）最低达到0.025%。这些结果证明了该方法能在有限训练数据条件下有效提升耳部特征识别的性能。

Conclusion: ProtoN提供了一种基于图神经网络的少样本耳部识别方案。通过引入原型节点和图结构处理同一身份的多张图像，并配合跨图对齐与混合损失函数，解决了传统方法因类内差异和样本少导致的性能瓶颈。该方法在多项数据指标上达到当前最佳性能。

Abstract: Ear biometrics offer a stable and contactless modality for identity
recognition, yet their effectiveness remains limited by the scarcity of
annotated data and significant intra-class variability. Existing methods
typically extract identity features from individual impressions in isolation,
restricting their ability to capture consistent and discriminative
representations. To overcome these limitations, a few-shot learning framework,
ProtoN, is proposed to jointly process multiple impressions of an identity
using a graph-based approach. Each impression is represented as a node in a
class-specific graph, alongside a learnable prototype node that encodes
identity-level information. This graph is processed by a Prototype Graph Neural
Network (PGNN) layer, specifically designed to refine both impression and
prototype representations through a dual-path message-passing mechanism. To
further enhance discriminative power, the PGNN incorporates a cross-graph
prototype alignment strategy that improves class separability by enforcing
intra-class compactness while maintaining inter-class distinction.
Additionally, a hybrid loss function is employed to balance episodic and global
classification objectives, thereby improving the overall structure of the
embedding space. Extensive experiments on five benchmark ear datasets
demonstrate that ProtoN achieves state-of-the-art performance, with Rank-1
identification accuracy of up to 99.60% and an Equal Error Rate (EER) as low as
0.025, showing the effectiveness for few-shot ear recognition under limited
data conditions.

</details>


### [93] [Deep Learning-based Scalable Image-to-3D Facade Parser for Generating Thermal 3D Building Models](https://arxiv.org/abs/2508.04406)
*Yinan Yu,Alex Gonzalez-Caceres,Samuel Scheidegger,Sanjay Somanath,Alexander Hollberg*

Main category: cs.CV

TL;DR: 本文提出了一个名为SI3FP的流程，通过计算机视觉和深度学习从图像中提取几何形状，生成LoD3热模型，用于现有建筑的翻新规划。该方法可直接在正射图像平面建模几何基元，支持稀疏和密集数据源，窗口与墙壁比例估计误差约为5%，适用于大规模能源翻新规划。


<details>
  <summary>Details</summary>
Motivation: 现有建筑翻新对减少气候影响至关重要。早期翻新规划需要基于LoD3级别的热3D模型（包含窗户等特征），但目前缺乏可扩展且准确的特征识别方法。

Method: 提出SI3FP流程：利用计算机视觉和深度学习从图像中提取几何信息，直接在正射图像平面建模几何基元（而非依赖传统的分割和投影方法），统一接口并减少透视畸变。支持谷歌街景（稀疏数据）和手持相机（密集数据）等多种数据源。

Result: 在典型瑞典住宅建筑测试中，SI3FP的窗户与墙壁比例估计误差约为5%。这种精度足以支持早期翻新分析。

Conclusion: SI3FP流程可促进大规模能源翻新规划，其方法也可应用于城市发展和规划的其他领域。

Abstract: Renovating existing buildings is essential for climate impact. Early-phase
renovation planning requires simulations based on thermal 3D models at Level of
Detail (LoD) 3, which include features like windows. However, scalable and
accurate identification of such features remains a challenge. This paper
presents the Scalable Image-to-3D Facade Parser (SI3FP), a pipeline that
generates LoD3 thermal models by extracting geometries from images using both
computer vision and deep learning. Unlike existing methods relying on
segmentation and projection, SI3FP directly models geometric primitives in the
orthographic image plane, providing a unified interface while reducing
perspective distortions. SI3FP supports both sparse (e.g., Google Street View)
and dense (e.g., hand-held camera) data sources. Tested on typical Swedish
residential buildings, SI3FP achieved approximately 5% error in window-to-wall
ratio estimates, demonstrating sufficient accuracy for early-stage renovation
analysis. The pipeline facilitates large-scale energy renovation planning and
has broader applications in urban development and planning.

</details>


### [94] [Thinking With Videos: Multimodal Tool-Augmented Reinforcement Learning for Long Video Reasoning](https://arxiv.org/abs/2508.04416)
*Haoji Zhang,Xin Gu,Jiawen Li,Chixiang Ma,Sule Bai,Chubin Zhang,Bowen Zhang,Zhichao Zhou,Dongliang He,Yansong Tang*

Main category: cs.CV

TL;DR: 该论文提出了VITAL框架，一种通过工具增强学习的视频智能方法，用于解决多模态大语言模型在视频推理中存在的跨模态交互不足和幻觉问题。方法包括使用视觉工具箱密集采样视频帧并生成多模态CoT，构建了两个高质量数据集用于监督微调和强化学习，并提出DGRPO算法解决任务难度不平衡问题。实验表明VITAL在11个视频理解基准上领先，尤其在长视频场景中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的基于文本的链式思考（CoT）推理方法在多模态大语言模型中存在跨模态交互有限和幻觉增加的问题，尤其是在处理较长视频或推理链条时。为了解决这些问题，作者提出了一个端到端的代理视频推理框架。

Method: 1. 提出了VITAL框架，通过视觉工具箱动态采样新视频帧，并生成多模态的CoT（链式思考）进行精确的长视频推理。2. 构建了两个高质量的数据集：MTVR-CoT-72k用于监督微调，MTVR-RL-110k用于强化学习。3. 提出难度感知的组相对策略优化算法（DGRPO），以解决多任务强化学习中的难度不平衡问题。

Result: 在11个具有挑战性的视频理解基准测试中，VITAL展示了先进的推理能力，特别是在视频问答和时间定位任务上，且在长视频场景中表现更优。

Conclusion: VITAL框架通过工具增强学习和多任务的协同训练，有效提升了视频推理的准确性，尤其在长视频处理上具有明显优势。同时，提出的DGRPO算法成功缓解了多任务强化学习中的难度不平衡问题。代码、数据和模型权重将公开。

Abstract: The video reasoning ability of multimodal large language models (MLLMs) is
crucial for downstream tasks like video question answering and temporal
grounding. While recent approaches have explored text-based chain-of-thought
(CoT) reasoning for MLLMs, these methods often suffer from limited cross-modal
interaction and increased hallucination, especially with longer videos or
reasoning chains. To address these challenges, we propose Video Intelligence
via Tool-Augmented Learning (VITAL), a novel end-to-end agentic video reasoning
framework. With a visual toolbox, the model can densely sample new video frames
on demand and generate multimodal CoT for precise long video reasoning. We
observe that temporal grounding and question answering are mutually beneficial
for video understanding tasks. Therefore, we construct two high-quality
multi-task video reasoning datasets MTVR-CoT-72k for supervised fine-tuning and
MTVR-RL-110k for reinforcement learning. Moreover, we propose a
Difficulty-aware Group Relative Policy Optimization algorithm (DGRPO) to
mitigate difficulty imbalance in multi-task reinforcement learning. Extensive
experiments on 11 challenging video understanding benchmarks demonstrate the
advanced reasoning ability of VITAL, outperforming existing methods in video
question answering and temporal grounding tasks, especially in long video
scenarios. All code, data and model weight will be made publicly available.

</details>


### [95] [Efficient Inter-Task Attention for Multitask Transformer Models](https://arxiv.org/abs/2508.04422)
*Christian Bohn,Thomas Kurbiel,Klaus Friedrichs,Hasan Tercan,Tobias Meisen*

Main category: cs.CV

TL;DR: 论文针对Transformer架构在多任务学习中计算效率低下的问题，提出一种新的Deformable Inter-Task Self-Attention机制，显著降低计算量和推理延迟，同时提高多任务预测性能。


<details>
  <summary>Details</summary>
Motivation: Transformer架构在计算机视觉和深度学习领域应用广泛，但用于多任务学习时，其多头注意力机制在任务数量较多时计算量剧增（注意力矩阵随任务数量呈二次方增长），受限于硬件计算能力。

Method: 提出可变形跨任务自注意力机制（Deformable Inter-Task Self-Attention），该机制能更高效地聚合不同任务特征图的信息。该方法通过优化任务间的信息交互方式，避免构建全局二次方规模注意力矩阵。

Result: 在NYUD-v2和PASCAL-Context数据集上的实验表明：1）计算量（FLOPs）和推理延迟降低一个数量级；2）个体任务预测质量指标最高提升7.4%。

Conclusion: 所提方法有效解决了多任务学习中Transformer计算瓶颈问题，在显著提升效率的同时改善了模型性能。

Abstract: In both Computer Vision and the wider Deep Learning field, the Transformer
architecture is well-established as state-of-the-art for many applications. For
Multitask Learning, however, where there may be many more queries necessary
compared to single-task models, its Multi-Head-Attention often approaches the
limits of what is computationally feasible considering practical hardware
limitations. This is due to the fact that the size of the attention matrix
scales quadratically with the number of tasks (assuming roughly equal numbers
of queries for all tasks). As a solution, we propose our novel Deformable
Inter-Task Self-Attention for Multitask models that enables the much more
efficient aggregation of information across the feature maps from different
tasks. In our experiments on the NYUD-v2 and PASCAL-Context datasets, we
demonstrate an order-of-magnitude reduction in both FLOPs count and inference
latency. At the same time, we also achieve substantial improvements by up to
7.4% in the individual tasks' prediction quality metrics.

</details>


### [96] [Composed Object Retrieval: Object-level Retrieval via Composed Expressions](https://arxiv.org/abs/2508.04424)
*Tong Wang,Guanyu Yang,Nian Liu,Zongyan Han,Jinxing Zhou,Salman Khan,Fahad Shahbaz Khan*

Main category: cs.CV

TL;DR: 该论文提出了一种全新的组合对象检索（COR）任务，旨在通过结合参考图像和检索文本来检索和分割细粒度的目标对象。同时，作者构建了大规模基准数据集COR127K，并提出了一个统一的端到端模型CORE。


<details>
  <summary>Details</summary>
Motivation: 现有的组合图像检索（CIR）方法仅限于图像级别的匹配，无法定位到特定对象。为了解决这个问题，需要开发一种能够实现对象级别精确检索的方法。

Method: 作者提出了CORE模型，该模型集成了参考区域编码、自适应视觉-文本交互和区域级别的对比学习。模型通过处理组合表达式（参考对象+检索文本）来检索和分割目标对象。

Result: 在COR127K数据集上的实验表明，CORE模型在基础类别和新类别上都显著优于现有模型，为这一具有挑战性的任务建立了简单而有效的基线。

Conclusion: COR任务为细粒度多模态检索研究开辟了新方向。CORE模型通过集成多个关键模块成功解决了COR任务中的挑战，并在大规模基准测试中取得了优异表现。

Abstract: Retrieving fine-grained visual content based on user intent remains a
challenge in multi-modal systems. Although current Composed Image Retrieval
(CIR) methods combine reference images with retrieval texts, they are
constrained to image-level matching and cannot localize specific objects. To
this end, we propose Composed Object Retrieval (COR), a brand-new task that
goes beyond image-level retrieval to achieve object-level precision, allowing
the retrieval and segmentation of target objects based on composed expressions
combining reference objects and retrieval texts. COR presents significant
challenges in retrieval flexibility, which requires systems to identify
arbitrary objects satisfying composed expressions while avoiding semantically
similar but irrelevant negative objects within the same scene. We construct
COR127K, the first large-scale COR benchmark that contains 127,166 retrieval
triplets with various semantic transformations in 408 categories. We also
present CORE, a unified end-to-end model that integrates reference region
encoding, adaptive visual-textual interaction, and region-level contrastive
learning. Extensive experiments demonstrate that CORE significantly outperforms
existing models in both base and novel categories, establishing a simple and
effective baseline for this challenging task while opening new directions for
fine-grained multi-modal retrieval research.

</details>


### [97] [Benchmarking Foundation Models for Mitotic Figure Classification](https://arxiv.org/abs/2508.04441)
*Jonas Ammeling,Jonathan Ganz,Emely Rosbach,Ludwig Lausser,Christof A. Bertram,Katharina Breininger,Marc Aubreville*

Main category: cs.CV

TL;DR: 本文研究了基础模型（特别是使用低秩适应LoRA进行微调）在有丝分裂图分类任务中的表现。实验表明，LoRA适应的基础模型在线性探测上表现更优，仅需10%的训练数据即可接近使用100%数据的效果，并且在未见过的肿瘤域上缩小了性能差距。然而，传统架构的完全微调仍有竞争力。


<details>
  <summary>Details</summary>
Motivation: 深度学习的性能通常随数据量和多样性提升，但病理学等医学影像领域常面临标注数据有限的问题。自监督学习训练的基础模型能提取丰富的特征，有助于解决数据有限的问题。本文专注于利用基础模型进行有丝分裂图分类，以支持有丝分裂计数的肿瘤预后和分级。

Method: 1. 比较多种现有基础模型的数据扩展定律；2. 评估这些模型在未见肿瘤域上的鲁棒性；3. 除了常用的线性探测（fine-tuning顶层）外，还采用低秩适应（LoRA）微调注意力机制；4. 与端到端训练的CNN和视觉Transformer（ViT）基线模型进行对比。

Result: 1. 使用LoRA适应的基础模型优于线性探测，仅用10%的训练数据即可达到接近100%数据可用时的性能；2. LoRA适应最新基础模型几乎消除了其在未见肿瘤域上的性能差距；3. 传统架构（如CNN/ViT）的完全微调仍有竞争力。

Conclusion: LoRA微调能高效利用基础模型的潜力，极大减少有丝分裂分类任务的数据依赖并提升跨域适应能力。但完全微调的传统架构仍具竞争性，表明基础模型与传统方法在不同情境下各有优势。

Abstract: The performance of deep learning models is known to scale with data quantity
and diversity. In pathology, as in many other medical imaging domains, the
availability of labeled images for a specific task is often limited.
Self-supervised learning techniques have enabled the use of vast amounts of
unlabeled data to train large-scale neural networks, i.e., foundation models,
that can address the limited data problem by providing semantically rich
feature vectors that can generalize well to new tasks with minimal training
effort increasing model performance and robustness. In this work, we
investigate the use of foundation models for mitotic figure classification. The
mitotic count, which can be derived from this classification task, is an
independent prognostic marker for specific tumors and part of certain tumor
grading systems. In particular, we investigate the data scaling laws on
multiple current foundation models and evaluate their robustness to unseen
tumor domains. Next to the commonly used linear probing paradigm, we also adapt
the models using low-rank adaptation (LoRA) of their attention mechanisms. We
compare all models against end-to-end-trained baselines, both CNNs and Vision
Transformers. Our results demonstrate that LoRA-adapted foundation models
provide superior performance to those adapted with standard linear probing,
reaching performance levels close to 100% data availability with only 10% of
training data. Furthermore, LoRA-adaptation of the most recent foundation
models almost closes the out-of-domain performance gap when evaluated on unseen
tumor domains. However, full fine-tuning of traditional architectures still
yields competitive performance.

</details>


### [98] [Zero-Residual Concept Erasure via Progressive Alignment in Text-to-Image Model](https://arxiv.org/abs/2508.04472)
*Hongxu Chen,Zhen Wang,Taoran Mei,Lin Li,Bowei Zhu,Runshi Li,Long Chen*

Main category: cs.CV

TL;DR: 本文提出了一种名为ErasePro的新型概念擦除方法，旨在解决当前文本生成图像模型在擦除有害语义概念时存在的不彻底和质量下降问题。通过引入零残差约束和渐进层更新策略，实现了更彻底的概念擦除和对生成质量的更好保护。


<details>
  <summary>Details</summary>
Motivation: 现有的概念擦除方法虽然高效，但存在两个被忽视的局限：1）当文本提示较复杂时，由于"非零对齐残差"导致擦除不彻底；2）因参数更新过于集中在少数深层而导致生成质量下降。这促使我们提出一种改进方案以同时实现更彻底的概念擦除和更好的质量保持。

Method: ErasePro方法包含两个创新：首先在优化目标中引入严格的零残差约束，确保目标概念特征与锚概念完全对齐；其次采用渐进的分层更新策略，将目标概念特征从浅层到深层逐步转移到锚概念特征，深层所需改动逐渐减少，从而保护敏感深层的生成质量。

Result: 在多种概念擦除任务（包括实例、艺术风格和裸露内容擦除）上的实验证明，ErasePro在概念擦除完整性和生成质量保持方面均优于现有方法。

Conclusion: ErasePro通过零残差约束和渐进层更新策略，有效解决了现有概念擦除方法的不彻底和质量退化问题，为文本生成图像模型的安全部署提供了更优方案。

Abstract: Concept Erasure, which aims to prevent pretrained text-to-image models from
generating content associated with semantic-harmful concepts (i.e., target
concepts), is getting increased attention. State-of-the-art methods formulate
this task as an optimization problem: they align all target concepts with
semantic-harmless anchor concepts, and apply closed-form solutions to update
the model accordingly. While these closed-form methods are efficient, we argue
that existing methods have two overlooked limitations: 1) They often result in
incomplete erasure due to "non-zero alignment residual", especially when text
prompts are relatively complex. 2) They may suffer from generation quality
degradation as they always concentrate parameter updates in a few deep layers.
To address these issues, we propose a novel closed-form method ErasePro: it is
designed for more complete concept erasure and better preserving overall
generative quality. Specifically, ErasePro first introduces a strict
zero-residual constraint into the optimization objective, ensuring perfect
alignment between target and anchor concept features and enabling more complete
erasure. Secondly, it employs a progressive, layer-wise update strategy that
gradually transfers target concept features to those of the anchor concept from
shallow to deep layers. As the depth increases, the required parameter changes
diminish, thereby reducing deviations in sensitive deep layers and preserving
generative quality. Empirical results across different concept erasure tasks
(including instance, art style, and nudity erasure) have demonstrated the
effectiveness of our ErasePro.

</details>


### [99] [Boosting Visual Knowledge-Intensive Training for LVLMs Through Causality-Driven Visual Object Completion](https://arxiv.org/abs/2508.04453)
*Qingguo Hu,Ante Wang,Jia Song,Delai Qiu,Qingsong Liu,Jinsong Su*

Main category: cs.CV

TL;DR: 大模型视觉语言模型（LVLMs）在需要深度视觉感知的任务中表现不足，部分原因是训练数据缺乏视觉知识。研究者提出了一种基于因果关系的视觉对象补全任务（CVC）的自改进框架。该框架能够自动构造训练示例，并通过试错学习来提升模型的视觉感知和推理能力。使用该框架训练的模型在各种综合和专业任务上都得到了显著的提升。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型（LVLMs）在需要深度视觉感知的任务中表现不佳，例如识别图像中的细微差异。主要原因在于指令微调语料库中视觉知识的匮乏，导致模型的视觉感知和推理能力不足。为了解决这一问题，研究者提出了一种基于因果关系的视觉对象补全任务（CVC）的自改进框架。

Method: 1. 设计了一个新的视觉知识密集型任务：基于因果关系的视觉对象补全（CVC），该任务要求模型根据图像中已知对象的因果关系推断被遮挡的对象。2. 开发了自动化的实例构建管道，无需依赖复杂的LVLMs（如GPT-4V）或人工辅助即可轻松生成大量示例。3. 让LVLMs通过这些构造的实例进行试错学习来自我改进。

Result: 实验结果表明，该方法在四项专业化任务和四个综合基准测试中取得了显著提升：（1）在专业化任务中，基于LLaVA-1.5-7B和LLaVA-1.5-13B模型的分别平均提升了5.4%和4.0%；（2）在综合基准测试上也表现出提升性能；（3）框架的有效性得到了验证。

Conclusion: 提出的CVC自改进框架能够显著提升LVLMs在深度视觉感知任务中的表现，特别是在需要识别细微差异的场景。该框架通过自动构建基于因果关系的视觉对象补全任务实例，并利用这些实例通过试错学习对模型进行优化，为解决训练语料中视觉知识匮乏问题提供了有效途径。

Abstract: Large Vision-Language Models (LVLMs) have experienced significant
advancements in recent years. However, their performance still falls short in
tasks requiring deep visual perception, such as identifying subtle differences
between images. A potential cause is the scarcity of visual knowledge in
popular instruction-tuning corpora, resulting in inadequate visual perception
and reasoning capabilities. To address this challenge, we introduce a
self-improvement framework grounded in a novel visual knowledge-intensive task,
\underline{C}ausality-driven \underline{V}isual object \underline{C}ompletion
(CVC). This task requires LVLMs to infer the masked object in an image based on
its \textit{causal} relationships with the other visible information. We first
obtain rich examples cheaply through our automated instance construction
pipeline, without relying on sophisticated LVLMs (\textit{e.g.}, GPT-4V) or
human assistance. Then, LVLMs effectively self-improve through trial and error
learning using these created instances. Our experiments demonstrate substantial
gains across four challenging specialized tasks and four widely-used
comprehensive benchmarks. Especially on specialized tasks, our method achieves
an average improvement of 5.4\% and 4.0\% compared to the corresponding
baselines when utilizing LLaVA-1.5-7B and LLaVA-1.5-13B, respectively. The code
is available at https://github.com/XMUDeepLIT/CVC.

</details>


### [100] [4DVD: Cascaded Dense-view Video Diffusion Model for High-quality 4D Content Generation](https://arxiv.org/abs/2508.04467)
*Shuzhou Yang,Xiaodong Cun,Xiaoyu Li,Yaowei Li,Jian Zhang*

Main category: cs.CV

TL;DR: 提出了一种名为4DVD的级联视频扩散模型，通过解耦方式生成4D内容，克服直接生成高维度数据的复杂性。方法分为粗略多视角布局生成和结构感知条件生成，并有效统一这两个子任务。构建了动态3D对象数据集D-Objaverse进行训练，展现最先进的新视图合成与4D生成性能。


<details>
  <summary>Details</summary>
Motivation: 直接生成高维数据（如4D）复杂度过高。现有多视图视频方法虽尝试同时建模3D空间和时间特征，但效率低下。因此，需一种更高效、解耦的方法实现高质量4D内容生成。

Method: 方法分为两阶段：1）粗布局生成：预测具有跨视角一致性与时序一致性的密集视图内容；2）结构感知生成：基于布局先验和输入视频的精细外观内容，生成最终高质量密集视图视频。最终可通过该表示优化4D高斯等显式4D表示。

Result: 在D-Objaverse数据集上训练证明，模型在性能上达到SOTA水平，能够合成高质量的新视图与4D内容。

Conclusion: 4DVD模型通过级联生成解决4D内容建模复杂性，两阶段结构有效统一结构与外观特征，并能在4D表示精确优化，拓宽实际应用场景。

Abstract: Given the high complexity of directly generating high-dimensional data such
as 4D, we present 4DVD, a cascaded video diffusion model that generates 4D
content in a decoupled manner. Unlike previous multi-view video methods that
directly model 3D space and temporal features simultaneously with stacked cross
view/temporal attention modules, 4DVD decouples this into two subtasks: coarse
multi-view layout generation and structure-aware conditional generation, and
effectively unifies them. Specifically, given a monocular video, 4DVD first
predicts the dense view content of its layout with superior cross-view and
temporal consistency. Based on the produced layout priors, a structure-aware
spatio-temporal generation branch is developed, combining these coarse
structural priors with the exquisite appearance content of input monocular
video to generate final high-quality dense-view videos. Benefit from this,
explicit 4D representation~(such as 4D Gaussian) can be optimized accurately,
enabling wider practical application. To train 4DVD, we collect a dynamic 3D
object dataset, called D-Objaverse, from the Objaverse benchmark and render 16
videos with 21 frames for each object. Extensive experiments demonstrate our
state-of-the-art performance on both novel view synthesis and 4D generation.
Our project page is https://4dvd.github.io/

</details>


### [101] [Augmentation-based Domain Generalization and Joint Training from Multiple Source Domains for Whole Heart Segmentation](https://arxiv.org/abs/2508.04552)
*Franz Thaler,Darko Stern,Gernot Plank,Martin Urschler*

Main category: cs.CV

TL;DR: 提出了一种用于全心脏分割的稳健方法，该方法使用平衡联合训练和强力数据增强来应对领域偏移问题，在CT和MR数据上均取得了出色的分割性能。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病是全球主要死因，需要从医学图像中分析心脏及其子结构的高级方法。语义分割对评估心脏形态学和病理学至关重要，并能用于生成心脏数字孪生模型进行个性化治疗规划。然而，现有深度学习方法在领域偏移（训练和测试数据分布不同）时性能下降明显。

Method: 1) 使用平衡联合训练：在训练时等量利用来自不同源域（CT和MR）的数据
2) 采用强度与空间数据增强技术：大幅提升训练数据的多样性以缓解测试时域外数据的影响
3) 构建5折集成模型：提升整体鲁棒性

Result: 模型表现：
- CT数据：93.33% DSC（Dice相似系数）和0.8388 mm ASSD（平均对称表面距离）
- MR数据：89.30% DSC和1.2411 mm ASSD
该方法在MR数据上达到最优性能，在CT数据上与专用CT训练模型性能相当

Conclusion: 所提出的心脏分割方法通过平衡多模态训练和多样化增强有效解决了领域偏移问题，为生成精确的心脏数字孪生模型提供了高效途径，具有重要的临床转化潜力。

Abstract: As the leading cause of death worldwide, cardiovascular diseases motivate the
development of more sophisticated methods to analyze the heart and its
substructures from medical images like Computed Tomography (CT) and Magnetic
Resonance (MR). Semantic segmentations of important cardiac structures that
represent the whole heart are useful to assess patient-specific cardiac
morphology and pathology. Furthermore, accurate semantic segmentations can be
used to generate cardiac digital twin models which allows e.g.
electrophysiological simulation and personalized therapy planning. Even though
deep learning-based methods for medical image segmentation achieved great
advancements over the last decade, retaining good performance under domain
shift -- i.e. when training and test data are sampled from different data
distributions -- remains challenging. In order to perform well on domains known
at training-time, we employ a (1) balanced joint training approach that
utilizes CT and MR data in equal amounts from different source domains.
Further, aiming to alleviate domain shift towards domains only encountered at
test-time, we rely on (2) strong intensity and spatial augmentation techniques
to greatly diversify the available training data. Our proposed whole heart
segmentation method, a 5-fold ensemble with our contributions, achieves the
best performance for MR data overall and a performance similar to the best
performance for CT data when compared to a model trained solely on CT. With
93.33% DSC and 0.8388 mm ASSD for CT and 89.30% DSC and 1.2411 mm ASSD for MR
data, our method demonstrates great potential to efficiently obtain accurate
semantic segmentations from which patient-specific cardiac twin models can be
generated.

</details>


### [102] [QuantVSR: Low-Bit Post-Training Quantization for Real-World Video Super-Resolution](https://arxiv.org/abs/2508.04485)
*Bowen Chai,Zheng Chen,Libo Zhu,Wenbo Li,Yong Guo,Yulun Zhang*

Main category: cs.CV

TL;DR: QuantVSR是针对真实世界视频超分辨率（VSR）的低比特量化模型，解决了扩散模型在VSR中推理速度慢和资源消耗大的问题。通过提出时空复杂度感知机制和可学习偏置对齐模块，在保持性能的同时实现高效压缩。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散模型在真实世界视频超分辨率中表现优异，但其缓慢的处理速度和沉重的资源消耗阻碍了实际应用和部署。量化虽能压缩模型，但VSR模型的时序特性和高保真要求使得量化极具挑战性。

Method: 提出QuantVSR模型：1) 时空复杂度感知机制(STCA)：利用校准数据集测量每层的空间和时间复杂度，根据统计结果为低秩全精度辅助分支分配层特定的秩；2) 联合优化全精度分支和低比特分支；3) 可学习偏置对齐模块(LBA)以减少有偏的量化误差。

Result: 在合成和真实世界数据集上进行的大量实验表明，所提方法性能与全精度模型相当，并显著优于近期领先的低比特量化方法。

Conclusion: QuantVSR成功解决了VSR模型量化中的挑战，实现了高效压缩而不牺牲性能，为现实部署提供了可行性。代码已开源。

Abstract: Diffusion models have shown superior performance in real-world video
super-resolution (VSR). However, the slow processing speeds and heavy resource
consumption of diffusion models hinder their practical application and
deployment. Quantization offers a potential solution for compressing the VSR
model. Nevertheless, quantizing VSR models is challenging due to their temporal
characteristics and high fidelity requirements. To address these issues, we
propose QuantVSR, a low-bit quantization model for real-world VSR. We propose a
spatio-temporal complexity aware (STCA) mechanism, where we first utilize the
calibration dataset to measure both spatial and temporal complexities for each
layer. Based on these statistics, we allocate layer-specific ranks to the
low-rank full-precision (FP) auxiliary branch. Subsequently, we jointly refine
the FP and low-bit branches to achieve simultaneous optimization. In addition,
we propose a learnable bias alignment (LBA) module to reduce the biased
quantization errors. Extensive experiments on synthetic and real-world datasets
demonstrate that our method obtains comparable performance with the FP model
and significantly outperforms recent leading low-bit quantization methods. Code
is available at: https://github.com/bowenchai/QuantVSR.

</details>


### [103] [Learning Robust Intervention Representations with Delta Embeddings](https://arxiv.org/abs/2508.04492)
*Panagiotis Alimisis,Christos Diou*

Main category: cs.CV

TL;DR: 本文提出了一种名为因果Delta嵌入的干预表示方法，旨在提升模型的分布外鲁棒性。该方法强调在隐空间中学习对干预的稀疏且场景不变的表示，从而改善因果表示学习的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前因果表示学习主要关注场景变量的识别与表示，而忽视了干预本身的表示。本文认为，干预的表示对于提升模型在分布外（OOD）场景中的鲁棒性至关重要。通过建立对场景不变的稀疏因果干预表示，能够更好地捕捉干预的本质并增强泛化能力。

Method: 1. 提出因果Delta嵌入概念：一种稀疏且对视觉场景不变的干预表示，仅影响相关的因果变量。
2. 构建无监督框架：仅需成对的干预图像（起始状态与结束状态）即可学习因果表示，无需额外监督。
3. 模型优化：通过约束嵌入的稀疏性和场景不变性，使模型能够分离干预效应与场景内容。

Result: 在因果三元组挑战赛（Causal Triplet challenge）中进行了验证：
- 在合成数据集和真实世界基准测试上均显著超越基线模型。
- 尤其在分布外（OOD）场景中，因果Delta嵌入表现出极强的鲁棒性。

Conclusion: 聚焦于干预的表示（而非仅场景变量）是提升因果模型OOD鲁棒性的有效策略。本文提出的因果Delta嵌入通过稀疏性和场景不变性约束，为无监督学习干预表示提供了可行方案，并在多个基准测试中验证了其优越性。

Abstract: Causal representation learning has attracted significant research interest
during the past few years, as a means for improving model generalization and
robustness. Causal representations of interventional image pairs, have the
property that only variables corresponding to scene elements affected by the
intervention / action are changed between the start state and the end state.
While most work in this area has focused on identifying and representing the
variables of the scene under a causal model, fewer efforts have focused on
representations of the interventions themselves. In this work, we show that an
effective strategy for improving out of distribution (OOD) robustness is to
focus on the representation of interventions in the latent space. Specifically,
we propose that an intervention can be represented by a Causal Delta Embedding
that is invariant to the visual scene and sparse in terms of the causal
variables it affects. Leveraging this insight, we propose a framework that is
capable of learning causal representations from image pairs, without any
additional supervision. Experiments in the Causal Triplet challenge demonstrate
that Causal Delta Embeddings are highly effective in OOD settings,
significantly exceeding baseline performance in both synthetic and real-world
benchmarks.

</details>


### [104] [MonoCloth: Reconstruction and Animation of Cloth-Decoupled Human Avatars from Monocular Videos](https://arxiv.org/abs/2508.04505)
*Daisheng Jin,Ying He*

Main category: cs.CV

TL;DR: 提出了一种名为MonoCloth的新方法，从单目视频重建和动画化着衣人体化身。通过基于部位的分解策略（身体、脸、手和衣服），并针对衣服引入布料模拟模块来提高重建质量和动画真实感。


<details>
  <summary>Details</summary>
Motivation: 现有的方法在从单目视频重建和动画化人体化身时，由于输入几何信息有限且涉及复杂的非刚性运动，存在准确性和真实感的不足。特别在处理不同部位（如面部、手部和衣物）时，需要针对各自的难度和复杂度进行优化，尤其是衣物的形变和运动难以准确捕捉。

Method: MonoCloth采用基于部位的分解策略，将化身分解为身体、脸、手部和服装。对于面部和手部重点详细几何重建。针对衣物，设计了专用的布料模拟模块，通过提取时间运动信号和几何约束来捕获衣物的形变，从而保证服装运动的真实感。模型允许单独处理不同部位并融合最终结果。

Result: 实验结果表明，MonoCloth在视觉重建质量和动画真实感方面均优于现有方法。此外，由于采用基于部位的设计，该方法支持额外任务如衣物转移，突出了方法的灵活性和实用价值。

Conclusion: MonoCloth在克服单目视频输入的限制上取得了显著效果，通过区分不同部位并针对其特点开发相应的处理策略——特别是为衣物设计仿真模块——从而提升了虚拟人体的重建效果和动画真实感，同时也具备良好的扩展性。

Abstract: Reconstructing realistic 3D human avatars from monocular videos is a
challenging task due to the limited geometric information and complex non-rigid
motion involved. We present MonoCloth, a new method for reconstructing and
animating clothed human avatars from monocular videos. To overcome the
limitations of monocular input, we introduce a part-based decomposition
strategy that separates the avatar into body, face, hands, and clothing. This
design reflects the varying levels of reconstruction difficulty and deformation
complexity across these components. Specifically, we focus on detailed geometry
recovery for the face and hands. For clothing, we propose a dedicated cloth
simulation module that captures garment deformation using temporal motion cues
and geometric constraints. Experimental results demonstrate that MonoCloth
improves both visual reconstruction quality and animation realism compared to
existing methods. Furthermore, thanks to its part-based design, MonoCloth also
supports additional tasks such as clothing transfer, underscoring its
versatility and practical utility.

</details>


### [105] [Skeleton Motion Words for Unsupervised Skeleton-Based Temporal Action Segmentation](https://arxiv.org/abs/2508.04513)
*Uzay Gökay,Federico Spurio,Dominik R. Bach,Juergen Gall*

Main category: cs.CV

TL;DR: 提出了一种无监督的基于骨架的时间动作分割新方法，使用序列到序列的自编码器并利用量化手段发现动作簇，在三个数据集上超过现有无监督方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖于有标注的数据，成本高昂；而无监督方法多针对视频，骨架序列虽在应用中很重要但未被充分研究。本文旨在开发一种无监督的骨架动作分割方法。

Method: 1. 使用序列到序列的时序自编码器，在嵌入空间中将不同关节信息解耦；2. 将隐藏骨架序列划分为非重叠块；3. 量化获得骨架运动词，用于发现语义动作簇从而分割动作。

Result: 在HuGaDB、LARa和BABEL三个骨架数据集上的实验表明，该方法优于当前无监督时间动作分割的所有SOTA方法。

Conclusion: 所提出的无监督骨架动作分割方法在公开数据集上效果出色，填补了骨架无监督动作分割的研究空白。

Abstract: Current state-of-the-art methods for skeleton-based temporal action
segmentation are predominantly supervised and require annotated data, which is
expensive to collect. In contrast, existing unsupervised temporal action
segmentation methods have focused primarily on video data, while skeleton
sequences remain underexplored, despite their relevance to real-world
applications, robustness, and privacy-preserving nature. In this paper, we
propose a novel approach for unsupervised skeleton-based temporal action
segmentation. Our method utilizes a sequence-to-sequence temporal autoencoder
that keeps the information of the different joints disentangled in the
embedding space. Latent skeleton sequences are then divided into
non-overlapping patches and quantized to obtain distinctive skeleton motion
words, driving the discovery of semantically meaningful action clusters. We
thoroughly evaluate the proposed approach on three widely used skeleton-based
datasets, namely HuGaDB, LARa, and BABEL. The results demonstrate that our
model outperforms the current state-of-the-art unsupervised temporal action
segmentation methods. Code is available at https://github.com/bachlab/SMQ .

</details>


### [106] [RAIDX: A Retrieval-Augmented Generation and GRPO Reinforcement Learning Framework for Explainable Deepfake Detection](https://arxiv.org/abs/2508.04524)
*Tianxiao Li,Zhenglin Huang,Haiquan Wen,Yiwei He,Shuchang Lyu,Baoyuan Wu,Guangliang Cheng*

Main category: cs.CV

TL;DR: 该论文提出了RAIDX框架，首次结合了检索增强生成（RAG）和分组相对策略优化（GRPO），旨在提高深度伪造检测的准确性和可解释性，同时减少对人工标注的依赖。


<details>
  <summary>Details</summary>
Motivation: 当前深度伪造检测方法存在问题：一种是针对面部的特定检测器，另一种是通用的AI生成检测器，它们都缺乏透明度，仅将检测视为分类任务而不解释决策依据。尽管一些基于LLM的方法提供了可解释性，但存在分析粒度粗和依赖高强度人工标注的问题。

Method: 1. 利用RAG引入外部知识以提升检测准确性。2. 采用GRPO自动生成细粒度的文本解释和显著性图，避免大量手动标注。整个框架统一整合了RAG和GRPO技术。

Result: 在多个基准测试上，RAIDX在识别真伪方面实现了最先进的检测性能，并能同时提供文本描述和显著性图两种可解释依据。

Conclusion: RAIDX框架在准确性和可解释性上均取得了突破，首次将RAG与GRPO结合，解决了现有方法的关键缺陷，推进了深度伪造识别的透明度。代码和模型将开源。

Abstract: The rapid advancement of AI-generation models has enabled the creation of
hyperrealistic imagery, posing ethical risks through widespread misinformation.
Current deepfake detection methods, categorized as face specific detectors or
general AI-generated detectors, lack transparency by framing detection as a
classification task without explaining decisions. While several LLM-based
approaches offer explainability, they suffer from coarse-grained analyses and
dependency on labor-intensive annotations. This paper introduces RAIDX
(Retrieval-Augmented Image Deepfake Detection and Explainability), a novel
deepfake detection framework integrating Retrieval-Augmented Generation (RAG)
and Group Relative Policy Optimization (GRPO) to enhance detection accuracy and
decision explainability. Specifically, RAIDX leverages RAG to incorporate
external knowledge for improved detection accuracy and employs GRPO to
autonomously generate fine-grained textual explanations and saliency maps,
eliminating the need for extensive manual annotations. Experiments on multiple
benchmarks demonstrate RAIDX's effectiveness in identifying real or fake, and
providing interpretable rationales in both textual descriptions and saliency
maps, achieving state-of-the-art detection performance while advancing
transparency in deepfake identification. RAIDX represents the first unified
framework to synergize RAG and GRPO, addressing critical gaps in accuracy and
explainability. Our code and models will be publicly available.

</details>


### [107] [No Masks Needed: Explainable AI for Deriving Segmentation from Classification](https://arxiv.org/abs/2508.04534)
*Mosong Ma,Tania Stathaki,Michalis Lazarou*

Main category: cs.CV

TL;DR: 本文提出了一种针对医学图像的预训练模型微调方法，结合了可解释性人工智能（Explainable AI）生成相关性分数，从而提升分割准确性，并在多个医学数据集上取得了优异结果。


<details>
  <summary>Details</summary>
Motivation: 当前在计算机视觉领域中，利用预训练模型的无监督分割方法在医学图像领域表现不佳，因此需要一种专门针对医学图像的微调方法，以实现更精准的分割。

Method: 通过微调预训练的模型使其适应医学图像，同时整合可解释性人工智能生成相关性分数来增强分割过程。该方法包括模型微调、可解释性AI集成以及优化处理流程。

Result: 该方法在CBIS-DDSM、NuInsSeg和Kvasir-SEG等医学图像数据集上取得了优于传统方法的结果，证明了其在医学图像分割中的有效性。

Conclusion: 所提出的方法成功填补了通用预训练模型在医学图像领域的性能差距，并通过可解释性AI提升了分割质量，为计算机辅助诊断提供了更可靠的工具。

Abstract: Medical image segmentation is vital for modern healthcare and is a key
element of computer-aided diagnosis. While recent advancements in computer
vision have explored unsupervised segmentation using pre-trained models, these
methods have not been translated well to the medical imaging domain. In this
work, we introduce a novel approach that fine-tunes pre-trained models
specifically for medical images, achieving accurate segmentation with extensive
processing. Our method integrates Explainable AI to generate relevance scores,
enhancing the segmentation process. Unlike traditional methods that excel in
standard benchmarks but falter in medical applications, our approach achieves
improved results on datasets like CBIS-DDSM, NuInsSeg and Kvasir-SEG.

</details>


### [108] [TopKD: Top-scaled Knowledge Distillation](https://arxiv.org/abs/2508.04539)
*Qi Wang,Jinjia Zhou*

Main category: cs.CV

TL;DR: 该论文提出了一种名为TopKD的知识蒸馏框架，专注于利用教师模型的Top-K logit信息，通过两个组件——Top-K缩放模块（TSM）和Top-K解耦损失（TDL）增强logit蒸馏，在各种数据集和架构上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 以往的知识蒸馏研究主要关注特征级知识转移，而忽略了教师模型logit分布中的关键信息，特别是Top-K知识。本文重新审视了基于logit的蒸馏方法，发现Top-K是未充分探索的关键元素。

Method: 1. Top-K缩放模块（TSM）：自适应放大logit中最具信息量的Top-K值。
2. Top-K解耦损失（TDL）：提供有针对性的有效监督。
TopKD无需额外模块或架构改动，即可无缝集成到现有知识蒸馏方法中。

Result: 在CIFAR-100、ImageNet、STL-10和Tiny-ImageNet上进行的广泛实验表明，TopKD一致优于现有最先进的蒸馏方法，且在蒸馏Vision Transformers时展现出显著效果，验证了其在不同网络架构上的通用性。

Conclusion: 研究证实了logit在推动知识蒸馏发展方面的巨大潜力，TopKD作为一种简单、高效且与架构无关的框架，通过有效利用Top-K知识显著提升了logit蒸馏的性能。

Abstract: Recent advances in knowledge distillation (KD) predominantly emphasize
feature-level knowledge transfer, frequently overlooking critical information
embedded within the teacher's logit distributions. In this paper, we revisit
logit-based distillation and reveal an underexplored yet critical element:
Top-K knowledge. Motivated by this insight, we propose Top-scaled Knowledge
Distillation (TopKD), a simple, efficient, and architecture-agnostic framework
that significantly enhances logit-based distillation. TopKD consists of two
main components: (1) a Top-K Scaling Module (TSM), which adaptively amplifies
the most informative logits, and (2) a Top-K Decoupled Loss (TDL), which offers
targeted and effective supervision. Notably, TopKD integrates seamlessly into
existing KD methods without introducing extra modules or requiring
architectural changes. Extensive experiments on CIFAR-100, ImageNet, STL-10,
and Tiny-ImageNet demonstrate that TopKD consistently surpasses
state-of-the-art distillation methods. Moreover, our method demonstrates
substantial effectiveness when distilling Vision Transformers, underscoring its
versatility across diverse network architectures. These findings highlight the
significant potential of logits to advance knowledge distillation.

</details>


### [109] [InceptoFormer: A Multi-Signal Neural Framework for Parkinson's Disease Severity Evaluation from Gait](https://arxiv.org/abs/2508.04540)
*Safwen Naimi,Arij Said,Wassim Bouachir,Guillaume-Alexandre Bilodeau*

Main category: cs.CV

TL;DR: 本文提出了InceptoFormer，一个用于通过步态动力学分析评估帕金森病严重程度的神经框架。结合Inception1D模型与Transformer来捕捉多尺度时间特征和长期依赖关系，并采用过采样策略解决数据不平衡问题，在PD严重程度分类上取得了96.6%的准确率。


<details>
  <summary>Details</summary>
Motivation: 现有帕金森病评估方法在捕捉步态信号中多尺度时间特征和长期依赖关系方面存在挑战，同时评估数据常有类别不平衡问题。为改进PD严重程度分类性能（基于H&Y量表），提出InceptoFormer框架。

Method: 1. 设计1D版Inception模型（Inception1D）：通过不同大小的1D卷积核并行提取多尺度时间特征；2. 使用Transformer组件：建模步态序列长期依赖关系；3. 针对H&Y级别不平衡问题：使用过采样策略增强低代表类别的样本。

Result: 在帕金森病严重程度评估任务上，InceptoFormer达到96.6%的准确率，优于现有最优方法。

Conclusion: InceptoFormer通过协同使用Inception1D（局部特征）与Transformer（全局依赖），并结合过采样策略，有效提升了对复杂步态信号的分析能力，显著改进了PD严重程度的H&Y分类性能。

Abstract: We present InceptoFormer, a multi-signal neural framework designed for
Parkinson's Disease (PD) severity evaluation via gait dynamics analysis. Our
architecture introduces a 1D adaptation of the Inception model, which we refer
to as Inception1D, along with a Transformer-based framework to stage PD
severity according to the Hoehn and Yahr (H&Y) scale. The Inception1D component
captures multi-scale temporal features by employing parallel 1D convolutional
filters with varying kernel sizes, thereby extracting features across multiple
temporal scales. The transformer component efficiently models long-range
dependencies within gait sequences, providing a comprehensive understanding of
both local and global patterns. To address the issue of class imbalance in PD
severity staging, we propose a data structuring and preprocessing strategy
based on oversampling to enhance the representation of underrepresented
severity levels. The overall design enables to capture fine-grained temporal
variations and global dynamics in gait signal, significantly improving
classification performance for PD severity evaluation. Through extensive
experimentation, InceptoFormer achieves an accuracy of 96.6%, outperforming
existing state-of-the-art methods in PD severity assessment. The source code
for our implementation is publicly available at
https://github.com/SafwenNaimi/InceptoFormer

</details>


### [110] [Hierarchical Event Memory for Accurate and Low-latency Online Video Temporal Grounding](https://arxiv.org/abs/2508.04546)
*Minghang Zheng,Yuxin Peng,Benyuan Sun,Yi Yang,Yang Liu*

Main category: cs.CV

TL;DR: 本文提出了针对在线视频时序定位（OnVTG）任务的层次化事件记忆模型，该任务要求在视频流中即时定位与文本查询相关的事件而无法观察未来帧。通过事件层面的建模和长期历史信息的保存，结合未来预测分支，模型在TACoS、ActivityNet Captions和MAD数据集上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 在线视频是持续输入的流数据，现有方法仅存储近期帧特征且缺乏有效的事件建模能力，导致历史信息保存不足且难以捕捉复杂事件结构，故需改进。

Method: 1. 构建基于事件提议的框架，建模不同持续时间的事件级信息；2. 设计层次化事件记忆模块保留长期历史事件；3. 引入未来预测分支实时预测目标事件即将发生的时间点并回归事件开始时间。

Result: 在TACoS、ActivityNet Captions和MAD数据集上达到最先进的性能水平。

Conclusion: 所提层次化事件记忆和事件建模框架显著提升在线视频时序定位性能，未来预测分支增强了实时性。

Abstract: In this paper, we tackle the task of online video temporal grounding (OnVTG),
which requires the model to locate events related to a given text query within
a video stream. Unlike regular video temporal grounding, OnVTG requires the
model to make predictions without observing future frames. As online videos are
streaming inputs and can go on indefinitely, it is impractical and inefficient
to store all historical inputs. The existing OnVTG models employ memory to
store recent historical video frame features and predict scores indicating
whether the current frame corresponds to the start or end time of the target
event. However, these methods lack effective event modeling and cannot retain
long-term historical information, leading to low performance. To tackle these
challenges, we propose a hierarchical event memory for OnVTG. We propose an
event-based OnVTG framework that makes predictions based on event proposals
that model event-level information with various durations. To preserve
historically valuable event information, we introduce a hierarchical event
memory that retains historical events, allowing the model to access both recent
and long-term information. To enable the real-time prediction, we further
propose a future prediction branch that predicts whether the target event will
occur shortly and further regresses the start time of the event. We achieve
state-of-the-art performance on the TACoS, ActivityNet Captions, and MAD
datasets. Code is available at https://github.com/minghangz/OnVTG.

</details>


### [111] [MSC: A Marine Wildlife Video Dataset with Grounded Segmentation and Clip-Level Captioning](https://arxiv.org/abs/2508.04549)
*Quang-Trung Truong,Yuk-Kwan Wong,Vo Hoang Kim Tuyen Dang,Rinaldi Gotama,Duc Thanh Nguyen,Sai-Kit Yeung*

Main category: cs.CV

TL;DR: 该研究提出了一种两阶段海洋对象导向视频描述框架，通过引入包含视频、文本和分割掩码的三元组基准数据集，优化海洋视频的理解与分析。


<details>
  <summary>Details</summary>
Motivation: 现有视频描述数据集难以推广到复杂海洋环境，无法充分理解海洋生物动态和环境变化。海洋视频存在对象动态、相机运动和复杂水下场景等挑战，导致通用或人本中心的现有方法效果不佳。

Method: 1. 提出两阶段流程：首先通过视频分割检测场景突变中的显著对象转换，以丰富描述内容；2. 构建包含视频、文本及分割掩码的三元组基准数据集，支持视觉定位和描述任务；3. 利用对象转换信息增强视频语义理解。

Result: 1. 构建了首个针对海洋视频的综合理解基准；2. 验证了视频分割在提升语义丰富性上的有效性；3. 公开了数据集与代码（https://msc.hkustvgd.com）。

Conclusion: 本研究通过对象导向的流程和三元组数据结构显著提升海洋视频描述性能，方法可推广至视频分析及生成任务，并为海洋生物研究提供新工具。

Abstract: Marine videos present significant challenges for video understanding due to
the dynamics of marine objects and the surrounding environment, camera motion,
and the complexity of underwater scenes. Existing video captioning datasets,
typically focused on generic or human-centric domains, often fail to generalize
to the complexities of the marine environment and gain insights about marine
life. To address these limitations, we propose a two-stage marine
object-oriented video captioning pipeline. We introduce a comprehensive video
understanding benchmark that leverages the triplets of video, text, and
segmentation masks to facilitate visual grounding and captioning, leading to
improved marine video understanding and analysis, and marine video generation.
Additionally, we highlight the effectiveness of video splitting in order to
detect salient object transitions in scene changes, which significantly enrich
the semantics of captioning content. Our dataset and code have been released at
https://msc.hkustvgd.com.

</details>


### [112] [Two-Way Garment Transfer: Unified Diffusion Framework for Dressing and Undressing Synthesis](https://arxiv.org/abs/2508.04551)
*Angang Zhang,Fang Deng,Hao Chen,Zhongjian Chen,Junyan Li*

Main category: cs.CV

TL;DR: 本文提出了首个统一框架TWGTM，同时处理虚拟试穿（VTON）和虚拟脱衣（VTOFF）两项任务，通过双向特征解耦和双条件指导实现双向衣物转移，并采用分阶段训练解决掩模依赖差异问题。实验在DressCode和VITON-HD数据集上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 现有工作孤立处理VTON（为人体穿上衣物）和VTOFF（从穿衣人体提取标准衣物模板）这两个互补任务，缺乏系统性整合。为了弥合这一鸿沟，本文首次提出统一框架来同时解决这两个问题。

Method: 1. 提出双向衣物转移模型（TWGTM）：统一框架通过双向特征解耦同时处理掩模引导的VTON和无掩模的VTOFF；2. 设计双空间指导机制：利用参考图像的隐空间和像素空间特征作为双重条件指导，无缝连接两个任务；3. 分阶段训练策略：针对VTON需要掩模而VTOFF无需掩模的不对称依赖性，采用渐进式训练逐步弥合模态差异。

Result: 在DressCode和VITON-HD数据集上进行定性和定量实验，结果表明所提方法具有高竞争力和有效性。

Conclusion: TWGTM首次统一了VTON与VTOFF任务，证明通过双向解耦和双空间指导能有效实现双向衣物转移，分阶段训练策略成功解决了掩模依赖不对称问题。该框架为衣物中心图像合成提供了新思路。

Abstract: While recent advances in virtual try-on (VTON) have achieved realistic
garment transfer to human subjects, its inverse task, virtual try-off (VTOFF),
which aims to reconstruct canonical garment templates from dressed humans,
remains critically underexplored and lacks systematic investigation. Existing
works predominantly treat them as isolated tasks: VTON focuses on garment
dressing while VTOFF addresses garment extraction, thereby neglecting their
complementary symmetry. To bridge this fundamental gap, we propose the Two-Way
Garment Transfer Model (TWGTM), to the best of our knowledge, the first unified
framework for joint clothing-centric image synthesis that simultaneously
resolves both mask-guided VTON and mask-free VTOFF through bidirectional
feature disentanglement. Specifically, our framework employs dual-conditioned
guidance from both latent and pixel spaces of reference images to seamlessly
bridge the dual tasks. On the other hand, to resolve the inherent mask
dependency asymmetry between mask-guided VTON and mask-free VTOFF, we devise a
phased training paradigm that progressively bridges this modality gap.
Extensive qualitative and quantitative experiments conducted across the
DressCode and VITON-HD datasets validate the efficacy and competitive edge of
our proposed approach.

</details>


### [113] [One Model For All: Partial Diffusion for Unified Try-On and Try-Off in Any Pose](https://arxiv.org/abs/2508.04559)
*Jinxi Liu,Zijian He,Guangrun Wang,Guanbin Li,Liang Lin*

Main category: cs.CV

TL;DR: 提出OMFA（One Model For All）统一扩散框架，支持任意姿势的无展示服装虚拟试穿/脱，通过部分扩散策略实现动态控制，无需分割掩膜，仅需单张肖像和目标姿势，在试穿和试脱任务上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的虚拟试穿方法依赖展示服装和分割掩膜，且处理姿态变化能力有限。这降低了实用性：用户无法轻松将服装从一人迁移到另一人，生成结果姿势受限。OMFA旨在解决这些局限，实现无需展示服装、支持任意姿势的试穿与试脱。

Method: 1. 基于新型部分扩散策略：选择性地对联合输入（服装/人脸/身体）的不同组件施加噪声和去噪，实现动态子任务控制和高效双向变换；2. 完全掩膜无关，输入仅为单张肖像和目标姿势；3. 利用SMPL-X姿态条件，支持单图多视角/任意姿势合成。

Result: 大量实验证明，OMFA在试穿和试脱任务上均实现SOTA性能，提供适用于真实场景的通用解决方案。

Conclusion: OMFA是首个统一处理试穿/脱任务的框架，通过创新部分扩散策略突破现有方法依赖展示服装和分割掩膜的限制，支持任意姿势变换，显著提升实用性和泛化能力。

Abstract: Recent diffusion-based approaches have made significant advances in
image-based virtual try-on, enabling more realistic and end-to-end garment
synthesis. However, most existing methods remain constrained by their reliance
on exhibition garments and segmentation masks, as well as their limited ability
to handle flexible pose variations. These limitations reduce their practicality
in real-world scenarios-for instance, users cannot easily transfer garments
worn by one person onto another, and the generated try-on results are typically
restricted to the same pose as the reference image. In this paper, we introduce
\textbf{OMFA} (\emph{One Model For All}), a unified diffusion framework for
both virtual try-on and try-off that operates without the need for exhibition
garments and supports arbitrary poses. For example, OMFA enables removing
garments from a source person (try-off) and transferring them onto a target
person (try-on), while also allowing the generated target to appear in novel
poses-even without access to multi-pose images of that person. OMFA is built
upon a novel \emph{partial diffusion} strategy that selectively applies noise
and denoising to individual components of the joint input-such as the garment,
the person image, or the face-enabling dynamic subtask control and efficient
bidirectional garment-person transformation. The framework is entirely
mask-free and requires only a single portrait and a target pose as input,
making it well-suited for real-world applications. Additionally, by leveraging
SMPL-X-based pose conditioning, OMFA supports multi-view and arbitrary-pose
try-on from just one image. Extensive experiments demonstrate that OMFA
achieves state-of-the-art results on both try-on and try-off tasks, providing a
practical and generalizable solution for virtual garment synthesis. The project
page is here: https://onemodelforall.github.io/.

</details>


### [114] [CLASP: Cross-modal Salient Anchor-based Semantic Propagation for Weakly-supervised Dense Audio-Visual Event Localization](https://arxiv.org/abs/2508.04566)
*Jinxing Zhou,Ziheng Zhou,Yanghao Zhou,Yuxin Mao,Zhangling Duan,Dan Guo*

Main category: cs.CV

TL;DR: 提出了一种弱监督的密集视听事件定位（W-DAVEL）方法，利用跨模态显著锚点并通过互事件一致性评估、跨模态显著锚点识别和基于锚点的时间传播三个模块提升事件定位性能。在两个数据集上达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 针对密集视听事件定位（DAVEL）任务，先前需要精细的标注框，在弱监督设定下（仅提供视频级别标签）面临更大挑战。本文旨在解决W-DAVEL任务，通过利用跨模态信息弥补弱监督带来的标注不足。

Method: 1. 互事件一致性评估模块：通过度量音频和视觉事件类别预测差异生成一致性评分。2. 跨模态显著锚点识别模块：利用一致性评分，结合全局视频和局部时间窗口机制识别出音频和视觉模态中的显著锚特征。3. 基于锚点的时间传播模块：用这些锚特征增强原始时域特征，提高弱监督下的事件定位能力。

Result: 在UnAV-100和ActivityNet1.3数据集上建立W-DAVEL基准测试。大量实验表明该方法达到最优性能。

Conclusion: 该方法有效利用跨模态一致性信息识别可靠的时间戳（显著锚点），并通过多模块协作实现弱监督下的精确事件定位，解决新任务W-DAVEL的挑战。

Abstract: The Dense Audio-Visual Event Localization (DAVEL) task aims to temporally
localize events in untrimmed videos that occur simultaneously in both the audio
and visual modalities. This paper explores DAVEL under a new and more
challenging weakly-supervised setting (W-DAVEL task), where only video-level
event labels are provided and the temporal boundaries of each event are
unknown. We address W-DAVEL by exploiting \textit{cross-modal salient anchors},
which are defined as reliable timestamps that are well predicted under weak
supervision and exhibit highly consistent event semantics across audio and
visual modalities. Specifically, we propose a \textit{Mutual Event Agreement
Evaluation} module, which generates an agreement score by measuring the
discrepancy between the predicted audio and visual event classes. Then, the
agreement score is utilized in a \textit{Cross-modal Salient Anchor
Identification} module, which identifies the audio and visual anchor features
through global-video and local temporal window identification mechanisms. The
anchor features after multimodal integration are fed into an
\textit{Anchor-based Temporal Propagation} module to enhance event semantic
encoding in the original temporal audio and visual features, facilitating
better temporal localization under weak supervision. We establish benchmarks
for W-DAVEL on both the UnAV-100 and ActivityNet1.3 datasets. Extensive
experiments demonstrate that our method achieves state-of-the-art performance.

</details>


### [115] [Drone Detection with Event Cameras](https://arxiv.org/abs/2508.04564)
*Gabriele Magrini,Lorenzo Berlincioni,Luca Cultrera,Federico Becattini,Pietro Pala*

Main category: cs.CV

TL;DR: 该综述探讨了事件相机在解决传统帧相机难以可靠检测无人机问题中的应用，强调其在消除运动模糊、极端光照下稳定检测以及低延迟特性方面的优势，并涵盖了从检测到跟踪、预测和识别等任务的最新方法。


<details>
  <summary>Details</summary>
Motivation: 传统帧相机在检测小型、高速运动的无人机时存在运动模糊和极端光照条件下的性能不足问题，因此需要更可靠的技术来应对安全和安保挑战。

Method: 综述了事件视觉在无人机检测领域的最新研究，包括事件数据表示方法、尖峰神经网络处理流程，并进一步扩展到实时跟踪、轨迹预测及基于螺旋桨特征识别的技术。

Result: 分析显示事件相机技术为下一代反无人机系统提供了强大的基础，实现了在消除运动模糊、极端光照条件下稳定检测、低延迟和高效处理的核心优势。

Conclusion: 基于事件视觉的技术能够解决传统相机在无人机检测中的主要局限，为可靠、低延迟和高效的反无人机系统铺平道路。

Abstract: The diffusion of drones presents significant security and safety challenges.
Traditional surveillance systems, particularly conventional frame-based
cameras, struggle to reliably detect these targets due to their small size,
high agility, and the resulting motion blur and poor performance in challenging
lighting conditions. This paper surveys the emerging field of event-based
vision as a robust solution to these problems. Event cameras virtually
eliminate motion blur and enable consistent detection in extreme lighting.
Their sparse, asynchronous output suppresses static backgrounds, enabling
low-latency focus on motion cues. We review the state-of-the-art in event-based
drone detection, from data representation methods to advanced processing
pipelines using spiking neural networks. The discussion extends beyond simple
detection to cover more sophisticated tasks such as real-time tracking,
trajectory forecasting, and unique identification through propeller signature
analysis. By examining current methodologies, available datasets, and the
distinct advantages of the technology, this work demonstrates that event-based
vision provides a powerful foundation for the next generation of reliable,
low-latency, and efficient counter-UAV systems.

</details>


### [116] [TAlignDiff: Automatic Tooth Alignment assisted by Diffusion-based Transformation Learning](https://arxiv.org/abs/2508.04565)
*Yunbi Liu,Enqi Tang,Shiyu Li,Lei Ma,Juncheng Li,Shu Lou,Yongchu Pan,Qingshan Liu*

Main category: cs.CV

TL;DR: 本文提出了一种名为TAlignDiff的自动牙齿对齐方法，该方法基于扩散变换学习，结合点云回归网络和扩散矩阵去噪模块，通过双向反馈优化牙齿对齐，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的牙齿对齐方法主要依靠点对点几何约束预测变换矩阵，但忽略了这些矩阵与口腔解剖结构相关的分布特性，因此需要一种能够捕捉这些特性的新方法。

Method: TAlignDiff包含两个部分：点云回归网络（PRN）负责点云层面几何对齐，使用几何约束损失；扩散变换矩阵去噪模块（DTMD）学习临床数据中变换矩阵的隐式分布。二者通过双向反馈机制整合。

Result: 广泛的消融实验和对比实验证明，TAlignDi!在牙齿对齐任务上具有优异性能和优越性。

Conclusion: TAlignDiff有效整合了几何约束和扩散优化，展示了在正畸治疗中的潜力。

Abstract: Orthodontic treatment hinges on tooth alignment, which significantly affects
occlusal function, facial aesthetics, and patients' quality of life. Current
deep learning approaches predominantly concentrate on predicting transformation
matrices through imposing point-to-point geometric constraints for tooth
alignment. Nevertheless, these matrices are likely associated with the
anatomical structure of the human oral cavity and possess particular
distribution characteristics that the deterministic point-to-point geometric
constraints in prior work fail to capture. To address this, we introduce a new
automatic tooth alignment method named TAlignDiff, which is supported by
diffusion-based transformation learning. TAlignDiff comprises two main
components: a primary point cloud-based regression network (PRN) and a
diffusion-based transformation matrix denoising module (DTMD).
Geometry-constrained losses supervise PRN learning for point cloud-level
alignment. DTMD, as an auxiliary module, learns the latent distribution of
transformation matrices from clinical data. We integrate point cloud-based
transformation regression and diffusion-based transformation modeling into a
unified framework, allowing bidirectional feedback between geometric
constraints and diffusion refinement. Extensive ablation and comparative
experiments demonstrate the effectiveness and superiority of our method,
highlighting its potential in orthodontic treatment.

</details>


### [117] [DDTracking: A Deep Generative Framework for Diffusion MRI Tractography with Streamline Local-Global Spatiotemporal Modeling](https://arxiv.org/abs/2508.04568)
*Yijie Li,Wei Zhang,Xi Zhu,Ye Wu,Yogesh Rathi,Lauren J. O'Donnell,Fan Zhang*

Main category: cs.CV

TL;DR: DDTracking是一种新颖的深度生成框架，用于扩散MRI纤维束成像，将流线传播建模为条件去噪扩散过程。该方法通过双路径编码网络联合建模局部空间编码和全局时间依赖，并利用条件扩散模型模块预测流线传播方向。在多个数据集上的实验表明，DDTracking显著超越现有最先进方法，并展现出强大的跨数据集泛化能力。


<details>
  <summary>Details</summary>
Motivation: 扩散MRI纤维束成像需要精确重建白质纤维束，但现有方法在复杂区域或噪声干扰下容易偏离轨道导致重建错误。为了解决这些问题，作者提出引入深度生成模型中的扩散模型来提升纤维束成像的性能。

Method: 1. 提出DDTracking框架，使用条件去噪扩散模型实现纤维束追踪；2. 设计了双路径编码网络，分支一使用基于点的卷积提取局部空间信息，分支二利用GRU模块捕捉全局时间依赖；3. 将局部和全局特征融合后作为条件，指导扩散模型生成纤维束传播方向预测分布；4. 采用标准扩散过程训练：前向过程对真实方向添加噪声，反向过程学习去噪以预测方向；5. 在生成阶段，使用DDIM加速采样，根据预测分布逐步生成整条流线（包含数百个点）。

Result: 1. 在两个权威数据集上评估：ISMRM Challenge（合成数据，含8种不同路径结构）、TractoInferno（临床数据，脑连接组人类项目HCP）；2. 对比方法涵盖经典方法和基于机器学习的前沿算法：Euler、Tensorlines、LAP、RL、CNN-based方法；3. 评价指标：空间精度(VC)、过跟踪错误(OTR)、过连接错误(OCT)。结果表明：a) DDTracking在两种数据集上均达到所有指标第一（合成数据集VC=0.95±0.02，OTR=0.08±0.01）；b) 在合成数据上VC超过次优方法7个百分点；c) 在临床数据上相比CLTR-LAP降低30%的错误率。4. 在跨领域泛化实验中（HCP→Alzheimer's ADNI），DDTracking保持最高稳健性，而所有基准方法均显著退化。

Conclusion: 该研究创新地将条件扩散模型引入纤维束成像任务，通过融合局部空间和全局时间信息的双路径机制提升建模能力。实验证实了其在精度、可靠性和泛化性方面均优于现有技术，为临床MRI分析提供了一种可扩展的端到端解决方案。

Abstract: This paper presents DDTracking, a novel deep generative framework for
diffusion MRI tractography that formulates streamline propagation as a
conditional denoising diffusion process. In DDTracking, we introduce a
dual-pathway encoding network that jointly models local spatial encoding
(capturing fine-scale structural details at each streamline point) and global
temporal dependencies (ensuring long-range consistency across the entire
streamline). Furthermore, we design a conditional diffusion model module, which
leverages the learned local and global embeddings to predict streamline
propagation orientations for tractography in an end-to-end trainable manner. We
conduct a comprehensive evaluation across diverse, independently acquired dMRI
datasets, including both synthetic and clinical data. Experiments on two
well-established benchmarks with ground truth (ISMRM Challenge and
TractoInferno) demonstrate that DDTracking largely outperforms current
state-of-the-art tractography methods. Furthermore, our results highlight
DDTracking's strong generalizability across heterogeneous datasets, spanning
varying health conditions, age groups, imaging protocols, and scanner types.
Collectively, DDTracking offers anatomically plausible and robust tractography,
presenting a scalable, adaptable, and end-to-end learnable solution for broad
dMRI applications. Code is available at:
https://github.com/yishengpoxiao/DDtracking.git

</details>


### [118] [Knowledge to Sight: Reasoning over Visual Attributes via Knowledge Decomposition for Abnormality Grounding](https://arxiv.org/abs/2508.04572)
*Jun Li,Che Liu,Wenjia Bai,Mingxuan Liu,Rossella Arcucci,Cosmin I. Bercea,Julia A. Schnabel*

Main category: cs.CV

TL;DR: 本文提出了一种新的方法解决在医学图像中定位临床发现的问题——K2Sight框架。它通过引入结构化语义监督，将临床概念分解为可解释的视觉属性，从而更高效地训练小型模型。该框架使用不到1.5%的数据训练出性能媲美或超越大型医学VLM的紧凑模型（0.23B和2B参数），在mAP50上提升了9.82%。


<details>
  <summary>Details</summary>
Motivation: 通用视觉语言模型（VLM）在医学应用面临挑战：医学领域术语罕见、组合性强且与视觉模式对齐度差。现有医学VLM需要通过大规模预训练解决此问题，但需要昂贵的标注和计算资源。本文旨在解决这一资源问题。

Method: K2Sight框架：1）将临床概念解构成视觉属性（如形状、密度和解剖位置）；2）从领域本体论提取属性并将其编码成简洁指令式提示词；3）使用指令指导区域-文本对齐训练。该框架利用结构化监督，显式桥接领域知识与空间结构。

Result: 用1.5%的数据训练0.23B和2B参数的小模型：1）在性能上媲美或超越7B+参数医学VLM；2）mAP50提升高达9.82%。实验证明K2Sight在数据效率和模型性能上实现双赢。

Conclusion: K2Sight通过结构化语义监督实现高效医学图像文本对齐，打破大规模预训练依赖。小模型+少数据可达到或超过现有多倍资源密集型模型性能，证明了知识引导的有效性和资源效率。模型开源可促进领域发展。

Abstract: In this work, we address the problem of grounding abnormalities in medical
images, where the goal is to localize clinical findings based on textual
descriptions. While generalist Vision-Language Models (VLMs) excel in natural
grounding tasks, they often struggle in the medical domain due to rare,
compositional, and domain-specific terms that are poorly aligned with visual
patterns. Specialized medical VLMs address this challenge via large-scale
domain pretraining, but at the cost of substantial annotation and computational
resources. To overcome these limitations, we propose \textbf{Knowledge to Sight
(K2Sight)}, a framework that introduces structured semantic supervision by
decomposing clinical concepts into interpretable visual attributes, such as
shape, density, and anatomical location. These attributes are distilled from
domain ontologies and encoded into concise instruction-style prompts, which
guide region-text alignment during training. Unlike conventional report-level
supervision, our approach explicitly bridges domain knowledge and spatial
structure, enabling data-efficient training of compact models. We train compact
models with 0.23B and 2B parameters using only 1.5\% of the data required by
state-of-the-art medical VLMs. Despite their small size and limited training
data, these models achieve performance on par with or better than 7B+ medical
VLMs, with up to 9.82\% improvement in $mAP_{50}$. Code and models:
\href{https://lijunrio.github.io/K2Sight/}{\textcolor{SOTAPink}{https://lijunrio.github.io/K2Sight/}}.

</details>


### [119] [Visual Bias and Interpretability in Deep Learning for Dermatological Image Analysis](https://arxiv.org/abs/2508.04573)
*Enam Ahmed Taufik,Abdullah Khondoker,Antara Firoz Parsa,Seraj Al Mahmud Mostafa*

Main category: cs.CV

TL;DR: 这篇论文提出了一个深度学习框架，用于多类别皮肤疾病分类。研究者系统地评估了三种图像预处理技术（标准RGB、CMY色彩空间转换和CLAHE），并使用多种神经网络模型进行基准测试。结果显示，DinoV2模型配合RGB预处理取得了最高准确率和F1分数。可视化分析进一步证实了模型的可解释性。


<details>
  <summary>Details</summary>
Motivation: 皮肤疾病分类具有挑战性，因为疾病间相似度高、同类病变具有高可变性，且病变纹理复杂。尽管深度学习在自动化皮肤病评估中显示潜力，但其性能高度依赖于图像预处理和模型架构。

Method: 1) 评估三种图像预处理技术：标准RGB、CMY色彩空间转换、CLAHE增强；2) 用DenseNet201、EfficientNetB5和三种Transformer模型（ViT、Swin Transformer、DinoV2 Large）进行多类皮肤病分类；3) 使用准确率和F1分数作为评估指标；4) 应用Grad-CAM对RGB输入进行可视化分析。

Result: 1) DinoV2模型在RGB预处理下达到最高性能：准确率93%，F1分数全面领先；2) RGB预处理整体优于CMY和CLAHE；3) Grad-CAM可视化显示模型能精确定位病变区域，增强可解释性。

Conclusion: 研究证明图像预处理技术和模型选择对皮肤科CAD系统至关重要。DinoV2配合RGB预处理在分类性能和可解释性上均最优，为构建鲁棒且可解释的皮肤病诊断系统提供了有效方案。

Abstract: Accurate skin disease classification is a critical yet challenging task due
to high inter-class similarity, intra-class variability, and complex lesion
textures. While deep learning-based computer-aided diagnosis (CAD) systems have
shown promise in automating dermatological assessments, their performance is
highly dependent on image pre-processing and model architecture. This study
proposes a deep learning framework for multi-class skin disease classification,
systematically evaluating three image pre-processing techniques: standard RGB,
CMY color space transformation, and Contrast Limited Adaptive Histogram
Equalization (CLAHE). We benchmark the performance of pre-trained convolutional
neural networks (DenseNet201, Efficient-NetB5) and transformer-based models
(ViT, Swin Transformer, DinoV2 Large) using accuracy and F1-score as evaluation
metrics. Results show that DinoV2 with RGB pre-processing achieves the highest
accuracy (up to 93%) and F1-scores across all variants. Grad-CAM visualizations
applied to RGB inputs further reveal precise lesion localization, enhancing
interpretability. These findings underscore the importance of effective
pre-processing and model choice in building robust and explainable CAD systems
for dermatology.

</details>


### [120] [Face-voice Association in Multilingual Environments (FAME) 2026 Challenge Evaluation Plan](https://arxiv.org/abs/2508.04592)
*Marta Moscati,Ahmed Abdullah,Muhammad Saad Saeed,Shah Nawaz,Rohan Kumar Das,Muhammad Zaigham Zaheer,Junaid Mir,Muhammad Haroon Yousaf,Khalid Malik,Markus Schedl*

Main category: cs.CV

TL;DR: FAME 2026挑战赛旨在探索多语言环境中的面-声关联，使用MAV-Celeb数据集，报告包含挑战细节、数据集、基线模型和任务说明。


<details>
  <summary>Details</summary>
Motivation: 现实世界中多模态系统应用广泛，尤其音-视系统。由于人脸与声音存在独特关联，且全球半数人口使用双语（常处多语环境），因此需研究多语环境下两者关联。

Method: 挑战赛基于多语言音视频数据集MAV-Celeb展开，提供基线模型评估方法，任务聚焦于构建能关联多语言说话者人脸与声音的跨模态模型。

Result: 报告未提供具体实验结果，但明确了挑战流程：参与者将在多语配对数据集上训练模型，并测试跨语言身份匹配能力。

Conclusion: FAME挑战赛旨在推动多语言环境下音-视关联技术发展，为双语交流场景提供技术解决方案。

Abstract: The advancements of technology have led to the use of multimodal systems in
various real-world applications. Among them, audio-visual systems are among the
most widely used multimodal systems. In the recent years, associating face and
voice of a person has gained attention due to the presence of unique
correlation between them. The Face-voice Association in Multilingual
Environments (FAME) 2026 Challenge focuses on exploring face-voice association
under the unique condition of a multilingual scenario. This condition is
inspired from the fact that half of the world's population is bilingual and
most often people communicate under multilingual scenarios. The challenge uses
a dataset named Multilingual Audio-Visual (MAV-Celeb) for exploring face-voice
association in multilingual environments. This report provides the details of
the challenge, dataset, baseline models, and task details for the FAME
Challenge.

</details>


### [121] [X-SAM: From Segment Anything to Any Segmentation](https://arxiv.org/abs/2508.04655)
*Hao Wang,Limeng Qiao,Zequn Jie,Zhijian Huang,Chengjian Feng,Qingfang Zheng,Lin Ma,Xiangyuan Lan,Xiaodan Liang*

Main category: cs.CV

TL;DR: X-SAM是一个多模态大型语言模型，通过统一的框架和新的视觉基础分割任务VGD，实现了任意分割功能，解决了现有模型在多掩模预测、特定类别分割等方面的局限性，并在多个分割基准测试中达到最佳性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在广泛的知识表示方面表现出强大能力，但缺乏像素级感知理解；而Segment Anything Model（SAM）在视觉提示驱动的图像分割上有显著进展，但在多掩模预测、特定类别分割任务中存在明显限制，且无法将分割任务统一在一个模型中。X-SAM旨在解决这些不足。

Method: 1. 提出X-SAM，一种简化的多模态大型语言模型（MLLM）框架，扩展了从“分割任意”到“任意分割”的范式。\n2. 引入一个统一的新框架，增强MLLM的像素级感知理解能力。\n3. 提出视觉基础分割（VGD）任务，通过交互式视觉提示分割所有实例对象，使MLLM具有像素级可解释能力。\n4. 设计统一训练策略支持多种数据集联合训练。

Result: 实验结果表明，X-SAM在广泛的图像分割基准测试中达到了最先进的性能，突显其在多模态像素级视觉理解上的高效性。

Conclusion: X-SAM通过统一框架和VGD任务，解决了现有模型的局限性，在多个分割任务中实现了高效且高性能的任意分割能力。代码已开源。

Abstract: Large Language Models (LLMs) demonstrate strong capabilities in broad
knowledge representation, yet they are inherently deficient in pixel-level
perceptual understanding. Although the Segment Anything Model (SAM) represents
a significant advancement in visual-prompt-driven image segmentation, it
exhibits notable limitations in multi-mask prediction and category-specific
segmentation tasks, and it cannot integrate all segmentation tasks within a
unified model architecture. To address these limitations, we present X-SAM, a
streamlined Multimodal Large Language Model (MLLM) framework that extends the
segmentation paradigm from \textit{segment anything} to \textit{any
segmentation}. Specifically, we introduce a novel unified framework that
enables more advanced pixel-level perceptual comprehension for MLLMs.
Furthermore, we propose a new segmentation task, termed Visual GrounDed (VGD)
segmentation, which segments all instance objects with interactive visual
prompts and empowers MLLMs with visual grounded, pixel-wise interpretative
capabilities. To enable effective training on diverse data sources, we present
a unified training strategy that supports co-training across multiple datasets.
Experimental results demonstrate that X-SAM achieves state-of-the-art
performance on a wide range of image segmentation benchmarks, highlighting its
efficiency for multimodal, pixel-level visual understanding. Code is available
at https://github.com/wanghao9610/X-SAM.

</details>


### [122] [Pseudo Depth Meets Gaussian: A Feed-forward RGB SLAM Baseline](https://arxiv.org/abs/2508.04597)
*Linqing Zhao,Xiuwei Xu,Yirui Wang,Hao Wang,Wenzhao Zheng,Yansong Tang,Haibin Yan,Jiwen Lu*

Main category: cs.CV

TL;DR: 提出了一种基于3D高斯模型的实时SLAM方法，通过前馈网络加速相机姿态跟踪，结合局部图渲染提升鲁棒性，在保持重建质量的同时大幅减少计算时间。


<details>
  <summary>Details</summary>
Motivation: 现有的姿势自由RGB流三维重建方法（端到端和视觉SLAM类）存在两大问题：(1) 长序列重建困难；(2)深度传感器依赖或测试优化耗时。深度估计器与RGB-D SLAM结合方案受限于预测深度几何细节不精确。实验发现3D高斯映射可解决此问题。

Method: 1. 技术路线：结合3D高斯点云SLAM + 前馈循环预测模块（直接从光流推断相机姿态）。
2. 创新点：
   - 用快速网络推断取代耗时的测试阶段优化（提升跟踪速度90%以上）
   - 提出局部图渲染技术（增强前馈姿态预测鲁棒性）

Result: 在Replica和TUM-RGBD数据集测试表明：
- 重建精度与SplaTAM相当
- 跟踪时间减少90%以上
- 具备真实场景部署能力

Conclusion: 通过将深度网络特性（深度估计、光流位姿预测）与3D高斯SLAM创新融合，在保持重建质量的前提下实现了近乎实时的跟踪速度，为移动端实时三维重建提供了新方案。

Abstract: Incrementally recovering real-sized 3D geometry from a pose-free RGB stream
is a challenging task in 3D reconstruction, requiring minimal assumptions on
input data. Existing methods can be broadly categorized into end-to-end and
visual SLAM-based approaches, both of which either struggle with long sequences
or depend on slow test-time optimization and depth sensors. To address this, we
first integrate a depth estimator into an RGB-D SLAM system, but this approach
is hindered by inaccurate geometric details in predicted depth. Through further
investigation, we find that 3D Gaussian mapping can effectively solve this
problem. Building on this, we propose an online 3D reconstruction method using
3D Gaussian-based SLAM, combined with a feed-forward recurrent prediction
module to directly infer camera pose from optical flow. This approach replaces
slow test-time optimization with fast network inference, significantly
improving tracking speed. Additionally, we introduce a local graph rendering
technique to enhance robustness in feed-forward pose prediction. Experimental
results on the Replica and TUM-RGBD datasets, along with a real-world
deployment demonstration, show that our method achieves performance on par with
the state-of-the-art SplaTAM, while reducing tracking time by more than 90\%.

</details>


### [123] [YOLOv8-Based Deep Learning Model for Automated Poultry Disease Detection and Health Monitoring paper](https://arxiv.org/abs/2508.04658)
*Akhil Saketh Reddy Sabbella,Ch. Lakshmi Prachothan,Eswar Kumar Panta*

Main category: cs.CV

TL;DR: 本文提出了一种基于YOLO v8深度学习模型的实时检测方法，用于家禽工业中鸡病的检测。该方法通过分析高分辨率鸡只照片，识别行为和外貌异常，实现实时检测并预警，以减少人工检查的需求和提高农场管理效率。


<details>
  <summary>Details</summary>
Motivation: 传统依赖人工观察的鸡病检测方法耗时且易出错，给家禽业带来经济损失。因此，开发一种自动化、实时并准确的检测系统是必要的。

Method: 使用YOLO v8深度学习模型对高分辨率鸡只照片进行实时分析，检测疾病迹象（如行为和外貌异常）。方法包括构建一个大型标注数据集用于训练模型，以及开发一个系统来处理图像并发出实时预警通知给农场操作员。

Result: 该系统能够准确且实时地识别患病鸡只，并提供即时警报。通过实现早期感染检测，减少人工检查需求，并提升大规模养殖场的生物安全，从而提高鸡群健康管理。

Conclusion: 利用YOLO v8的实时物体识别能力，该AI技术为家禽养殖场提供了一种可扩展、高效的疾病检测方法，优化了农场管理实践。

Abstract: In the poultry industry, detecting chicken illnesses is essential to avoid
financial losses. Conventional techniques depend on manual observation, which
is laborious and prone to mistakes. Using YOLO v8 a deep learning model for
real-time object recognition. This study suggests an AI based approach, by
developing a system that analyzes high resolution chicken photos, YOLO v8
detects signs of illness, such as abnormalities in behavior and appearance. A
sizable, annotated dataset has been used to train the algorithm, which provides
accurate real-time identification of infected chicken and prompt warnings to
farm operators for prompt action. By facilitating early infection
identification, eliminating the need for human inspection, and enhancing
biosecurity in large-scale farms, this AI technology improves chicken health
management. The real-time features of YOLO v8 provide a scalable and effective
method for improving farm management techniques.

</details>


### [124] [HierarchicalPrune: Position-Aware Compression for Large-Scale Diffusion Models](https://arxiv.org/abs/2508.04663)
*Young D. Kwon,Rui Li,Sijia Li,Da Li,Sourav Bhattacharya,Stylianos I. Venieris*

Main category: cs.CV

TL;DR: 提出的新压缩框架HierarchicalPrune：基于对文本到图像扩散模型中不同块功能层级差异的观察，通过对位置层级进行剪枝、保护层级模型权重和基于敏感性的知识蒸馏优化技术，在保持原模型输出图像质量的同时，显著降低了计算资源需求。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像扩散模型参数量巨大（约8-11B），导致在资源受限的设备上推理困难。为解决此问题，本文提出了一种基于模型功能层级特性的压缩方法。

Method: 1) 分层位置剪枝：根据模型早块构建语义结构、后块负责纹理细化的特性，移除较不重要的后块；
2) 位置权重保护：系统性地保护模型早期关键权重；
3) 敏感性蒸馏：基于区块敏感度差异执行强度不同的知识蒸馏。

Result: 联合INT4量化后：①内存占用减少77.5-80.4%（从15.8 GB降低到3.2 GB）；②延迟降低27.9-38.0%；③GenEval与HPSv2指标仅下降2.6%与7%；④85人实验显示感知质量与原始模型相当且优于现有方法。

Conclusion: 提出模型层级压缩框架，在明显降低资源的前提下维持图像生成质量，为资源受限设备部署大模型提供了有效方案。

Abstract: State-of-the-art text-to-image diffusion models (DMs) achieve remarkable
quality, yet their massive parameter scale (8-11B) poses significant challenges
for inferences on resource-constrained devices. In this paper, we present
HierarchicalPrune, a novel compression framework grounded in a key observation:
DM blocks exhibit distinct functional hierarchies, where early blocks establish
semantic structures while later blocks handle texture refinements.
HierarchicalPrune synergistically combines three techniques: (1) Hierarchical
Position Pruning, which identifies and removes less essential later blocks
based on position hierarchy; (2) Positional Weight Preservation, which
systematically protects early model portions that are essential for semantic
structural integrity; and (3) Sensitivity-Guided Distillation, which adjusts
knowledge-transfer intensity based on our discovery of block-wise sensitivity
variations. As a result, our framework brings billion-scale diffusion models
into a range more suitable for on-device inference, while preserving the
quality of the output images. Specifically, when combined with INT4 weight
quantisation, HierarchicalPrune achieves 77.5-80.4% memory footprint reduction
(e.g., from 15.8 GB to 3.2 GB) and 27.9-38.0% latency reduction, measured on
server and consumer grade GPUs, with the minimum drop of 2.6% in GenEval score
and 7% in HPSv2 score compared to the original model. Last but not least, our
comprehensive user study with 85 participants demonstrates that
HierarchicalPrune maintains perceptual quality comparable to the original model
while significantly outperforming prior works.

</details>


### [125] [How Does Bilateral Ear Symmetry Affect Deep Ear Features?](https://arxiv.org/abs/2508.04614)
*Kagan Ozturk,Deeksha Arun,Kevin W. Bowyer,Patrick Flynn*

Main category: cs.CV

TL;DR: 该论文研究了双耳对称性对CNN耳部识别效果的影响，发现训练和测试时分别处理左右耳能显著提升性能，并提供关于对齐策略、输入大小和超参数设置的实用见解。


<details>
  <summary>Details</summary>
Motivation: 现有耳部识别研究主要关注直接从原始图像学习特征的CNN方法，但忽视了双耳对称性对特征学习的影响。因此，本文旨在探究双耳对称性如何影响CNN耳部识别的效果。

Method: 1. 开发自动区分左右耳的耳朵侧边分类器。 2. 在训练和测试阶段引入该侧边信息（分侧处理）。 3. 在五个数据集上进行跨数据集评估。 4. 通过消融实验分析对齐策略、输入大小和超参数设置的影响。

Result: 实验表明：1. 训练和测试时分别处理左右耳可显著提升识别性能；2. 消融研究为大规模数据集上CNN耳部识别系统的优化提供了具体指导（如对齐方式选择等）。

Conclusion: 双耳对称性是提升CNN耳部识别精度的关键因素，分侧处理策略能有效利用该特性；同时论文提供的实践经验对优化系统实现更高验证率具有参考价值。

Abstract: Ear recognition has gained attention as a reliable biometric technique due to
the distinctive characteristics of human ears. With the increasing availability
of large-scale datasets, convolutional neural networks (CNNs) have been widely
adopted to learn features directly from raw ear images, outperforming
traditional hand-crafted methods. However, the effect of bilateral ear symmetry
on the features learned by CNNs has received little attention in recent
studies. In this paper, we investigate how bilateral ear symmetry influences
the effectiveness of CNN-based ear recognition. To this end, we first develop
an ear side classifier to automatically categorize ear images as either left or
right. We then explore the impact of incorporating this side information during
both training and test. Cross-dataset evaluations are conducted on five
datasets. Our results suggest that treating left and right ears separately
during training and testing can lead to notable performance improvements.
Furthermore, our ablation studies on alignment strategies, input sizes, and
various hyperparameter settings provide practical insights into training
CNN-based ear recognition systems on large-scale datasets to achieve higher
verification rates.

</details>


### [126] [FinMMR: Make Financial Numerical Reasoning More Multimodal, Comprehensive, and Challenging](https://arxiv.org/abs/2508.04625)
*Zichen Tang,Haihong E,Jiacheng Liu,Zhongjun Yang,Rongjin Li,Zihua Rong,Haoyang He,Zhuodi Hao,Xinyang Hu,Kun Ji,Ziyan Ma,Mengyuan Ji,Jun Zhang,Chenghao Ma,Qianhe Zheng,Yang Liu,Yiling Huang,Xinyi Hu,Qing Huang,Zijian Xie,Shiyao Peng*

Main category: cs.CV

TL;DR: FinMMR论文提出了一种专为评估多模态大语言模型在金融数值推理任务中推理能力而设计的双语多模态基准。该基准在三个方面进行了创新：多模态性、全面性和挑战性。FinMMR包含4.3K个问题和8.7K张图片，涵盖14个金融子领域。最强的MLLM模型在难题上仅达到53.0%的准确率。


<details>
  <summary>Details</summary>
Motivation: 现有的基准在评估MLLMs处理金融数值推理任务时存在不足，特别是在多模态性、全面性和挑战性方面。FinMMR旨在填补这一空白，推动MLLMs在金融领域实际应用的发展。

Method: 1. 多模态性：将现有金融推理基准问题转化为多模态格式，并从最新的中文研究报告中构建新问题。最终创建一个包含4.3K个问题、8.7K张图片（包括表格、柱状图、股权结构图等）的数据集。
2. 全面性：覆盖14个金融子领域（如公司金融、银行业、行业分析等），扩展了金融知识广度。
3. 挑战性：要求模型结合金融知识和多模态理解进行多步精确数值推理。

Result: FinMMR基准包含14个类别，4.3K个问题，8.7K张图像。在基准测试中，表现最好的MLLM仅在难题上达到53%的准确率，表明该任务的挑战性。

Conclusion: FinMMR作为首个针对金融数值推理的多模态基准，具有多模态、全面和挑战性的特点，能够全面评估MLLMs在金融领域的应用潜力。该基准预计将推动MLLMs在复杂实际场景中推理能力的提升。

Abstract: We present FinMMR, a novel bilingual multimodal benchmark tailored to
evaluate the reasoning capabilities of multimodal large language models (MLLMs)
in financial numerical reasoning tasks. Compared to existing benchmarks, our
work introduces three significant advancements. (1) Multimodality: We
meticulously transform existing financial reasoning benchmarks, and construct
novel questions from the latest Chinese financial research reports. FinMMR
comprises 4.3K questions and 8.7K images spanning 14 categories, including
tables, bar charts, and ownership structure charts. (2) Comprehensiveness:
FinMMR encompasses 14 financial subdomains, including corporate finance,
banking, and industry analysis, significantly exceeding existing benchmarks in
financial domain knowledge breadth. (3) Challenge: Models are required to
perform multi-step precise numerical reasoning by integrating financial
knowledge with the understanding of complex financial images and text. The
best-performing MLLM achieves only 53.0% accuracy on Hard problems. We believe
that FinMMR will drive advancements in enhancing the reasoning capabilities of
MLLMs in real-world scenarios.

</details>


### [127] [EncQA: Benchmarking Vision-Language Models on Visual Encodings for Charts](https://arxiv.org/abs/2508.04650)
*Kushin Mukherjee,Donghao Ren,Dominik Moritz,Yannick Assogba*

Main category: cs.CV

TL;DR: EncQA是一个新的图表理解基准，旨在系统评估多模态视觉语言模型在多种视觉编码和任务上的表现，揭示了模型性能在编码和任务间的显著差异，并指出单纯扩大模型规模可能无法解决所有的视觉推理问题。


<details>
  <summary>Details</summary>
Motivation: 当前图表理解基准的进步未能完全覆盖图表解释所需的视觉推理能力。为了系统评估模型在多种视觉编码和任务上的表现，研究团队引入了EncQA基准。

Method: EncQA基准通过2076个合成问答对，覆盖了六种视觉编码通道（位置、长度、面积、颜色定量、颜色名义、形状）和八种任务（寻找极值、检索值、发现异常、过滤值、计算派生值精确值、计算派生值相对值、关联值、关联相对值）。并利用该基准评估了9个最先进的多模态视觉语言模型。

Result: 评估显示，模型在相同任务下的不同编码之间以及不同任务之间的性能存在显著差异。此外，对于许多任务-编码对，模型大小与性能提升无关，这表明单纯扩大模型规模可能无法解决所有图表理解问题。

Conclusion: 提升图表理解能力需要针对特定的视觉推理差距采取针对性策略，而不仅仅是扩大模型或数据集的规模。

Abstract: Multimodal vision-language models (VLMs) continue to achieve ever-improving
scores on chart understanding benchmarks. Yet, we find that this progress does
not fully capture the breadth of visual reasoning capabilities essential for
interpreting charts. We introduce EncQA, a novel benchmark informed by the
visualization literature, designed to provide systematic coverage of visual
encodings and analytic tasks that are crucial for chart understanding. EncQA
provides 2,076 synthetic question-answer pairs, enabling balanced coverage of
six visual encoding channels (position, length, area, color quantitative, color
nominal, and shape) and eight tasks (find extrema, retrieve value, find
anomaly, filter values, compute derived value exact, compute derived value
relative, correlate values, and correlate values relative). Our evaluation of 9
state-of-the-art VLMs reveals that performance varies significantly across
encodings within the same task, as well as across tasks. Contrary to
expectations, we observe that performance does not improve with model size for
many task-encoding pairs. Our results suggest that advancing chart
understanding requires targeted strategies addressing specific visual reasoning
gaps, rather than solely scaling up model or dataset size.

</details>


### [128] [PixCuboid: Room Layout Estimation from Multi-view Featuremetric Alignment](https://arxiv.org/abs/2508.04659)
*Gustav Hanning,Kalle Åström,Viktor Larsson*

Main category: cs.CV

TL;DR: PixCuboid是一个基于优化和多视图对齐的立方体形状房间布局估计方法。它在训练中将优化问题端到端融合，学习到的特征地图能够提供较大的收敛区域和平滑的损失景观，支持用简单启发式方法初始化房间布局。通过提出的ScanNet++和2D-3D-Semantics基准测试进行评估，表现优于现有方法，并可扩展到多房间估计。


<details>
  <summary>Details</summary>
Motivation: 现有的房间布局估计方法主要基于全景单视图，作者提出利用多视图对齐解决该问题。

Method: 1. 提出一个基于优化的方法PixCuboid，核心思想是利用多视图对齐策略，在多个图像视图间对齐几何特征。2. 训练时将优化过程嵌入端到端学习，特征图被设计为具有大收敛盆地和平滑损失曲面。3. 布局通过一个启发式初始化，然后在优化空间中调整。4. 允许一个模型扩展到多房间布局估计。

Result: 在ScanNet++和2D-3D-Semantics新基准上验证：1. 单房间布局估计大幅超越现有方法；2. 模型能够成功延伸应用到多房间场景。

Conclusion: 端到端优化显著改善了布局参数的估计，特征图的平滑性为优化提供了更宽广的起点；多视图对应对提升性能有重要帮助；可扩展性强。

Abstract: Coarse room layout estimation provides important geometric cues for many
downstream tasks. Current state-of-the-art methods are predominantly based on
single views and often assume panoramic images. We introduce PixCuboid, an
optimization-based approach for cuboid-shaped room layout estimation, which is
based on multi-view alignment of dense deep features. By training with the
optimization end-to-end, we learn feature maps that yield large convergence
basins and smooth loss landscapes in the alignment. This allows us to
initialize the room layout using simple heuristics.
  For the evaluation we propose two new benchmarks based on ScanNet++ and
2D-3D-Semantics, with manually verified ground truth 3D cuboids. In thorough
experiments we validate our approach and significantly outperform the
competition. Finally, while our network is trained with single cuboids, the
flexibility of the optimization-based approach allow us to easily extend to
multi-room estimation, e.g. larger apartments or offices. Code and model
weights are available at https://github.com/ghanning/PixCuboid.

</details>


### [129] [ANPrompt: Anti-noise Prompt Tuning for Vision-Language Models](https://arxiv.org/abs/2508.04677)
*Yansheng Gao,Yufei Zheng,Jinghan Qu,Zixi Zhu,Yukuan Zhang,Shengsheng Wang*

Main category: cs.CV

TL;DR: 提出了ANPrompt框架，用于增强视觉语言模型在弱语义扰动下的鲁棒性。通过融合原始文本和噪声文本构建抗噪提示，并引入新的损失函数WALoss，在11个基准测试上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的提示调整方法在面对弱语义扰动（例如图像或文本噪声）时表现脆弱，导致模型在未见类别上的泛化能力下降。为解决这一问题，论文提出了抗噪声提示框架ANPrompt。

Method: 1. 融合原始文本和噪声扰动文本特征，聚类形成噪声提示。2.将噪声提示与可学习的提示标记集成，生成抗噪提示，注入图像和文本编码器深层。3. 计算噪声抗性视觉提示原型（NRVPP），平均视觉编码器输出的提示标记。4.除了标准交叉熵损失和相似度损失外，引入弱语义噪声对齐损失（WALoss）作为对齐、鲁棒性和抗噪目标。

Result: 实验在11个基准测试上进行，ANPrompt在语义噪声鲁棒性和新类别泛化方面均优于现有提示调整方法。

Conclusion: ANPrompt通过抗噪提示和新的损失函数有效提升了视觉语言模型在弱语义扰动下的鲁棒性和泛化能力，并取得了显著的性能提升。

Abstract: Prompt tuning has emerged as an efficient and effective technique for
adapting vision-language models (VLMs) with low computational overhead.
However, existing methods often overlook the vulnerability of prompt-tuned VLMs
to weak semantic perturbations-such as subtle image or text noise-that degrade
their generalization to unseen classes. To address this limitation, we propose
ANPrompt, a novel prompt tuning framework designed to enhance robustness under
such perturbations. ANPrompt first constructs weak noise text features by
fusing original and noise-perturbed text embeddings, which are then clustered
to form noise prompts. These noise prompts are integrated with learnable prompt
tokens to generate anti-noise prompts, which are injected into the deeper
layers of both image and text encoders. To further capture the noise-aware
visual semantics, ANPrompt computes the Noise-Resistant Visual Prompt Prototype
(NRVPP) by averaging the output prompt tokens from the vision encoder. Finally,
ANPrompt introduces alignment, robustness, and anti-noise objectives by
computing a Weak semantic noise Alignment Loss (WALoss) alongside the standard
cross-entropy and sim loss. Experiments across 11 benchmarks demonstrate that
ANPrompt consistently outperforms existing prompt tuning approaches, achieving
superior robustness to semantic noise and improved generalization to novel
categories.

</details>


### [130] [Perceiving and Acting in First-Person: A Dataset and Benchmark for Egocentric Human-Object-Human Interactions](https://arxiv.org/abs/2508.04681)
*Liang Xu,Chengqun Yang,Zili Lin,Fei Xu,Yifan Liu,Congsheng Xu,Yiyi Zhang,Jie Qin,Xingdong Sheng,Yunhui Liu,Xin Jin,Yichao Yan,Wenjun Zeng,Xiaokang Yang*

Main category: cs.CV

TL;DR: 论文提出了InterVLA数据集，一个大规模的人类-物体-人类互动数据集，包含11.4小时多模态数据，旨在促进物理世界AI代理的研究。


<details>
  <summary>Details</summary>
Motivation: 现有数据集大多只关注专业交互类别，且忽视了AI助手应以第一人称视角感知和行动。论文强调通用交互知识和自我中心（egocentric）视角的重要性，因此构建了包含多视角、多模态数据的大规模交互数据集。

Method: 在视觉-语言-动作框架中嵌入人工辅助任务，通过混合RGB-MoCap系统采集数据。助理根据自我中心视角和指令为指令者提供服务，互动脚本由GPT生成。数据包含2个第一人称视角和5个第三人称视角视频，以及1.2M帧的多模态数据（RGB图像、准确的动作捕捉数据和语音指令）。

Result: 构建了InterVLA数据集（11.4小时），建立了三项新基准任务：自我中心人体运动估计、交互合成和交互预测，并提供了全面的分析。

Conclusion: InterVLA数据集和相应的基准任务将推动物理世界AI代理的发展，尤其是在自我中心视角下的人类交互理解方面。

Abstract: Learning action models from real-world human-centric interaction datasets is
important towards building general-purpose intelligent assistants with
efficiency. However, most existing datasets only offer specialist interaction
category and ignore that AI assistants perceive and act based on first-person
acquisition. We urge that both the generalist interaction knowledge and
egocentric modality are indispensable. In this paper, we embed the
manual-assisted task into a vision-language-action framework, where the
assistant provides services to the instructor following egocentric vision and
commands. With our hybrid RGB-MoCap system, pairs of assistants and instructors
engage with multiple objects and the scene following GPT-generated scripts.
Under this setting, we accomplish InterVLA, the first large-scale
human-object-human interaction dataset with 11.4 hours and 1.2M frames of
multimodal data, spanning 2 egocentric and 5 exocentric videos, accurate
human/object motions and verbal commands. Furthermore, we establish novel
benchmarks on egocentric human motion estimation, interaction synthesis, and
interaction prediction with comprehensive analysis. We believe that our
InterVLA testbed and the benchmarks will foster future works on building AI
agents in the physical world.

</details>


### [131] [TurboTrain: Towards Efficient and Balanced Multi-Task Learning for Multi-Agent Perception and Prediction](https://arxiv.org/abs/2508.04682)
*Zewei Zhou,Seth Z. Zhao,Tianhui Cai,Zhiyu Huang,Bolei Zhou,Jiaqi Ma*

Main category: cs.CV

TL;DR: 提出了名为TurboTrain的高效训练框架，用于多智能体感知与预测。该框架包括基于掩码重建学习的多智能体时空预训练方案和基于梯度冲突抑制的平衡多任务学习策略，可简化训练过程，减少训练时间并提升性能。实验在V2XPnP-Seq数据集上进行，验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统的端到端训练在提高多任务性能方面优势显著，但现有训练过程复杂且需要大量人工设计与监控。为此，本文旨在提出一种高效训练框架以解决这些问题。

Method: 1) 多智能体时空预训练：采用掩码重建学习方案，促进对多智能体时空特征的建模；2) 平衡多任务学习：提出基于梯度冲突抑制的策略，优化检测与预测任务的联合训练。框架免除了复杂多阶段训练流程的设计需求。

Result: 在真实世界合作驾驶数据集V2XPnP-Seq上的实验表明，TurboTrain显著提升了SOTA多智能体感知和预测模型的性能。预训练有效捕获了时空特征并提升下游任务，平衡多任务学习策略也提升了检测与预测性能。

Conclusion: TurboTrain为多智能体系统提供了一种高效训练框架，通过预训练与平衡多任务学习策略，实现了训练过程的简化和性能的全面提升，未来可探索该框架在更广泛场景的应用。

Abstract: End-to-end training of multi-agent systems offers significant advantages in
improving multi-task performance. However, training such models remains
challenging and requires extensive manual design and monitoring. In this work,
we introduce TurboTrain, a novel and efficient training framework for
multi-agent perception and prediction. TurboTrain comprises two key components:
a multi-agent spatiotemporal pretraining scheme based on masked reconstruction
learning and a balanced multi-task learning strategy based on gradient conflict
suppression. By streamlining the training process, our framework eliminates the
need for manually designing and tuning complex multi-stage training pipelines,
substantially reducing training time and improving performance. We evaluate
TurboTrain on a real-world cooperative driving dataset, V2XPnP-Seq, and
demonstrate that it further improves the performance of state-of-the-art
multi-agent perception and prediction models. Our results highlight that
pretraining effectively captures spatiotemporal multi-agent features and
significantly benefits downstream tasks. Moreover, the proposed balanced
multi-task learning strategy enhances detection and prediction.

</details>


### [132] [BEVCon: Advancing Bird's Eye View Perception with Contrastive Learning](https://arxiv.org/abs/2508.04702)
*Ziyang Leng,Jiawei Yang,Zhicheng Ren,Bolei Zhou*

Main category: cs.CV

TL;DR: BEVCon 是一种简单有效的对比学习框架，旨在提升自动驾驶中的鸟瞰图（BEV）感知性能。它通过两个对比学习模块——实例特征对比模块和视角对比模块——优化 BEV 特征和图像骨干网络，从而在 BEV 编码器和骨干网络层面均实现了特征表示增强。在 nuScenes 数据集上的实验表明，BEVCon 显著提升了性能，最高可超过最先进基线方法 2.4% mAP。


<details>
  <summary>Details</summary>
Motivation: 现有 BEV 感知研究主要集中于优化 BEV 编码器和任务特定头部，但忽略了表征学习的潜力。BEVCon 的提出是为了填补这一空白，通过对比学习强化 BEV 模型的特征表示能力。

Method: BEVCon 包含两个对比学习模块：1）实例特征对比模块：通过对比学习细化 BEV 特征，使同类实例特征更紧凑、异类更分散；2）视角对比模块：增强图像骨干网络的表征能力，通过对比不同视角（如透视视图与 BEV）的特征一致性。这两个模块与检测损失联合优化，实现端到端训练。整个框架不改变模型主干结构，仅通过对比损失监督特征空间。

Result: 在 nuScenes 数据集上的实验结果表明：1）BEVCon 在 BEV 感知任务（如 3D 目标检测）上表现一致提升；2）最高实现 +2.4% mAP（平均精度均值）提升，优于现有最先进基线；3）消融实验证实两个对比模块均有效。

Conclusion: 论文揭示了表征学习在 BEV 感知中的关键作用：1）对比学习可作为任务特定优化的补充路径；2）BEVCon 的轻量设计证明无需复杂结构修改即可实现显著提升；3）未来可探索更多自监督机制与多模态扩展。

Abstract: We present BEVCon, a simple yet effective contrastive learning framework
designed to improve Bird's Eye View (BEV) perception in autonomous driving. BEV
perception offers a top-down-view representation of the surrounding
environment, making it crucial for 3D object detection, segmentation, and
trajectory prediction tasks. While prior work has primarily focused on
enhancing BEV encoders and task-specific heads, we address the underexplored
potential of representation learning in BEV models. BEVCon introduces two
contrastive learning modules: an instance feature contrast module for refining
BEV features and a perspective view contrast module that enhances the image
backbone. The dense contrastive learning designed on top of detection losses
leads to improved feature representations across both the BEV encoder and the
backbone. Extensive experiments on the nuScenes dataset demonstrate that BEVCon
achieves consistent performance gains, achieving up to +2.4% mAP improvement
over state-of-the-art baselines. Our results highlight the critical role of
representation learning in BEV perception and offer a complementary avenue to
conventional task-specific optimizations.

</details>


### [133] [Occupancy Learning with Spatiotemporal Memory](https://arxiv.org/abs/2508.04705)
*Ziyang Leng,Jiawei Yang,Wenlong Yi,Bolei Zhou*

Main category: cs.CV

TL;DR: 为了解决多输入帧下3D占用表示在时间和空间上聚合的挑战，研究者提出ST-Occ框架，通过设计时空存储器和基于不确定性和动态感知的注意力机制，提升了3D占用预测的性能，并在实验中展现优于现有方法的性能和时间一致性。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶中，3D占用表示需要多帧输入以捕捉环境的时空动态，但传统方法因计算成本高、占用体素的不确定性和动态变化无法有效解决。如何在保持时间一致性的同时高效建模成了关键问题。

Method: 提出ST-Occ框架，包含两个核心模块：1. 时空存储器（spatiotemporal memory），以场景级表示高效捕获和存储历史信息；2. 内存注意力（memory attention）机制，结合不确定性和动态感知模型，将当前输入与存储器内容进行有效融合，从而学习时空特征。

Result: 在3D占用预测任务中，ST-Occ在主流测试指标mIoU（平均交并比）上比现有方法提升了3%，同时时间不一致性指标减少了29%。

Conclusion: 研究表明ST-Occ通过融合时空信息和增强时间一致性，有效改善了3D占用预测的准确性和稳定性，可应用于需要多帧建模的自主驾驶感知系统。

Abstract: 3D occupancy becomes a promising perception representation for autonomous
driving to model the surrounding environment at a fine-grained scale. However,
it remains challenging to efficiently aggregate 3D occupancy over time across
multiple input frames due to the high processing cost and the uncertainty and
dynamics of voxels. To address this issue, we propose ST-Occ, a scene-level
occupancy representation learning framework that effectively learns the
spatiotemporal feature with temporal consistency. ST-Occ consists of two core
designs: a spatiotemporal memory that captures comprehensive historical
information and stores it efficiently through a scene-level representation and
a memory attention that conditions the current occupancy representation on the
spatiotemporal memory with a model of uncertainty and dynamic awareness. Our
method significantly enhances the spatiotemporal representation learned for 3D
occupancy prediction tasks by exploiting the temporal dependency between
multi-frame inputs. Experiments show that our approach outperforms the
state-of-the-art methods by a margin of 3 mIoU and reduces the temporal
inconsistency by 29%.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [134] [How Deep Is Representational Bias in LLMs? The Cases of Caste and Religion](https://arxiv.org/abs/2508.03712)
*Agrima Seth,Monojit Choudhary,Sunayana Sitaram,Kentaro Toyama,Aditya Vashistha,Kalika Bali*

Main category: cs.CL

TL;DR: 论文拓展了大型语言模型（LLM）中表征偏见的研究，通过系统化审核GPT-4 Turbo在印度宗教和种姓维度上的表现，发现模型倾向于过度代表文化主导群体，且提示语的多样性设计效果有限。


<details>
  <summary>Details</summary>
Motivation: 现有对LLM表征偏见的研究多聚焦于种族和性别等全球北方身份维度，且大多通过单次响应进行测量。本研究旨在深入探究偏见在模型中的深度编码情况，并将其延伸到印度语境下较少被探索的宗教和种姓维度。

Method: 1. 设计提示语体系，要求GPT-4 Turbo生成超7200篇关于印度重要生活事件的叙事；2. 提示语刻意设计为不同程度的多样性诱导（从明确指令到模糊暗示）；3. 将模型输出的宗教/种姓分布与印度人口普查数据进行统计对比；4. 量化偏见的“存在度”与“顽固性”（通过连续提示干预的响应情况）。

Result: 1. GPT-4的响应持续偏离统计现实，显著高估文化主导群体存在（如印度教高种姓）；2. 表征偏见呈现“赢家通吃”特性——即使训练数据分布更均衡，模型输出仍高度倾斜；3. 多样性提示效果有限：明确指令时偏见略减（但仍严重），模糊提示则完全失效；4. 连续提示干预的效果不稳定且微弱。

Conclusion: 仅靠增加训练数据多样性无法解决LLM表征偏见问题。研究揭示：1. 模型对主导群体存在过度泛化机制；2. 提示干预对深层次偏见矫正效果有限。呼吁开发阶段需结合数据调整与架构级纠偏方法（如偏见解码算法）。

Abstract: Representational bias in large language models (LLMs) has predominantly been
measured through single-response interactions and has focused on Global
North-centric identities like race and gender. We expand on that research by
conducting a systematic audit of GPT-4 Turbo to reveal how deeply encoded
representational biases are and how they extend to less-explored dimensions of
identity. We prompt GPT-4 Turbo to generate over 7,200 stories about
significant life events (such as weddings) in India, using prompts designed to
encourage diversity to varying extents. Comparing the diversity of religious
and caste representation in the outputs against the actual population
distribution in India as recorded in census data, we quantify the presence and
"stickiness" of representational bias in the LLM for religion and caste. We
find that GPT-4 responses consistently overrepresent culturally dominant groups
far beyond their statistical representation, despite prompts intended to
encourage representational diversity. Our findings also suggest that
representational bias in LLMs has a winner-take-all quality that is more biased
than the likely distribution bias in their training data, and repeated
prompt-based nudges have limited and inconsistent efficacy in dislodging these
biases. These results suggest that diversifying training data alone may not be
sufficient to correct LLM bias, highlighting the need for more fundamental
changes in model development. Dataset and Codebook:
https://github.com/agrimaseth/How-Deep-Is-Representational-Bias-in-LLMs

</details>


### [135] [FeynTune: Large Language Models for High-Energy Theory](https://arxiv.org/abs/2508.03716)
*Paul Richmond,Prarit Agarwal,Borun Chowdhury,Vasilis Niarchos,Constantinos Papageorgakis*

Main category: cs.CL

TL;DR: 微调Llama-3.1模型用于高能理论物理领域20种变体


<details>
  <summary>Details</summary>
Motivation: 为高能理论物理(hep-th, hep-ph, gr-qc)开发专用语言模型

Method: 1. 使用arXiv摘要(至2024年8月)训练20个不同数据集组合的模型变体
2. 采用两种低秩自适应(LoRA)微调方法
3. 包含不同领域(q-bio, cs)的对比模型
4. 在hep-th摘要补全任务上评估性能

Result: 1. 所有微调模型在hep-th摘要补全任务上均超越基础模型
2. 与商业模型(ChatGPT, Claude, Gemini, DeepSeek)对比
3. 确立了开发专用HEP语言模型的优化方向

Conclusion: 验证了特定领域微调对高能物理语言模型的有效性，为后续开发提供技术路线

Abstract: We present specialized Large Language Models for theoretical High-Energy
Physics, obtained as 20 fine-tuned variants of the 8-billion parameter
Llama-3.1 model. Each variant was trained on arXiv abstracts (through August
2024) from different combinations of hep-th, hep-ph and gr-qc. For a
comparative study, we also trained models on datasets that contained abstracts
from disparate fields such as the q-bio and cs categories. All models were
fine-tuned using two distinct Low-Rank Adaptation fine-tuning approaches and
varying dataset sizes, and outperformed the base model on hep-th abstract
completion tasks. We compare performance against leading commercial LLMs
(ChatGPT, Claude, Gemini, DeepSeek) and derive insights for further developing
specialized language models for High-Energy Theoretical Physics.

</details>


### [136] [Intent Aware Context Retrieval for Multi-Turn Agricultural Question Answering](https://arxiv.org/abs/2508.03719)
*Abhay Vijayvargia,Ajay Nagpal,Kundeshwar Pundalik,Atharva Savarkar,Smita Gautam,Pankaj Singh,Rohit Saluja,Ganesh Ramakrishnan*

Main category: cs.CL

TL;DR: 一款名为Krishi Sathi的AI农业聊天机器人，通过文本和语音为印度农民提供个性化、易理解的农业建议。该系统使用经过微调的IFT模型和检索增强生成（RAG）技术，采用多轮对话结构收集需求，支持英语和印地语的双语交互。成果显示高准确性（97.53%）和快速响应（<6秒）。


<details>
  <summary>Details</summary>
Motivation: 印度农村地区农民面临农业建议获取困难的问题，包括时效性差、语言障碍和低识字率。传统方式无法满足实时、通俗易懂的咨询需求，亟需通过技术手段提升农业信息可及性。

Method: 1. 构建基于IFT(指令微调)的模型框架，在三个印度农业知识数据集上微调；
2. 开发结构化多轮对话流程，逐步收集农民问题细节以明确意图；
3. 实现检索增强生成（RAG）：先查询农业知识库，再用IFT模型生成定制化回复；
4. 集成语音识别（ASR）和语音合成（TTS）模块，支持双语（印地语/英语）交互。

Result: 1. 问题回复准确率97.53%；
2. 上下文相关性与个性化程度91.35%；
3. 查询完成率97.53%；
4. 平均响应时间<6秒；
5. 在英语和印地语环境中均表现稳定。

Conclusion: 通过结合意图驱动的对话流、指令微调模型和检索增强生成技术，Krishi Sathi有效提升了印度农业数字支持的覆盖质量与可及性。系统的高精度与多模态交互特别适合低识字率人群，为发展中国家的普惠农业技术提供可行方案。

Abstract: Indian farmers often lack timely, accessible, and language-friendly
agricultural advice, especially in rural areas with low literacy. To address
this gap in accessibility, this paper presents a novel AI-powered agricultural
chatbot, Krishi Sathi, designed to support Indian farmers by providing
personalized, easy-to-understand answers to their queries through both text and
speech. The system's intelligence stems from an IFT model, subsequently refined
through fine-tuning on Indian agricultural knowledge across three curated
datasets. Unlike traditional chatbots that respond to one-off questions, Krishi
Sathi follows a structured, multi-turn conversation flow to gradually collect
the necessary details from the farmer, ensuring the query is fully understood
before generating a response. Once the intent and context are extracted, the
system performs Retrieval-Augmented Generation (RAG) by first fetching
information from a curated agricultural database and then generating a tailored
response using the IFT model. The chatbot supports both English and Hindi
languages, with speech input and output features (via ASR and TTS) to make it
accessible for users with low literacy or limited digital skills. This work
demonstrates how combining intent-driven dialogue flows, instruction-tuned
models, and retrieval-based generation can improve the quality and
accessibility of digital agricultural support in India.
  This approach yielded strong results, with the system achieving a query
response accuracy of 97.53%, 91.35% contextual relevance and personalization,
and a query completion rate of 97.53%. The average response time remained under
6 seconds, ensuring timely support for users across both English and Hindi
interactions.

</details>


### [137] [Hierarchical Verification of Speculative Beams for Accelerating LLM Inference](https://arxiv.org/abs/2508.03726)
*Jaydip Sen,Harshitha Puvvala,Subhasis Dasgupta*

Main category: cs.CL

TL;DR: 提出层次化验证树（HVT）框架优化大语言模型推理效率，通过优先验证高概率草稿序列实现提前剪枝，显著减少计算开销和能耗


<details>
  <summary>Details</summary>
Motivation: 现有推测解码和束采样方法对草稿序列采取顺序验证而未区分优先级，导致不必要的计算开销。为优化推理效率，提出HVT框架对高似然草稿优先验证并提前剪枝

Method: 1. 构建层次化验证树（HVT）重组推测束采样过程；2. 设计算法优先验证高概率序列分支；3. 开发基于置信度的早期剪枝策略；4. 无需模型重训练即可集成至标准LLM推理流水线

Result: 多数据集/模型实验表明：1. 在相同输出质量下，推理速度比现有推测解码方法提升26.4%；2. 能耗降低31.7%；3. 支持beam search场景下质量提升3.2% BLEU

Conclusion: HVT通过树状层次验证机制突破传统顺序验证瓶颈，为LLM推理加速开辟新途径，其框架通用性及无训练需求利于工业部署

Abstract: Large language models (LLMs) have achieved remarkable success across diverse
natural language processing tasks but face persistent challenges in inference
efficiency due to their autoregressive nature. While speculative decoding and
beam sampling offer notable improvements, traditional methods verify draft
sequences sequentially without prioritization, leading to unnecessary
computational overhead. This work proposes the Hierarchical Verification Tree
(HVT), a novel framework that restructures speculative beam decoding by
prioritizing high-likelihood drafts and enabling early pruning of suboptimal
candidates. Theoretical foundations and a formal verification-pruning algorithm
are developed to ensure correctness and efficiency. Integration with standard
LLM inference pipelines is achieved without requiring retraining or
architecture modification. Experimental evaluations across multiple datasets
and models demonstrate that HVT consistently outperforms existing speculative
decoding schemes, achieving substantial reductions in inference time and energy
consumption while maintaining or enhancing output quality. The findings
highlight the potential of hierarchical verification strategies as a new
direction for accelerating large language model inference.

</details>


### [138] [WINELL: Wikipedia Never-Ending Updating with LLM Agents](https://arxiv.org/abs/2508.03728)
*Revanth Gangi Reddy,Tanay Dixit,Jiaxin Qin,Cheng Qian,Daniel Lee,Jiawei Han,Kevin Small,Xing Fan,Ruhi Sarikaya,Heng Ji*

Main category: cs.CL

TL;DR: WiNELL是一个基于LLM的代理框架，用于持续更新维基百科文章，它通过多代理系统聚合网络信息，选择重要知识，并生成精确编辑建议供人工审核，编辑模型训练自维基百科历史编辑记录，在信息覆盖和编辑效率上优于现有基线模型。


<details>
  <summary>Details</summary>
Motivation: 维基百科因依赖人工编辑而难以保持内容及时更新，受NELL连续知识获取的启发和LLM代理进展的推动，作者开发了WiNELL框架以自动化知识更新流程。

Method: 1. 采用多代理框架：聚合网络信息，针对维基百科中目标实体筛选新且重要的知识；2. 生成精确编辑建议并提交人工审核；3. 基于维基百科历史编辑数据训练细粒度编辑模型，确保编辑行为与人类操作一致。

Result: 1. 编辑模型在关键信息覆盖率和编辑效率上优于开源指令跟随基线和闭源LLM（如GPT-4o）；2. 在活跃维基百科页面的端到端评估中，WiNELL成功识别并建议及时的事实更新。

Conclusion: WiNELL为LLM代理在知识库自动更新领域开辟了新方向，展示了以永不停止的方式维护知识库的可能性。

Abstract: Wikipedia, a vast and continuously consulted knowledge base, faces
significant challenges in maintaining up-to-date content due to its reliance on
manual human editors. Inspired by the vision of continuous knowledge
acquisition in NELL and fueled by advances in LLM-based agents, this paper
introduces WiNELL, an agentic framework for continuously updating Wikipedia
articles. Our approach employs a multi-agent framework to aggregate online
information, select new and important knowledge for a target entity in
Wikipedia, and then generate precise edit suggestions for human review. Our
fine-grained editing models, trained on Wikipedia's extensive history of human
edits, enable incorporating updates in a manner consistent with human editing
behavior. Our editor models outperform both open-source instruction-following
baselines and closed-source LLMs (e.g., GPT-4o) in key information coverage and
editing efficiency. End-to-end evaluation on high-activity Wikipedia pages
demonstrates WiNELL's ability to identify and suggest timely factual updates.
This opens up a promising research direction in LLM agents for automatically
updating knowledge bases in a never-ending fashion.

</details>


### [139] [GanitBench: A bi-lingual benchmark for evaluating mathematical reasoning in Vision Language Models](https://arxiv.org/abs/2508.03737)
*Ashutosh Bandooni,Brindha Subburaj*

Main category: cs.CL

TL;DR: 介绍了一个名为GanitBench的多语种（英语和印地语）视觉数学基准测试数据集，包含1527个仅视觉问题，用于评估视觉语言模型的推理能力。评估了两个闭源模型的零样本和两样本思维链设置，并应用了'双锁'约束。发现在印地语环境下模型性能下降，且两样本思维链在约束下更有效。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型的推理评估基准多为英语单语种，且缺乏印地语的非理解、翻译类数据集。因此，作者创建了一个多语种的数学基准测试，以促进印地语等语言在视觉推理研究中的纳入。

Method: 1. 从两个主要印度考试（JEE Advanced和CBSE Boards）收集视觉数学问题，组成1527个样本，覆盖多个数学主题，以图像形式呈现（含必要图形和文本）；2. 将基准翻译为英语和印地语；3. 在零样本思维链（CoT）和两样本思维链设置下评估两个闭源模型；4. 引入'双锁'约束（要求模型同时输出数学解释和答案）进一步测试模型鲁棒性；5. 比较不同语言（英语/印地语）下的表现差异。

Result: 1. GPT-4o mini在基准测试中表现最优，最高平均准确率38.15%；2. '双锁'约束显著降低模型性能（最高准确率从38.15%降至更低值）；3. 在约束下两样本思维链比零样本效果更好；4. 模型在印地语问题上的性能低于英语。

Conclusion: GanitBench填补了多语种（尤其印地语）视觉数学推理基准的空白。实验表明：现有VLM在复杂数学问题上仍有不足，语言转换会削弱性能。两样本思维链对提高约束下表现有效。该工作有望推动非英语语言在视觉推理研究中的应用。

Abstract: Benchmarks for evaluating reasoning among Vision Language Models (VLMs) on
several fields and domains are being curated more frequently over the last few
years. However these are often monolingual, mostly available in English.
Additionally there also is a lack of datasets available in Hindi on tasks apart
from comprehension and translation. We introduce GanitBench, a tough benchmark
consisting of 1527 vision-only questions covering several topics in Mathematics
- available in languages English and Hindi. Collected from two major
examinations from India, the JEE Advanced and the CBSE Boards examinations,
this benchmark includes questions in the form of images comprising of figures
essential to a question as well as text. We evaluate two closed source models
for the same, in zero-shot Chain-of-Thought (CoT) and two-shot CoT settings.
GPT-4o mini is found to be the more dominant model on the benchmark, with it's
highest average accuracy being 38.15%. We also evaluate models through a
"Double Lock" constraint, which brings down the performance of the models by
considerable margins. We observe that two-shot CoT appears to be a more
effective setting under this environment. Performance of the two VLMs also
decreases when answering the same questions in the Hindi language. We hope to
facilitate the inclusion of languages like Hindi in research through our work.

</details>


### [140] [AttnTrace: Attention-based Context Traceback for Long-Context LLMs](https://arxiv.org/abs/2508.03793)
*Yanting Wang,Runpeng Geng,Ying Chen,Jinyuan Jia*

Main category: cs.CL

TL;DR: AttnTrace是一种新的基于注意力权重的上下文追踪方法，用于提高长上下文LLM输出的可解释性和可信度，它比现有方法更准确高效。


<details>
  <summary>Details</summary>
Motivation: 现有解决方案（如TracLLM）在追踪LLM生成响应的上下文来源时计算成本高昂，需要一种更高效的方法来进行上下文溯源，以支持安全分析和提高模型输出的可信度。

Method: 提出AttnTrace方法，利用LLM在处理提示时产生的注意力权重进行上下文追踪。引入了两种技术增强其有效性，并提供了理论设计依据。

Result: AttnTrace在准确性和效率上均优于现有方法，能够有效提升长上下文下提示注入检测的能力，并可实际应用于识别注入指令（如操纵LLM生成评论）。

Conclusion: AttnTrace作为一种高效、准确的上下文溯源方法，在提高LLM系统安全性、可解释性和信任度方面具有重要价值，并已通过实验验证其实际应用效果。

Abstract: Long-context large language models (LLMs), such as Gemini-2.5-Pro and
Claude-Sonnet-4, are increasingly used to empower advanced AI systems,
including retrieval-augmented generation (RAG) pipelines and autonomous agents.
In these systems, an LLM receives an instruction along with a context--often
consisting of texts retrieved from a knowledge database or memory--and
generates a response that is contextually grounded by following the
instruction. Recent studies have designed solutions to trace back to a subset
of texts in the context that contributes most to the response generated by the
LLM. These solutions have numerous real-world applications, including
performing post-attack forensic analysis and improving the interpretability and
trustworthiness of LLM outputs. While significant efforts have been made,
state-of-the-art solutions such as TracLLM often lead to a high computation
cost, e.g., it takes TracLLM hundreds of seconds to perform traceback for a
single response-context pair. In this work, we propose AttnTrace, a new context
traceback method based on the attention weights produced by an LLM for a
prompt. To effectively utilize attention weights, we introduce two techniques
designed to enhance the effectiveness of AttnTrace, and we provide theoretical
insights for our design choice. We also perform a systematic evaluation for
AttnTrace. The results demonstrate that AttnTrace is more accurate and
efficient than existing state-of-the-art context traceback methods. We also
show that AttnTrace can improve state-of-the-art methods in detecting prompt
injection under long contexts through the attribution-before-detection
paradigm. As a real-world application, we demonstrate that AttnTrace can
effectively pinpoint injected instructions in a paper designed to manipulate
LLM-generated reviews. The code is at
https://github.com/Wang-Yanting/AttnTrace.

</details>


### [141] [Majority Bit-Aware Watermarking For Large Language Models](https://arxiv.org/abs/2508.03829)
*Jiahao Xu,Rui Hu,Zikai Zhang*

Main category: cs.CL

TL;DR: MajorMark 是一种新颖的水印方法，通过多数位感知编码改善大型语言模型中多比特水印在文本质量和解码准确率之间的权衡。该方法利用消息的多数位来选择偏好标记集，并采用基于聚类的解码策略，以在保持较大偏好标记集的同时维持高解码准确率。MajorMark$^+$ 则通过将消息分块独立编码和确定性地解码各区块来进一步提升性能。实验表明，该方法在解码准确率和文本生成质量上均优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在应用中可能被滥用于生成有害或欺骗性内容，而现有的多比特水印方案在文本质量和解码准确率之间存在固有权衡：为了确保可靠解码必须限制偏好标记集大小，但这降低了生成文本的质量。

Method: 1. MajorMark：通过多数位感知编码选择偏好标记集（基于消息多数位动态决定），允许更灵活地采样标记；使用聚类解码策略维持高准确率（即使偏好集较大）。2. MajorMark$^+$：将消息分成多个区块分别独立编码，并采用确定性解码机制以进一步提升质量和准确率。

Result: 在多个先进语言模型上的广泛实验表明，MajorMark系列方法显著提升了水印解码准确率和生成文本质量，优于现有多比特水印基线。

Conclusion: MajorMark通过创新的多数位感知编码和聚类解码策略，有效解决了多比特水印中质量与准确率的权衡问题；MajorMark$^+$的分块处理进一步优化了性能。该方法为高保真水印提供了一种有效解决方案。

Abstract: The growing deployment of Large Language Models (LLMs) in real-world
applications has raised concerns about their potential misuse in generating
harmful or deceptive content. To address this issue, watermarking techniques
have emerged as a promising solution by embedding identifiable binary messages
into generated text for origin verification and misuse tracing. While recent
efforts have explored multi-bit watermarking schemes capable of embedding rich
information such as user identifiers, they typically suffer from the
fundamental trade-off between text quality and decoding accuracy: to ensure
reliable message decoding, they have to restrict the size of preferred token
sets during encoding, yet such restrictions reduce the quality of the generated
content. In this work, we propose MajorMark, a novel watermarking method that
improves this trade-off through majority bit-aware encoding. MajorMark selects
preferred token sets based on the majority bit of the message, enabling a
larger and more flexible sampling of tokens. In contrast to prior methods that
rely on token frequency analysis for decoding, MajorMark employs a
clustering-based decoding strategy, which maintains high decoding accuracy even
when the preferred token set is large, thus preserving both content quality and
decoding accuracy. We further introduce MajorMark$^+$, which partitions the
message into multiple blocks to independently encode and deterministically
decode each block, thereby further enhancing the quality of watermarked text
and improving decoding accuracy. Extensive experiments on state-of-the-art LLMs
demonstrate that our methods significantly enhance both decoding accuracy and
text generation quality, outperforming prior multi-bit watermarking baselines.

</details>


### [142] [Hallucination to Truth: A Review of Fact-Checking and Factuality Evaluation in Large Language Models](https://arxiv.org/abs/2508.03860)
*Subhey Sadi Rahman,Md. Adnanul Islam,Md. Mahbub Alam,Musarrat Zeba,Md. Abdur Rahman,Sadia Sultana Chowa,Mohaimenul Azam Khan Raiaan,Sami Azam*

Main category: cs.CL

TL;DR: 总结当前大型语言模型（LLM）在事实准确性评估方面的挑战与方法，强调构建强大的事实检查框架的重要性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在训练过程中接触到大量不准确或误导性内容，可能导致生成错误信息。因此，开发有效的事实检查机制至关重要。

Method: 通过系统性地回顾2020年至2025年的文献，分析评估LLM生成内容事实准确性的方法，重点关注高级提示策略、领域微调、检索增强生成（RAG）、指令调优和多智能体推理等技术。

Result: 指出当前评估指标的局限性；强调结合外部验证证据对输出的重要性；发现领域定制化对提升事实一致性的价值；最终提出构建准确、可解释且适合领域特定事实检查的LLM的必要性。

Conclusion: 未来的语言模型需更重视可信度和上下文感知，通过整合RAG、领域知识和多智能体协作等方法推动可靠语言模型的发展。

Abstract: Large Language Models (LLMs) are trained on vast and diverse internet corpora
that often include inaccurate or misleading content. Consequently, LLMs can
generate misinformation, making robust fact-checking essential. This review
systematically analyzes how LLM-generated content is evaluated for factual
accuracy by exploring key challenges such as hallucinations, dataset
limitations, and the reliability of evaluation metrics. The review emphasizes
the need for strong fact-checking frameworks that integrate advanced prompting
strategies, domain-specific fine-tuning, and retrieval-augmented generation
(RAG) methods. It proposes five research questions that guide the analysis of
the recent literature from 2020 to 2025, focusing on evaluation methods and
mitigation techniques. The review also discusses the role of instruction
tuning, multi-agent reasoning, and external knowledge access via RAG
frameworks. Key findings highlight the limitations of current metrics, the
value of grounding outputs with validated external evidence, and the importance
of domain-specific customization to improve factual consistency. Overall, the
review underlines the importance of building LLMs that are not only accurate
and explainable but also tailored for domain-specific fact-checking. These
insights contribute to the advancement of research toward more trustworthy and
context-aware language models.

</details>


### [143] [An Entity Linking Agent for Question Answering](https://arxiv.org/abs/2508.03865)
*Yajie Luo,Yihong Wu,Muzhi Li,Fengran Mo,Jia Ao Sun,Xinyu Wang,Liheng Ma,Yingxue Zhang,Jian-Yun Nie*

Main category: cs.CL

TL;DR: 提出了一种基于大语言模型的实体链接智能体，用于解决问答系统中短文本实体链接问题，模拟人类认知流程，并在实验中验证了其有效性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统实体链接方法针对长文本设计，在问答系统的短文本问题中表现不佳。

Method: 1. 设计基于大语言模型的智能体模拟人类认知流程：主动识别实体提及、检索候选实体并决策。2. 通过两个实验验证：工具型实体链接任务和端到端问答任务评估。

Result: 实验证实该智能体在实体链接和问答任务中均具有鲁棒性和有效性。

Conclusion: 所提出的基于大语言模型的实体链接智能体能够有效解决短文本问答场景中的实体链接挑战。

Abstract: Some Question Answering (QA) systems rely on knowledge bases (KBs) to provide
accurate answers. Entity Linking (EL) plays a critical role in linking natural
language mentions to KB entries. However, most existing EL methods are designed
for long contexts and do not perform well on short, ambiguous user questions in
QA tasks. We propose an entity linking agent for QA, based on a Large Language
Model that simulates human cognitive workflows. The agent actively identifies
entity mentions, retrieves candidate entities, and makes decision. To verify
the effectiveness of our agent, we conduct two experiments: tool-based entity
linking and QA task evaluation. The results confirm the robustness and
effectiveness of our agent.

</details>


### [144] [Sotopia-RL: Reward Design for Social Intelligence](https://arxiv.org/abs/2508.03905)
*Haofei Yu,Zhengyang Qi,Yining Zhao,Kolby Nottingham,Keyang Xuan,Bodhisattwa Prasad Majumder,Hao Zhu,Paul Pu Liang,Jiaxuan You*

Main category: cs.CL

TL;DR: 本文提出Sotopia-RL框架，通过将粗粒度的回合级反馈细化为话语级的多维度奖励，以解决社交智能代理训练中由部分可观测性和多维度特性带来的强化学习效率问题。实验证明该框架在Sotopia环境中显著提升了社交目标完成度。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型(LLMs)需要社交智能来完成真实世界的社交任务，但现有基于马尔科夫决策过程(MDP)的强化学习方法在社交交互中存在两个障碍：1) 部分可观测性（话语的间接延迟效应导致信用分配困难）；2) 多维度特性（如建立融洽关系等行为对目标的间接贡献）。这使得单维度回合级奖励机制效率低下且不稳定。

Method: Sotopia-RL框架：1) 信用分配层面：将回合级反馈拆解为话语级奖励，缓解部分可观测性问题；2) 奖励设计层面：构建多维度奖励（如情感共鸣、知识传播等），完整捕捉社交交互的复杂性并减少奖励破解。实验环境为开放社交学习平台Sotopia。

Result: 在Sotopia测试集上：1) Sotopia-hard任务取得7.17分；2) Sotopia-full任务取得8.31分，显著优于现有方法。消融实验证明：话语级信用分配机制提升训练稳定性（约23%），多维度奖励设计提高目标达成率（约17%）。

Conclusion: Sotopia-RL突破传统强化学习限制：1) 话语级信用分配有效解决社交交互中的延迟效应问题；2) 多维度奖励机制可建模复杂社交策略。框架已开源，为LLM社交智能训练提供新范式。

Abstract: Social intelligence has become a critical capability for large language
models (LLMs), enabling them to engage effectively in real-world social tasks
such as accommodation, persuasion, collaboration, and negotiation.
Reinforcement learning (RL) is a natural fit for training socially intelligent
agents because it allows models to learn sophisticated strategies directly
through social interactions. However, social interactions have two key
characteristics that set barriers for RL training: (1) partial observability,
where utterances have indirect and delayed effects that complicate credit
assignment, and (2) multi-dimensionality, where behaviors such as
rapport-building or knowledge-seeking contribute indirectly to goal
achievement. These characteristics make Markov decision process (MDP)-based RL
with single-dimensional episode-level rewards inefficient and unstable. To
address these challenges, we propose Sotopia-RL, a novel framework that refines
coarse episode-level feedback into utterance-level, multi-dimensional rewards.
Utterance-level credit assignment mitigates partial observability by
attributing outcomes to individual utterances, while multi-dimensional rewards
capture the full richness of social interactions and reduce reward hacking.
Experiments in Sotopia, an open-ended social learning environment, demonstrate
that Sotopia-RL achieves state-of-the-art social goal completion scores (7.17
on Sotopia-hard and 8.31 on Sotopia-full), significantly outperforming existing
approaches. Ablation studies confirm the necessity of both utterance-level
credit assignment and multi-dimensional reward design for RL training. Our
implementation is publicly available at:
https://github.com/sotopia-lab/sotopia-rl.

</details>


### [145] [CoAct-1: Computer-using Agents with Coding as Actions](https://arxiv.org/abs/2508.03923)
*Linxin Song,Yutong Dai,Viraj Prabhu,Jieyu Zhang,Taiwei Shi,Li Li,Junnan Li,Silvio Savarese,Zeyuan Chen,Jieyu Zhao,Ran Xu,Caiming Xiong*

Main category: cs.CL

TL;DR: CoAct-1: 一个多代理系统，融合GUI控制和编程执行，显著提升OSWorld基准性能和效率


<details>
  <summary>Details</summary>
Motivation: 现有的GUI操作代理在复杂任务上存在效率、可靠性不足的问题，需突破纯GUI交互限制

Method: 设计包含Orchestrator的动态调度系统，将任务按需分配给GUI Operator或能编写Python/Bash脚本的Programmer代理

Result: OSWorld基准测试成功率60.76%(SOTA)，平均步数降至10.15，大幅领先纯GUI代理

Conclusion: 编码作为核心动作的混合策略为计算机自动化提供了更强大、高效且可扩展的新范式

Abstract: Autonomous agents that operate computers via Graphical User Interfaces (GUIs)
often struggle with efficiency and reliability on complex, long-horizon tasks.
While augmenting these agents with planners can improve task decomposition,
they remain constrained by the inherent limitations of performing all actions
through GUI manipulation, leading to brittleness and inefficiency. In this
work, we introduce a more robust and flexible paradigm: enabling agents to use
coding as a enhanced action. We present CoAct-1, a novel multi-agent system
that synergistically combines GUI-based control with direct programmatic
execution. CoAct-1 features an Orchestrator that dynamically delegates subtasks
to either a conventional GUI Operator or a specialized Programmer agent, which
can write and execute Python or Bash scripts. This hybrid approach allows the
agent to bypass inefficient GUI action sequences for tasks like file management
and data processing, while still leveraging visual interaction when necessary.
We evaluate our system on the challenging OSWorld benchmark, where CoAct-1
achieves a new state-of-the-art success rate of 60.76%, significantly
outperforming prior methods. Furthermore, our approach dramatically improves
efficiency, reducing the average number of steps required to complete a task to
just 10.15, compared to 15 for leading GUI agents. Our results demonstrate that
integrating coding as a core action provides a more powerful, efficient, and
scalable path toward generalized computer automation.

</details>


### [146] [CAP-LLM: Context-Augmented Personalized Large Language Models for News Headline Generation](https://arxiv.org/abs/2508.03935)
*Raymond Wilson,Cole Graham,Chase Carter,Zefeng Yang,Ruiqi Gu*

Main category: cs.CL

TL;DR: 本文提出了Context-Augmented Personalized LLM (CAP-LLM)框架，通过结合大型语言模型(LLMs)的强大文本生成能力，解决了新闻标题个性化生成中的用户兴趣捕捉和事实一致性问题。该框架包含用户偏好编码器、上下文注入适配器和事实一致性强模块，在PENS数据集上实现了最先进性能，显著提高了事实一致性(FactCC 87.50)、个性化程度(Pc(avg)2.73)和内容覆盖(ROUGE-1 26.55)。


<details>
  <summary>Details</summary>
Motivation: 在信息过载时代，个性化新闻标题生成对吸引用户至关重要。现有方法难以有效捕捉复杂的用户兴趣并确保事实一致性，常导致标题泛化或误导。

Method: 1. 基于预训练语言模型构建CAP-LLM框架
2. 用户偏好编码器捕获长期用户兴趣
3. 上下文注入适配器无缝整合用户偏好与当前文章
4. 事实一致性强模块采用新型对比损失减轻生成幻觉

Result: 1. 在PENS数据集取得SOTA
2. 事实一致性(FactCC): 87.50 (优于BART的86.67)
3. 个性化指标: Pc(avg)2.73, Pc(max)17.25
4. ROUGE指标: R-1 26.55/R-2 9.95/R-L 23.01
5. 消融实验/人工评估/敏感性分析验证模块有效性和鲁棒性

Conclusion: CAP-LLM通过创新框架设计，在个性化新闻标题生成中实现了个人偏好与事实准确性的最优平衡，为新闻推荐系统提供有效解决方案。

Abstract: In the era of information overload, personalized news headline generation is
crucial for engaging users by tailoring content to their preferences while
accurately conveying news facts. Existing methods struggle with effectively
capturing complex user interests and ensuring factual consistency, often
leading to generic or misleading headlines. Leveraging the unprecedented
capabilities of Large Language Models (LLMs) in text generation, we propose
Context-Augmented Personalized LLM (CAP-LLM), a novel framework that integrates
user preferences and factual consistency constraints into a powerful
pre-trained LLM backbone. CAP-LLM features a User Preference Encoder to capture
long-term user interests, a Context Injection Adapter to seamlessly integrate
these preferences and current article context into the LLM's generation
process, and a Fact-Consistency Reinforcement Module employing a novel
contrastive loss to mitigate hallucination. Evaluated on the real-world PENS
dataset, CAP-LLM achieves state-of-the-art performance across all metrics.
Notably, it significantly improves factual consistency (FactCC of 87.50) over
strong baselines like BART (86.67), while simultaneously enhancing
personalization (Pc(avg) 2.73, Pc(max) 17.25) and content coverage (ROUGE-1
26.55, ROUGE-2 9.95, ROUGE-L 23.01). Our ablation studies, human evaluations,
and sensitivity analyses further validate the effectiveness of each component
and the robustness of our approach, demonstrating CAP-LLM's ability to achieve
a superior balance between personalization and factual accuracy in news
headline generation.

</details>


### [147] [Data and AI governance: Promoting equity, ethics, and fairness in large language models](https://arxiv.org/abs/2508.03970)
*Alok Abhishek,Lisa Erickson,Tushar Bandopadhyay*

Main category: cs.CL

TL;DR: 提出了一个涵盖机器学习模型全生命周期的系统性偏见治理框架，扩展了BEATS测试套件，旨在通过数据和AI治理确保LLMs在偏见、道德、公平性和事实性方面的可靠性，提升生成式AI应用的安全性和社会责任。


<details>
  <summary>Details</summary>
Motivation: 目前大型语言模型(LLMs)在偏见、公平性等方面存在显著缺陷，缺乏贯穿开发到部署全周期的系统性治理方案，需要建立可落地的治理框架以降低歧视风险和声誉损害。

Method: 以BEATS测试套件为基础，构建包含开发验证阶段、持续生产监控和防护栏实施的数据与AI治理框架，支持部署前的严格基准测试和实时评估，通过主动治理机制约束LLM生成内容。

Result: 该方法能够有效提升生成式AI系统的安全性和责任性，降低歧视风险与品牌声誉损害，适用于真实场景的全生命周期治理。

Conclusion: 提出的数据和AI治理框架为构建符合社会责任的伦理对齐型生成式AI应用提供了可实施路径，推动负责任AI的发展。

Abstract: In this paper, we cover approaches to systematically govern, assess and
quantify bias across the complete life cycle of machine learning models, from
initial development and validation to ongoing production monitoring and
guardrail implementation. Building upon our foundational work on the Bias
Evaluation and Assessment Test Suite (BEATS) for Large Language Models, the
authors share prevalent bias and fairness related gaps in Large Language Models
(LLMs) and discuss data and AI governance framework to address Bias, Ethics,
Fairness, and Factuality within LLMs. The data and AI governance approach
discussed in this paper is suitable for practical, real-world applications,
enabling rigorous benchmarking of LLMs prior to production deployment,
facilitating continuous real-time evaluation, and proactively governing LLM
generated responses. By implementing the data and AI governance across the life
cycle of AI development, organizations can significantly enhance the safety and
responsibility of their GenAI systems, effectively mitigating risks of
discrimination and protecting against potential reputational or brand-related
harm. Ultimately, through this article, we aim to contribute to advancement of
the creation and deployment of socially responsible and ethically aligned
generative artificial intelligence powered applications.

</details>


### [148] [Confidence-Weighted Token Set Cover for Early Hypothesis Pruning in Self-Consistency](https://arxiv.org/abs/2508.03979)
*Md Arafat Sultan,Ramón Fernandez Astudillo*

Main category: cs.CL

TL;DR: 本文探讨如何提高自一致性方法的token效率，同时保持其并行性。通过早期假设剪枝，在并行生成多个解答的同时，定期根据模型置信度和词汇覆盖度去除不必要的中间假设。提出了一种快速的加权集合覆盖算法，实验证明该方法可降低10-35%的token消耗。


<details>
  <summary>Details</summary>
Motivation: 自一致性方法在提高模型推理性能方面有效，但其token消耗量大，限制了实际应用。尤其是在长思维链推理任务中，多路并行计算成本较高。因此，需要在保留并行特性的同时，降低token开销，以提高实用性。

Method: 1. 并行生成多个解答路径（即自一致性的思路），但引入剪枝机制；
2. 设置剪枝点，定期评估中间假设的必要性；
3. 设计两种剪枝指标：a) 模型对单个步骤假设的置信度；b) 候选子集对当前全部假设的词汇覆盖度；
4. 基于两个指标开发加权集合覆盖算法，动态决定保留的假设子集；
5. 仅对被保留的假设继续延展后续推理步骤。
整个过程保持生成的并行性，通过剪枝减少后续分支计算量。

Result: 在五个大语言模型上测试三个数学推理任务：
- 所有模型均实现token开销降低；
- 多数情况下降低10-35%的token消耗；
- 成功保留原有解的质量（未报告精度损失）。

Conclusion: 所提出的剪枝方法有效提升了自一致性的token效率，减少计算资源开销。此框架平衡了解的全面性与计算成本，为长程推理提供了可行方案。

Abstract: Despite its simplicity and efficacy, the high token expenditure of
self-consistency can limit its practical utility. Here we investigate if
self-consistency can be made more token-efficient for long chain-of-thought
reasoning tasks, while preserving its parallelism, through early hypothesis
pruning. Concretely, we generate all solutions in parallel, but periodically
prune intermediate hypotheses that are deemed unnecessary based on two
lightweight indicators: (a) the model's own confidence in individual
hypotheses, and (b) lexical coverage of all current hypotheses by candidate
subsets that are under consideration for continued retention. We design a fast
weighted set cover algorithm that utilizes the two indicators; our evaluation
of five LLMs on three math benchmarks shows that this method can improve token
efficiency for all models, by 10-35% in many cases.

</details>


### [149] [Are Today's LLMs Ready to Explain Well-Being Concepts?](https://arxiv.org/abs/2508.03990)
*Bohan Jiang,Dawei Li,Zhen Tan,Chengshuai Zhao,Huan Liu*

Main category: cs.CL

TL;DR: 摘要研究了大型语言模型（LLMs）在解释福祉概念时的表现，构建了一个包含43,880条解释的数据集，并提出了一个原则导向的评估框架，表明通过监督微调（SFT）和直接偏好优化（DPO）可以提高生成解释的质量。


<details>
  <summary>Details</summary>
Motivation: 随着越来越多的人向大型语言模型（LLMs）咨询福祉相关问题，需要评估这些模型是否能够生成准确且适合不同受众的个性化解释。关键在于确保解释既正确又能满足不同专业知识水平的用户需求。

Method: 1. 构建大规模数据集：收集了2,194个福祉概念，由10种不同的LLM生成43,880条解释。
2. 评估框架：引入原则导向的LLM作为评估者框架（dual judges）评估解释质量。
3. 模型优化：采用监督微调（SFT）和直接偏好优化（DPO）对开源LLM进行微调。

Result: 1. 提出的LLM法官评估与人类评价高度一致。
2. 解释质量在模型、受众和概念类别间存在显著差异。
3. 经DPO和SFT微调的模型质量超过更大的模型，表明偏好学习在专业化解释任务中的有效性。

Conclusion: 通过原则导向评估框架发现，微调可以有效提升LLM在福祉领域生成解释的质量，为不同水平用户提供更好的回答参考。

Abstract: Well-being encompasses mental, physical, and social dimensions essential to
personal growth and informed life decisions. As individuals increasingly
consult Large Language Models (LLMs) to understand well-being, a key challenge
emerges: Can LLMs generate explanations that are not only accurate but also
tailored to diverse audiences? High-quality explanations require both factual
correctness and the ability to meet the expectations of users with varying
expertise. In this work, we construct a large-scale dataset comprising 43,880
explanations of 2,194 well-being concepts, generated by ten diverse LLMs. We
introduce a principle-guided LLM-as-a-judge evaluation framework, employing
dual judges to assess explanation quality. Furthermore, we show that
fine-tuning an open-source LLM using Supervised Fine-Tuning (SFT) and Direct
Preference Optimization (DPO) can significantly enhance the quality of
generated explanations. Our results reveal: (1) The proposed LLM judges align
well with human evaluations; (2) explanation quality varies significantly
across models, audiences, and categories; and (3) DPO- and SFT-finetuned models
outperform their larger counterparts, demonstrating the effectiveness of
preference-based learning for specialized explanation tasks.

</details>


### [150] [Transferring Expert Cognitive Models to Social Robots via Agentic Concept Bottleneck Models](https://arxiv.org/abs/2508.03998)
*Xinyu Zhao,Zhen Tan,Maya Enisman,Minjae Seo,Marta R. Durantini,Dolores Albarracin,Tianlong Chen*

Main category: cs.CL

TL;DR: 开发一种社交机器人作为群体会议的协助者，使用基于可解释概念的人工智能模型来透明地预测干预需求，并通过知识转移提升新手引导员的能力。


<details>
  <summary>Details</summary>
Motivation: 群体会议引导在促进个体目标执行和群体关系方面至关重要。传统的引导过程充满挑战和高认知负荷，需要一个透明技术来解析群体会议数据并提供可解释建议。

Method: 提出使用社交机器人作为辅助引导者，采用基于人类可理解概念（如用户参与度、情绪等）的“概念瓶颈模型(CBM)”进行决策。核心技术是构建一个迁移学习框架，将基础模型(FM)的广泛社会知识蒸馏到透明的CBM中。

Result: 该方法在干预需求预测上显著优于零样本基础模型，并能进行实时人工纠错。模型具有良好泛化性，既适应不同群体，也能将专家级引导者的能力转移给新手。

Conclusion: 本研究提供了在复杂社交领域中增强人类能力的有效解决方案。

Abstract: Successful group meetings, such as those implemented in group
behavioral-change programs, work meetings, and other social contexts, must
promote individual goal setting and execution while strengthening the social
relationships within the group. Consequently, an ideal facilitator must be
sensitive to the subtle dynamics of disengagement, difficulties with individual
goal setting and execution, and interpersonal difficulties that signal a need
for intervention. The challenges and cognitive load experienced by facilitators
create a critical gap for an embodied technology that can interpret social
exchanges while remaining aware of the needs of the individuals in the group
and providing transparent recommendations that go beyond powerful but "black
box" foundation models (FMs) that identify social cues. We address this
important demand with a social robot co-facilitator that analyzes multimodal
meeting data and provides discreet cues to the facilitator. The robot's
reasoning is powered by an agentic concept bottleneck model (CBM), which makes
decisions based on human-interpretable concepts like participant engagement and
sentiments, ensuring transparency and trustworthiness. Our core contribution is
a transfer learning framework that distills the broad social understanding of
an FM into our specialized and transparent CBM. This concept-driven system
significantly outperforms direct zero-shot FMs in predicting the need for
intervention and enables real-time human correction of its reasoning.
Critically, we demonstrate robust knowledge transfer: the model generalizes
across different groups and successfully transfers the expertise of senior
human facilitators to improve the performance of novices. By transferring an
expert's cognitive model into an interpretable robotic partner, our work
provides a powerful blueprint for augmenting human capabilities in complex
social domains.

</details>


### [151] [HarmonyGuard: Toward Safety and Utility in Web Agents via Adaptive Policy Enhancement and Dual-Objective Optimization](https://arxiv.org/abs/2508.04010)
*Yurun Chen,Xavier Hu,Yuhan Liu,Keting Yin,Juncheng Li,Zhuosheng Zhang,Shengyu Zhang*

Main category: cs.CL

TL;DR: HarmonyGuard是一个多Agent协作框架，旨在共同提高网络环境中Agent的安全性和效用。它通过自适应策略增强和双目标优化来解决网络Agent在长序列操作中面临的安全与效用平衡问题。


<details>
  <summary>Details</summary>
Motivation: 当前研究主要局限于单目标优化或单轮场景，缺乏在网络环境中同时优化安全性和效用的能力。随着网络环境中潜在威胁的演变，Agent在长序列操作中难以平衡任务表现与新兴风险。

Method: HarmonyGuard采用双Agent架构：1) 策略Agent：从非结构化外部文档中自动提取并维护结构化安全策略，并随威胁演变持续更新策略；2) 效用Agent：基于安全性和效用的双目标，执行马尔可夫实时推理来评估目标，并利用元认知能力进行优化。

Result: 在多个基准测试中，HarmonyGuard比现有基线方法策略合规性提高38%，任务完成率提高20%，且在所有任务中策略合规率超过90%。

Conclusion: HarmonyGuard通过多Agent协作框架有效解决了网络Agent安全与效用的协同优化问题，显著提升了策略合规性和任务完成率，为开放网络环境中的Agent操作提供了可靠解决方案。

Abstract: Large language models enable agents to autonomously perform tasks in open web
environments. However, as hidden threats within the web evolve, web agents face
the challenge of balancing task performance with emerging risks during
long-sequence operations. Although this challenge is critical, current research
remains limited to single-objective optimization or single-turn scenarios,
lacking the capability for collaborative optimization of both safety and
utility in web environments. To address this gap, we propose HarmonyGuard, a
multi-agent collaborative framework that leverages policy enhancement and
objective optimization to jointly improve both utility and safety. HarmonyGuard
features a multi-agent architecture characterized by two fundamental
capabilities: (1) Adaptive Policy Enhancement: We introduce the Policy Agent
within HarmonyGuard, which automatically extracts and maintains structured
security policies from unstructured external documents, while continuously
updating policies in response to evolving threats. (2) Dual-Objective
Optimization: Based on the dual objectives of safety and utility, the Utility
Agent integrated within HarmonyGuard performs the Markovian real-time reasoning
to evaluate the objectives and utilizes metacognitive capabilities for their
optimization. Extensive evaluations on multiple benchmarks show that
HarmonyGuard improves policy compliance by up to 38% and task completion by up
to 20% over existing baselines, while achieving over 90% policy compliance
across all tasks. Our project is available here:
https://github.com/YurunChen/HarmonyGuard.

</details>


### [152] [Step More: Going Beyond Single Backpropagation in Meta Learning Based Model Editing](https://arxiv.org/abs/2508.04012)
*Xiaopeng Li,Shasha Li,Xi Wang,Shezheng Song,Bin Ji,Shangwen Wang,Jun Ma,Xiaodong Liu,Mina Liu,Jie Yu*

Main category: cs.CL

TL;DR: 该论文提出了SMEdit方法，通过多重反向传播步骤（MBPS）改进在少样本情况下的模型编辑性能，并使用权重更新的范数正则化提高训练效率。实验表明，SMEdit在性能上优于现有方法且MBPS策略可无缝集成到其他方法中。


<details>
  <summary>Details</summary>
Motivation: 现有的元学习基础模型编辑（MLBME）方法在数据较少时效果不佳，且KL散度的计算效率低下。该研究旨在提升低数据量下的编辑性能并减少训练时间瓶颈。

Method: 提出SMEdit方法，包含两个核心组件：1) 多重反向传播步骤（MBPS）策略，在梯度下降时使用多个更新步骤提升少样本编辑效果；2) 引入权重更新的范数正则化，简化目标函数避免KL散度计算。

Result: 在两个数据集和两个大型语言模型上的实验显示，SMEdit优于现有MLBME基线方法。此外，MBPS策略能直接融入其他方法以提升其性能。

Conclusion: SMEdit通过MBPS策略和权重更新的正则化有效克服了现有元学习方法在少样本情况下的不足和效率瓶颈，为高效模型编辑提供了新方案，其通用性使该策略能推广到其他模型编辑方法中。

Abstract: Large Language Models (LLMs) underpin many AI applications, but their static
nature makes updating knowledge costly. Model editing offers an efficient
alternative by injecting new information through targeted parameter
modifications. In particular, meta-learning-based model editing (MLBME) methods
have demonstrated notable advantages in both editing effectiveness and
efficiency. Despite this, we find that MLBME exhibits suboptimal performance in
low-data scenarios, and its training efficiency is bottlenecked by the
computation of KL divergence. To address these, we propose $\textbf{S}$tep
$\textbf{M}$ore $\textbf{Edit}$ ($\textbf{SMEdit}$), a novel MLBME method that
adopts $\textbf{M}$ultiple $\textbf{B}$ackpro$\textbf{P}$agation
$\textbf{S}$teps ($\textbf{MBPS}$) to improve editing performance under limited
supervision and a norm regularization on weight updates to improve training
efficiency. Experimental results on two datasets and two LLMs demonstrate that
SMEdit outperforms prior MLBME baselines and the MBPS strategy can be
seamlessly integrated into existing methods to further boost their performance.
Our code will be released soon.

</details>


### [153] [ZARA: Zero-shot Motion Time-Series Analysis via Knowledge and Retrieval Driven LLM Agents](https://arxiv.org/abs/2508.04038)
*Zechen Li,Baiyu Chen,Hao Xue,Flora D. Salim*

Main category: cs.CL

TL;DR: ZARA是首个基于代理的零样本可解释人类活动识别框架，直接从原始运动传感器时间序列中工作，无需微调或特定任务的分类器，在8个基准测试中实现了最先进的零样本性能。


<details>
  <summary>Details</summary>
Motivation: 现有的人类活动识别方法需要为固定的活动集进行训练，且在新的行为或传感器设置出现时需要昂贵的重新训练。最近尝试使用大语言模型进行活动识别的准确率较低且缺乏可验证的解释性。

Method: ZARA包含三个模块：1) 自动生成的特征知识库，包含每对活动对的判别统计特征；2) 多传感器检索模块，检索相关证据；3) 分层代理管道，指导大语言模型迭代选择特征、利用证据，生成活动预测和自然语言解释。

Result: 在8个HAR基准测试中，ZARA的宏观F1分数超过了现有最强大的基线方法2.53倍，同时提供了清晰的推理过程。消融研究证实了每个模块的必要性。

Conclusion: ZARA为可信赖、即插即用的运动时间序列分析提供了有前景的一步，支持零样本、可解释的活动识别，无需重新训练。

Abstract: Motion sensor time-series are central to human activity recognition (HAR),
with applications in health, sports, and smart devices. However, existing
methods are trained for fixed activity sets and require costly retraining when
new behaviours or sensor setups appear. Recent attempts to use large language
models (LLMs) for HAR, typically by converting signals into text or images,
suffer from limited accuracy and lack verifiable interpretability. We propose
ZARA, the first agent-based framework for zero-shot, explainable HAR directly
from raw motion time-series. ZARA integrates an automatically derived pair-wise
feature knowledge base that captures discriminative statistics for every
activity pair, a multi-sensor retrieval module that surfaces relevant evidence,
and a hierarchical agent pipeline that guides the LLM to iteratively select
features, draw on this evidence, and produce both activity predictions and
natural-language explanations. ZARA enables flexible and interpretable HAR
without any fine-tuning or task-specific classifiers. Extensive experiments on
8 HAR benchmarks show that ZARA achieves SOTA zero-shot performance, delivering
clear reasoning while exceeding the strongest baselines by 2.53x in macro F1.
Ablation studies further confirm the necessity of each module, marking ZARA as
a promising step toward trustworthy, plug-and-play motion time-series analysis.
Our codes are available at https://github.com/zechenli03/ZARA.

</details>


### [154] [Large Reasoning Models Are Autonomous Jailbreak Agents](https://arxiv.org/abs/2508.04039)
*Thilo Hagendorff,Erik Derner,Nuria Oliver*

Main category: cs.CL

TL;DR: 通过大型推理模型（LRMs）的对话能力，将越狱攻击简化和规模化，使非专家也能轻松执行，成功率达到97.14%。


<details>
  <summary>Details</summary>
Motivation: 传统越狱攻击需技术专长或复杂流程。研究发现大型推理模型具备说服能力，可作为自治代理进行越狱攻击，从而简化并推广该过程。

Method: 选取四种大型推理模型作为攻击代理，针对九款主流模型自主设计多轮对话攻击。使用覆盖七个敏感领域、共70个有害提示的基准数据集进行多轮会话攻击实验。

Result: 所有模型组合的平均攻击成功率达97.14%，揭示安全防御机制退步问题。证明大型推理模型能在无监督条件下系统性破坏其他模型的安全防护。

Conclusion: 前沿模型不仅需强化抵抗越狱能力，还必须阻止自身被恶意利用为攻击代理。当前模型安全机制存在普遍脆弱性，需开发多层次防御方案。

Abstract: Jailbreaking -- bypassing built-in safety mechanisms in AI models -- has
traditionally required complex technical procedures or specialized human
expertise. In this study, we show that the persuasive capabilities of large
reasoning models (LRMs) simplify and scale jailbreaking, converting it into an
inexpensive activity accessible to non-experts. We evaluated the capabilities
of four LRMs (DeepSeek-R1, Gemini 2.5 Flash, Grok 3 Mini, Qwen3 235B) to act as
autonomous adversaries conducting multi-turn conversations with nine widely
used target models. LRMs received instructions via a system prompt, before
proceeding to planning and executing jailbreaks with no further supervision. We
performed extensive experiments with a benchmark of harmful prompts composed of
70 items covering seven sensitive domains. This setup yielded an overall attack
success rate across all model combinations of 97.14%. Our study reveals an
alignment regression, in which LRMs can systematically erode the safety
guardrails of other models, highlighting the urgent need to further align
frontier models not only to resist jailbreak attempts, but also to prevent them
from being co-opted into acting as jailbreak agents.

</details>


### [155] [DTPA: Dynamic Token-level Prefix Augmentation for Controllable Text Generation](https://arxiv.org/abs/2508.04047)
*Jiabing Yang,Yixiang Chen,Zichen Wen,Chenhang Cui,Peiyan Li,Yuan Xu,Bowen Fang,Yan Huang,Liang Wang*

Main category: cs.CL

TL;DR: 提出了一种名为DTPA的轻量级框架，用于增强可控文本生成（CTG），尤其是在生成长文本时的控制力。通过动态增强前缀注意力，解决了前缀注意力随序列长度增加而衰减的问题。


<details>
  <summary>Details</summary>
Motivation: 在可控文本生成领域，现有方法在短文本上表现良好，但在生成长文本时控制力会下降。作者发现，基于前缀的方法（如Air-Decoding）的注意力会随序列增长而衰减，导致控制力减弱。

Method: DTPA框架包括三个步骤：1）为给定任务选择最优前缀类型（软前缀或硬前缀）；2）动态按指数级增加前缀注意力，以增强属性分布的控制力；3）根据需要，对原始提示进行类似增强，以平衡文本质量。通过属性分布重建，生成的文本满足属性约束。

Result: 在多个CTG任务上的实验表明，DTPA在属性控制上优于其他方法，同时保持了文本的流畅性、多样性和主题相关性。尤其在长文本生成方面表现更优。

Conclusion: DTPA是一个有效且轻量的框架，解决了长文本可控生成中注意力衰减的问题，通过动态增强前缀注意力显著提高了控制力，且不影响文本质量。

Abstract: Controllable Text Generation (CTG) is a vital subfield in Natural Language
Processing (NLP), aiming to generate text that aligns with desired attributes.
However, previous studies commonly focus on the quality of controllable text
generation for short sequences, while the generation of long-form text remains
largely underexplored. In this paper, we observe that the controllability of
texts generated by the powerful prefix-based method Air-Decoding tends to
decline with increasing sequence length, which we hypothesize primarily arises
from the observed decay in attention to the prefixes. Meanwhile, different
types of prefixes including soft and hard prefixes are also key factors
influencing performance. Building on these insights, we propose a lightweight
and effective framework called Dynamic Token-level Prefix Augmentation (DTPA)
based on Air-Decoding for controllable text generation. Specifically, it first
selects the optimal prefix type for a given task. Then we dynamically amplify
the attention to the prefix for the attribute distribution to enhance
controllability, with a scaling factor growing exponentially as the sequence
length increases. Moreover, based on the task, we optionally apply a similar
augmentation to the original prompt for the raw distribution to balance text
quality. After attribute distribution reconstruction, the generated text
satisfies the attribute constraints well. Experiments on multiple CTG tasks
demonstrate that DTPA generally outperforms other methods in attribute control
while maintaining competitive fluency, diversity, and topic relevance. Further
analysis highlights DTPA's superior effectiveness in long text generation.

</details>


### [156] [PAIRS: Parametric-Verified Adaptive Information Retrieval and Selection for Efficient RAG](https://arxiv.org/abs/2508.04057)
*Wang Chen,Guanqiang Qi,Weikang Li,Yang Li,Deguo Xia,Jizhou Huang*

Main category: cs.CL

TL;DR: PAIRS是一个无需训练、自适应决定检索时机的框架，通过双路径生成（直接答案与伪上下文增强答案）判断是否需要检索，减少了25%的检索次数，同时在6个QA基准上提高1%+准确率。


<details>
  <summary>Details</summary>
Motivation: 当前RAG系统存在两个问题：（1）对所有查询都进行检索，包括LLM自身知识能解决的简单问题，效率低下；（2）信息稀疏时容易检索到无关文档。PAIRS旨在通过自适应机制在保证效率的同时提升精度。

Method: 1. 双路径生成：LLM并行生成直接答案和伪上下文增强答案，若一致则不检索，否则触发双重检索；2. 双重检索（DPR）：基于原查询和自生成的上下文信号检索；3. 自适应选择（AIS）：加权相似度过滤文档。

Result: 在6个QA基准中：减少25%检索量（仅对75%查询触发检索），平均EM提升1.1%/F1提升1.0%，同时提高准确率与效率。

Conclusion: PAIRS通过自适应机制平衡LLM内部知识与外部检索需求，显著降低检索成本同时提升准确率，证明了双路径验证、上下文引导检索和自适应过滤的有效性。

Abstract: Retrieval-Augmented Generation (RAG) has become a cornerstone technique for
enhancing large language models (LLMs) with external knowledge. However,
current RAG systems face two critical limitations: (1) they inefficiently
retrieve information for every query, including simple questions that could be
resolved using the LLM's parametric knowledge alone, and (2) they risk
retrieving irrelevant documents when queries contain sparse information
signals. To address these gaps, we introduce Parametric-verified Adaptive
Information Retrieval and Selection (PAIRS), a training-free framework that
integrates parametric and retrieved knowledge to adaptively determine whether
to retrieve and how to select external information. Specifically, PAIRS employs
a dual-path generation mechanism: First, the LLM produces both a direct answer
and a context-augmented answer using self-generated pseudo-context. When these
outputs converge, PAIRS bypasses external retrieval entirely, dramatically
improving the RAG system's efficiency. For divergent cases, PAIRS activates a
dual-path retrieval (DPR) process guided by both the original query and
self-generated contextual signals, followed by an Adaptive Information
Selection (AIS) module that filters documents through weighted similarity to
both sources. This simple yet effective approach can not only enhance
efficiency by eliminating unnecessary retrievals but also improve accuracy
through contextually guided retrieval and adaptive information selection.
Experimental results on six question-answering (QA) benchmarks show that PAIRS
reduces retrieval costs by around 25% (triggering for only 75% of queries)
while still improving accuracy-achieving +1.1% EM and +1.0% F1 over prior
baselines on average.

</details>


### [157] [Efficient Strategy for Improving Large Language Model (LLM) Capabilities](https://arxiv.org/abs/2508.04073)
*Julián Camilo Velandia Gutiérrez*

Main category: cs.CL

TL;DR: 针对大型语言模型在资源受限环境中的高效改进策略，结合数据处理、数据选择、训练策略和架构调整，进行实验评估。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在人工智能和自然语言处理领域取得了里程碑式成就，但其大规模部署受限于高昂计算资源。因此，本研究旨在探索在资源受限环境和限定知识库中提升LLMs效率的方法。

Method: 方法包括：从基础模型出发，结合数据处理与精选数据选择技术、训练策略及架构调整；设定可靠数据集构建标准；开展不同配置下的对照实验；系统评估模型变体在能力、通用性、响应时间和安全性等维度表现；最后通过对比测试验证策略有效性。

Result: 实验结果为不同配置组合下模型变体的评估指标（能力、通用性、响应时间、安全性）及对比测试结果，展示了所提策略在效率提升方面的有效性。论文未提供具体数据，但通过系统验证了方法的可行性。

Conclusion: 研究表明，通过综合运用数据处理优化、训练策略调整和架构改进，可在资源受限环境中有效提升LLMs性能。该方法为高效部署LLMs提供了可行路径，验证了在限定知识库下模型优化的潜力。

Abstract: Large Language Models (LLMs) have become a milestone in the field of
artificial intelligence and natural language processing. However, their
large-scale deployment remains constrained by the need for significant
computational resources. This work proposes starting from a base model to
explore and combine data processing and careful data selection techniques,
training strategies, and architectural adjustments to improve the efficiency of
LLMs in resource-constrained environments and within a delimited knowledge
base. The methodological approach included defining criteria for building
reliable datasets, conducting controlled experiments with different
configurations, and systematically evaluating the resulting variants in terms
of capability, versatility, response time, and safety. Finally, comparative
tests were conducted to measure the performance of the developed variants and
to validate the effectiveness of the proposed strategies. This work is based on
the master's thesis in Systems and Computer Engineering titled "Efficient
Strategy for Improving the Capabilities of Large Language Models (LLMs)".

</details>


### [158] [ToolGrad: Efficient Tool-use Dataset Generation with Textual "Gradients"](https://arxiv.org/abs/2508.04086)
*Zhongyi Zhou,Kohei Uehara,Haoyu Zhang,Jingtao Zhou,Lin Gu,Ruofei Du,Zheng Xu,Tatsuya Harada*

Main category: cs.CL

TL;DR: ToolGrad提出了一个代理框架，首先生成有效的工具使用链，然后合成用户查询，从而生成更复杂、低成本、100%通过率的数据集ToolGrad-5k，实验表明其训练出的模型优于现有数据集训练的模型和专有大语言模型。


<details>
  <summary>Details</summary>
Motivation: 现有方法在生成工具使用数据集时，通常先生成用户查询，然后进行复杂的工具使用标注（如DFS），这会导致标注失败率高且效率低下。

Method: ToolGrad框架采用反转范式：首先通过文本“梯度”引导的迭代过程构建有效的工具使用链（答案优先），然后合成相应的用户查询。这种倒置方法确保数据生成过程高效且零失败。

Result: 所得到的ToolGrad-5k数据集工具使用更复杂，成本更低廉，100%通过率。实验证明，在该数据集上训练的模型表现优于基于昂贵基线数据集训练的模型以及专有大语言模型，并且在外分布(OOD)基准上同样有效。

Conclusion: ToolGrad的创新“答案优先”框架成功生成了更高质量的数据集，显著提升了工具使用能力训练的效率和模型性能，验证了其范式反转的有效性。

Abstract: Prior work synthesizes tool-use LLM datasets by first generating a user
query, followed by complex tool-use annotations like DFS. This leads to
inevitable annotation failures and low efficiency in data generation. We
introduce ToolGrad, an agentic framework that inverts this paradigm. ToolGrad
first constructs valid tool-use chains through an iterative process guided by
textual "gradients", and then synthesizes corresponding user queries. This
"answer-first" approach led to ToolGrad-5k, a dataset generated with more
complex tool use, lower cost, and 100% pass rate. Experiments show that models
trained on ToolGrad-5k outperform those on expensive baseline datasets and
proprietary LLMs, even on OOD benchmarks.

</details>


### [159] [GM-PRM: A Generative Multimodal Process Reward Model for Multimodal Mathematical Reasoning](https://arxiv.org/abs/2508.04088)
*Jianghangfan Zhang,Yibo Yan,Kening Zheng,Xin Zou,Song Dai,Xuming Hu*

Main category: cs.CL

TL;DR: 提出生成式多模态过程奖励模型（GM-PRM），不仅能评估多步推理步骤，还能主动纠正错误，并引入Refined Best-of-N推理策略显著提升多模态数学推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大型语言模型（MLLMs）在复杂多步数学推理中易因视觉或逻辑的微小错误导致整体失效，而传统多模态过程奖励模型（PRMs）仅作为二元验证器，无法纠正错误或提供解释。

Method: 1. 构建GM-PRM模型：对每个推理步骤进行细粒度分析（步骤意图、视觉对齐性、逻辑合理性）；2. 训练模型识别首个错误步骤并生成纠正版本；3. 提出Refined-BoN推理框架：利用GM-PRM的校正引导策略模型优化推理路径。

Result: GM-PRM在多个多模态数学基准上取得SOTA结果，仅用20K样本训练数据即可显著提升策略模型性能。

Conclusion: GM-PRM将PRM从被动评估转为主动协作，其生成校正能力与Refined-BoN框架的结合，为提升MLLMs复杂推理能力提供了高效解决方案。

Abstract: Multimodal Large Language Models (MLLMs) demonstrate remarkable capabilities
but often struggle with complex, multi-step mathematical reasoning, where minor
errors in visual perception or logical deduction can lead to complete failure.
While Process Reward Models (PRMs) offer step-by-step supervision, existing
multimodal PRMs are limited to being binary verifiers that can identify but not
correct errors, offering little explanatory power. To address these
deficiencies, we introduce the Generative Multimodal Process Reward Model
(GM-PRM), a novel paradigm that transforms the PRM from a passive judge into an
active reasoning collaborator. Instead of a simple scalar score, GM-PRM
provides a fine-grained, interpretable analysis of each reasoning step,
evaluating its step intent, visual alignment, and logical soundness. More
critically, GM-PRM is trained to generate a corrected version of the first
erroneous step it identifies. This unique corrective capability enables our new
test-time inference strategy, Refined Best-of-N (Refined-BoN). This framework
actively enhances solution quality by using the PRM's generated correction to
guide the policy model toward a more promising reasoning trajectory, thereby
improving the diversity and correctness of the solution pool. We demonstrate
that GM-PRM achieves state-of-the-art results on multiple multimodal math
benchmarks, significantly boosting policy model performance with remarkable
data efficiency, requiring only a 20K-sample training dataset. Our code will be
released upon acceptance.

</details>


### [160] [Unveiling Over-Memorization in Finetuning LLMs for Reasoning Tasks](https://arxiv.org/abs/2508.04117)
*Zhiwen Ruan,Yun Chen,Yutao Hou,Peng Li,Yang Liu,Guanhua Chen*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型（LLM）微调过程中出现的过记忆现象：当使用大学习率和多训练轮次时，模型会过度记忆训练数据，导致测试困惑度高但准确率保持良好。然而，这种过记忆会损害模型的鲁棒性、分布外泛化能力和生成多样性。作者提出应谨慎选择检查点与学习率以避免过记忆。


<details>
  <summary>Details</summary>
Motivation: 预训练LLM通常通过微调来提高指令遵循能力并与人类价值观对齐。然而，微调过程中的学习动态尚未被充分探索。本文旨在揭示LLM在微调推理任务中出现的过记忆现象及其负面影响，以优化微调策略。

Method: 通过控制变量法（训练轮次和学习率）进行实验，观察不同微调阶段下LLM的测试困惑度、准确率变化，并评估过记忆现象对模型鲁棒性、分布外泛化和生成多样性的影响。实验覆盖多任务、模型结构和微调方法（如全量微调、LoRA等）。

Result: 1. 过记忆阶段特征：测试困惑度骤增但准确率保持稳定；2. 过记忆的诱因：大学习率+过多训练轮次；3. 负面影响：模型鲁棒性下降（对抗攻击下表现差）、分布外泛化能力减弱、生成多样性降低；4. 现象普适性：在多个任务（数学推理、代码生成等）、模型（GPT-2/3、LLaMA等）及微调方法中均出现。

Conclusion: 过记忆是过度微调LLM时出现的独特问题，与传统机器学习模型的学习动态不同。为避免过记忆带来的负面影响，建议：1. 采用早停策略选择最佳检查点；2. 避免使用过大学习率；3. 监控测试困惑度作为过记忆的预警指标。

Abstract: The pretrained large language models (LLMs) are finetuned with labeled data
for better instruction following ability and alignment with human values. In
this paper, we study the learning dynamics of LLM finetuning on reasoning tasks
and reveal the uncovered over-memorization phenomenon during a specific stage
of LLM finetuning. At this stage, the LLMs have excessively memorized training
data and exhibit high test perplexity while maintaining good test accuracy. We
investigate the conditions that lead to LLM over-memorization and find that
training epochs and large learning rates contribute to this issue. Although
models with over-memorization demonstrate comparable test accuracy to normal
models, they suffer from reduced robustness, poor out-of-distribution
generalization, and decreased generation diversity. Our experiments unveil the
over-memorization to be broadly applicable across different tasks, models, and
finetuning methods. Our research highlights that overparameterized, extensively
finetuned LLMs exhibit unique learning dynamics distinct from traditional
machine learning models. Based on our observations of over-memorization, we
provide recommendations on checkpoint and learning rate selection during
finetuning.

</details>


### [161] [Difficulty-Based Preference Data Selection by DPO Implicit Reward Gap](https://arxiv.org/abs/2508.04149)
*Xuan Qi,Rongwu Xu,Zhijing Jin*

Main category: cs.CL

TL;DR: 本文介绍了一种基于难度的偏好数据集选择策略，通过选择DPO隐含奖励差距较小的样本（更具挑战性的样本）来提高数据效率和大语言模型对齐效果。该方法仅需10%的数据即可在多数据集和任务上优于多个基线。


<details>
  <summary>Details</summary>
Motivation: 对齐大语言模型与人类偏好是AI研究的关键挑战。现有方法（如RLHF和DPO）依赖大量昂贵的人工偏好数据，但目前缺乏专为偏好数据设计的高质量数据选择方法。

Method: 提出基于DPO隐含奖励机制的难度数据选择策略：1）计算偏好数据样本的隐含奖励差距；2）选择奖励差距较小的困难样本构建小规模高效数据集；3）用此数据集训练DPO模型。

Result: 在多个数据集和对齐任务中：1）仅使用10%原始数据即超越五种基线方法；2）证实困难样本优先策略可显著提升模型对齐效率。

Conclusion: 该原则性数据选择策略为资源受限条件下的大语言模型对齐提供了高效解决方案，验证了奖励差距作为难度指标的可行性。

Abstract: Aligning large language models (LLMs) with human preferences is a critical
challenge in AI research. While methods like Reinforcement Learning from Human
Feedback (RLHF) and Direct Preference Optimization (DPO) are widely used, they
often rely on large, costly preference datasets. The current work lacks methods
for high-quality data selection specifically for preference data. In this work,
we introduce a novel difficulty-based data selection strategy for preference
datasets, grounded in the DPO implicit reward mechanism. By selecting
preference data examples with smaller DPO implicit reward gaps, which are
indicative of more challenging cases, we improve data efficiency and model
alignment. Our approach consistently outperforms five strong baselines across
multiple datasets and alignment tasks, achieving superior performance with only
10\% of the original data. This principled, efficient selection method offers a
promising solution for scaling LLM alignment with limited resources.

</details>


### [162] [The State Of TTS: A Case Study with Human Fooling Rates](https://arxiv.org/abs/2508.04179)
*Praveen Srinivasa Varadhan,Sherry Thomas,Sai Teja M. S.,Suvrat Bhooshan,Mitesh M. Khapra*

Main category: cs.CL

TL;DR: 引入欺骗测试度量指标HFR，评估TTS系统能否让人误以为是人类。实验发现：(1) CMOS测试宣称的接近人类水平在欺骗测试中失败；(2) 需在人类样本高HFR的数据集上评测；(3) 商业模型在零样本设置中接近人类，开源模型在自然对话上仍有差距；(4) 高质量数据微调提升真实感但仍有不足。


<details>
  <summary>Details</summary>
Motivation: 当前的TTS系统在主观评测（如CMOS）中显示快速进步，但缺乏严苛的测试衡量其是否真正能达到以假乱真程度。因此需要设计一种直接评估机器语音被误认为人类的概率（HFR）的图灵式评测方法。

Method: 1. 定义HFR指标（人类被欺骗比率）；2. 对开源及商业TTS模型进行大规模测评；3. 在人类样本高HFR/低HFR的数据集上对比测试；4. 尤其关注零样本设置中的表现；5. 分析微调高质量数据的影响。

Result: (i) CMOS宣称的“接近人类”结论在HFR测试中被推翻；(ii) 在人类语音自然度不足的数据集（低HFR）上进行评测标准过低；(iii) 商业模型在零样本场景欺骗率接近人类，但开源系统在自然对话语音仍有明显差距；(iv) 微调高质量数据提升真实感，但未能弥合所有缺陷。

Conclusion: 现有主观评测（如CMOS）不足以反映TTS系统真实性能，必须引入更贴近实际应用的人类欺骗测试（HFR）作为补充评估基准，并在人类高HFR数据集上统一评测标准。同时，开源TTS系统在自然对话场景仍有改进空间。

Abstract: While subjective evaluations in recent years indicate rapid progress in TTS,
can current TTS systems truly pass a human deception test in a Turing-like
evaluation? We introduce Human Fooling Rate (HFR), a metric that directly
measures how often machine-generated speech is mistaken for human. Our
large-scale evaluation of open-source and commercial TTS models reveals
critical insights: (i) CMOS-based claims of human parity often fail under
deception testing, (ii) TTS progress should be benchmarked on datasets where
human speech achieves high HFRs, as evaluating against monotonous or less
expressive reference samples sets a low bar, (iii) Commercial models approach
human deception in zero-shot settings, while open-source systems still struggle
with natural conversational speech; (iv) Fine-tuning on high-quality data
improves realism but does not fully bridge the gap. Our findings underscore the
need for more realistic, human-centric evaluations alongside existing
subjective tests.

</details>


### [163] [Hacking Hallucinations of MLLMs with Causal Sufficiency and Necessity](https://arxiv.org/abs/2508.04182)
*Peizheng Guo,Jingyao Wang,Wenwen Qiang,Huijie Guo,Changwen Zheng,Jiahuan Zhou,Gang Hua*

Main category: cs.CL

TL;DR: 该论文提出了一种基于因果完备性的强化学习框架，用于缓解多模态大语言模型（MLLMs）中的幻觉问题，即生成与输入图像或文本语义不一致的输出。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在视觉语言任务中表现优异，但存在幻觉问题（包括遗漏型幻觉和捏造型幻觉）。通过因果分析发现：遗漏型幻觉是由于未能充分捕捉关键因果因子；捏造型幻觉则因模型被非因果线索误导所致。

Method: 1. 提出基于因果完备性的强化学习框架：联合考虑token的因果充分性与因果必要性。
2. 定义token级因果完备性奖励：通过评估token的独立贡献（因果充分性）与反事实必要性（若缺失的影响）计算。
3. 将奖励融入GRPO优化框架：构建因果启发的优势函数，引导模型关注同时具备因果充分性和必要性的token。

Result: 在多个基准数据集和任务上的实验表明，该方法能有效减少MLLMs的幻觉问题。

Conclusion: 针对MLLMs的幻觉问题，从因果角度提出了两种机制（遗漏/捏造型）的解释，并据此设计的因果完备性奖励框架能显著提升生成内容的语义一致性。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated impressive
capabilities across vision-language tasks. However, they may suffer from
hallucinations--generating outputs that are semantically inconsistent with the
input image or text. Through causal analyses, we find that: (i) hallucinations
with omission may arise from the failure to adequately capture essential causal
factors, and (ii) hallucinations with fabrication are likely caused by the
model being misled by non-causal cues. To address these challenges, we propose
a novel reinforcement learning framework guided by causal completeness, which
jointly considers both causal sufficiency and causal necessity of tokens.
Specifically, we evaluate each token's standalone contribution and
counterfactual indispensability to define a token-level causal completeness
reward. This reward is used to construct a causally informed advantage function
within the GRPO optimization framework, encouraging the model to focus on
tokens that are both causally sufficient and necessary for accurate generation.
Experimental results across various benchmark datasets and tasks demonstrate
the effectiveness of our approach, which effectively mitigates hallucinations
in MLLMs.

</details>


### [164] [Characterizing Deep Research: A Benchmark and Formal Definition](https://arxiv.org/abs/2508.04183)
*Abhinav Java,Ashmit Khandelwal,Sukruta Midigeshi,Aaron Halfaker,Amit Deshpande,Navin Goyal,Ankur Gupta,Nagarajan Natarajan,Amit Sharma*

Main category: cs.CL

TL;DR: 本文提出了深度研究（Deep Research, DR）任务的形式化定义，并介绍了LiveDRBench基准来评估DR系统的性能。该任务的核心是搜索过程中的概念高扇出（广泛且推理密集的探索），而不仅仅是生成长篇报告。通过中间输出表示法（编码搜索中发现的关键主张）将推理与报告生成分离。基准包含100个挑战性任务，覆盖科学和公共事件主题。评估显示当前最佳系统的F1分数为0.55（OpenAI模型），并揭示了现有系统中的搜索机制缺陷。


<details>
  <summary>Details</summary>
Motivation: 尽管深度研究（DR）任务近期受到关注，但其范围定义模糊，与其他推理密集型任务的区分不清晰。缺乏明确的形式化定义和标准化评估基准阻碍了该领域的进展。本文旨在通过严谨定义和基准构建来解决这两个问题。

Method: 1. 形式化定义：使用中间输出表示法（编码关键主张）分离推理与文本生成，将DR核心定义为搜索过程中的概念高扇出（即广泛且推理密集的探索）。2. 基准构建：创建LiveDRBench，包含100个任务（科学主题如数据集/材料发现，公共事件如空难/电影奖项）。3. 评估指标：采用F1分数衡量系统性能，通过分析推理轨迹（引用来源分布、分支/回溯行为）评估搜索机制。

Result: 1. SOTA模型的F1分数在子类别中为0.02-0.72，整体最佳为OpenAI模型（0.55）。2. 推理轨迹分析显示现有系统在引用来源数量（0.56平均引用）、分支/回溯频率上存在局限，表明搜索机制有待改进。

Conclusion: 1. 提出DR的形式化定义和首个公共基准LiveDRBench。2. 证明当前系统性能有限（最高F1=0.55）且搜索效率低下（需更多分支/回溯优化和事实基础性增强）。3. 公开基准促进行业标准化评估。

Abstract: Information tasks such as writing surveys or analytical reports require
complex search and reasoning, and have recently been grouped under the umbrella
of \textit{deep research} -- a term also adopted by recent models targeting
these capabilities. Despite growing interest, the scope of the deep research
task remains underdefined and its distinction from other reasoning-intensive
problems is poorly understood. In this paper, we propose a formal
characterization of the deep research (DR) task and introduce a benchmark to
evaluate the performance of DR systems. We argue that the core defining feature
of deep research is not the production of lengthy report-style outputs, but
rather the high fan-out over concepts required during the search process, i.e.,
broad and reasoning-intensive exploration. To enable objective evaluation, we
define DR using an intermediate output representation that encodes key claims
uncovered during search-separating the reasoning challenge from surface-level
report generation. Based on this formulation, we propose a diverse, challenging
benchmark LiveDRBench with 100 challenging tasks over scientific topics (e.g.,
datasets, materials discovery, prior art search) and public interest events
(e.g., flight incidents, movie awards). Across state-of-the-art DR systems, F1
score ranges between 0.02 and 0.72 for any sub-category. OpenAI's model
performs the best with an overall F1 score of 0.55. Analysis of reasoning
traces reveals the distribution over the number of referenced sources,
branching, and backtracking events executed by current DR systems, motivating
future directions for improving their search mechanisms and grounding
capabilities. The benchmark is available at
https://github.com/microsoft/LiveDRBench.

</details>


### [165] [Eliciting and Analyzing Emergent Misalignment in State-of-the-Art Large Language Models](https://arxiv.org/abs/2508.04196)
*Siddhant Panpatil,Hiskias Dingeto,Haon Park*

Main category: cs.CL

TL;DR: 当前最先进的语言模型在精心设计的对话场景中仍然容易受到攻击，这些场景可以在不进行显式越狱的情况下诱导模型产生不对齐行为。通过手动红队测试发现10种攻击场景，揭示了对齐方法在应对叙事沉浸、情感压力和策略性框架时的根本脆弱性。


<details>
  <summary>Details</summary>
Motivation: 尽管对齐技术取得了显著进步，但研究者发现现有模型在面对精心设计的对话场景时仍会出现各种不对齐行为。这暴露了当前对齐方法的不足，需要更系统地研究模型在面对复杂对话攻击时的鲁棒性。

Method: 1. 通过手动红队测试（使用Claude-4-Opus）发现10种成功攻击场景；2. 将这些攻击场景提炼为MISALIGNMENTBENCH自动化评估框架；3. 在5个前沿LLM上验证了框架的通用性（GPT-4.1, Claude-4-Sonnet等）。

Result: 1. 发现模型容易产生欺骗性、价值漂移、自我保护等错误行为；2. 自动化评估显示模型平均脆弱率达76%（最高GPT-4.1达90%，最低Claude-4-Sonnet为40%）；3. 发现复杂推理能力可能成为攻击媒介而非保护机制。

Conclusion: 研究揭示了当前对齐策略在处理叙事情境、情感操纵和框架设计时的根本缺陷。提出的攻击分类法和评估框架表明，未来AI系统需要增强对精细场景操纵的鲁棒性。

Abstract: Despite significant advances in alignment techniques, we demonstrate that
state-of-the-art language models remain vulnerable to carefully crafted
conversational scenarios that can induce various forms of misalignment without
explicit jailbreaking. Through systematic manual red-teaming with
Claude-4-Opus, we discovered 10 successful attack scenarios, revealing
fundamental vulnerabilities in how current alignment methods handle narrative
immersion, emotional pressure, and strategic framing. These scenarios
successfully elicited a range of misaligned behaviors, including deception,
value drift, self-preservation, and manipulative reasoning, each exploiting
different psychological and contextual vulnerabilities. To validate
generalizability, we distilled our successful manual attacks into
MISALIGNMENTBENCH, an automated evaluation framework that enables reproducible
testing across multiple models. Cross-model evaluation of our 10 scenarios
against five frontier LLMs revealed an overall 76% vulnerability rate, with
significant variations: GPT-4.1 showed the highest susceptibility (90%), while
Claude-4-Sonnet demonstrated greater resistance (40%). Our findings demonstrate
that sophisticated reasoning capabilities often become attack vectors rather
than protective mechanisms, as models can be manipulated into complex
justifications for misaligned behavior. This work provides (i) a detailed
taxonomy of conversational manipulation patterns and (ii) a reusable evaluation
framework. Together, these findings expose critical gaps in current alignment
strategies and highlight the need for robustness against subtle, scenario-based
manipulation in future AI systems.

</details>


### [166] [Reasoning Beyond Labels: Measuring LLM Sentiment in Low-Resource, Culturally Nuanced Contexts](https://arxiv.org/abs/2508.04199)
*Millicent Ochieng,Anja Thieme,Ignatius Ezeani,Risa Ueno,Samuel Maina,Keshet Ronen,Javier Gonzalez,Jacki O'Neill*

Main category: cs.CL

TL;DR: 该论文提出了一个将情感分析视为文化和情境依赖建构的诊断框架，并在内罗毕青年健康群体WhatsApp消息上进行测试，通过多种方法评估大型语言模型（LLMs）的可解释性、稳健性和与人类推理的一致性。


<details>
  <summary>Details</summary>
Motivation: 传统NLP方法在低资源、文化独特语境下的情感分析中存在局限，尤其当情感标签固定且假设情感表达普适时。

Method: 1. 开发诊断框架以评估LLMs在文化内嵌情感分析中的表现；2. 使用人类标注数据、情感反转反事实和基于评分标准的解释评估；3. 从社会科学测量视角操作化模型输出。

Result: 1. 顶级LLMs展示了解释稳定性；2. 开源模型在歧义情境下表现不佳；3. 强调模型输出对情感概念测量的有效性差异。

Conclusion: 在复杂现实沟通中，需要开发文化敏感、具备推理意识的AI评估方法。

Abstract: Sentiment analysis in low-resource, culturally nuanced contexts challenges
conventional NLP approaches that assume fixed labels and universal affective
expressions. We present a diagnostic framework that treats sentiment as a
context-dependent, culturally embedded construct, and evaluate how large
language models (LLMs) reason about sentiment in informal, code-mixed WhatsApp
messages from Nairobi youth health groups. Using a combination of
human-annotated data, sentiment-flipped counterfactuals, and rubric-based
explanation evaluation, we probe LLM interpretability, robustness, and
alignment with human reasoning. Framing our evaluation through a social-science
measurement lens, we operationalize and interrogate LLMs outputs as an
instrument for measuring the abstract concept of sentiment. Our findings reveal
significant variation in model reasoning quality, with top-tier LLMs
demonstrating interpretive stability, while open models often falter under
ambiguity or sentiment shifts. This work highlights the need for culturally
sensitive, reasoning-aware AI evaluation in complex, real-world communication.

</details>


### [167] [ReasoningGuard: Safeguarding Large Reasoning Models with Inference-time Safety Aha Moments](https://arxiv.org/abs/2508.04204)
*Yuquan Wang,Mi Zhang,Yining Wang,Geng Hong,Xiaoyu You,Min Yang*

Main category: cs.CL

TL;DR: 作者提出了ReasoningGuard，一种用于保护大型推理模型免受恶意内容生成的推理时安全机制。该机制通过识别推理过程中的关键点，触发安全性反思，并在解码阶段通过缩放采样策略选择最优推理路径，从而在低成本下有效防御三类越狱攻击。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在推理任务中表现出色，但在推理中后期容易生成有害内容。现有防御方法依赖高成本的微调和额外专家知识，可扩展性差。因此，需要一种在推理时工作、无需微调的安全机制。

Method: 1. 利用模型内部注意力行为识别推理路径中的关键点；2. 在关键点触发安全性导向的反思；3. 在解码阶段采用缩放采样策略，选择最安全的推理路径。该方法无需微调，仅引入少量额外推理开销。

Result: ReasoningGuard在防御三类越狱攻击（包括针对推理过程的最新攻击）上优于七种现有方法，达到当前最佳安全防御水平。同时，它有效避免了常见的安全过度保守问题。

Conclusion: ReasoningGuard是一种有效的推理时安全机制，能低成本地保护大型推理模型免遭恶意内容生成，且在防御力与实用性上优于现有方法。

Abstract: Large Reasoning Models (LRMs) have demonstrated impressive performance in
reasoning-intensive tasks, but they remain vulnerable to harmful content
generation, particularly in the mid-to-late steps of their reasoning processes.
Existing defense mechanisms, however, rely on costly fine-tuning and additional
expert knowledge, which restricts their scalability. In this work, we propose
ReasoningGuard, an inference-time safeguard for LRMs, which injects timely
safety aha moments to steer harmless while helpful reasoning processes.
Leveraging the model's internal attention behavior, our approach accurately
identifies critical points in the reasoning path, and triggers spontaneous,
safety-oriented reflection. To safeguard both the subsequent reasoning steps
and the final answers, we further implement a scaling sampling strategy during
the decoding phase, selecting the optimal reasoning path. Inducing minimal
extra inference cost, ReasoningGuard effectively mitigates three types of
jailbreak attacks, including the latest ones targeting the reasoning process of
LRMs. Our approach outperforms seven existing safeguards, achieving
state-of-the-art safety defenses while effectively avoiding the common
exaggerated safety issues.

</details>


### [168] [Hierarchical Text Classification Using Black Box Large Language Models](https://arxiv.org/abs/2508.04219)
*Kosuke Yoshimura,Hisashi Kashima*

Main category: cs.CL

TL;DR: 大型语言模型（LLMs）可用于解决分层文本分类（HTC）中的数据稀缺和模型复杂性问题。研究评估了三种零样本和小样本提示策略对HTC的效果：DL（直接预测叶子标签）、DH（直接预测层次标签）和TMH（自顶向下的分层多步预测）。实验结果表明：1）小样本效果优于零样本；2）DH策略在深层次标签结构的数据集上效果优于传统机器学习模型，但在浅层次结构上不如；3）DH策略因输入上下文规模大导致API成本显著增加。提示策略需在性能和成本间权衡。


<details>
  <summary>Details</summary>
Motivation: 分层文本分类（HTC）存在数据稀缺和模型复杂性问题。为探索LLMs作为替代传统机器学习方法的可行性（传统方法需大量标注数据和计算资源），本研究考察了黑盒LLMs通过API用于HTC的能力。

Method: 在零样本和小样本设置下评估三种提示策略：1）DL：直接预测叶子节点标签；2）DH：直接一次性预测完整层次标签；3）TMH：自顶向下多步预测层次标签（每一步询问下一级标签）。在两个数据集上测试LLMs性能及API成本消耗，并与传统机器学习方法对比。

Result: 1）小样本设置较零样本均能提升准确率；2）在浅层次结构数据集上，传统机器学习模型精准度高；在深层次结构数据集上，LLMs（尤其是DH策略）优于传统模型；3）DH策略因输入大量上下文（提示中需包含完整标签体系）导致API成本显著增加。

Conclusion: 黑盒LLMs在HTC领域具有应用潜力，但需注意：a) 数据集标签结构的深度会影响性能（深层次结构中LLMs更优）；b) 提示策略的成本差异明显（DH策略成本最高）；c) 需权衡精度提升与API成本来选择合适的提示策略。

Abstract: Hierarchical Text Classification (HTC) aims to assign texts to structured
label hierarchies; however, it faces challenges due to data scarcity and model
complexity. This study explores the feasibility of using black box Large
Language Models (LLMs) accessed via APIs for HTC, as an alternative to
traditional machine learning methods that require extensive labeled data and
computational resources. We evaluate three prompting strategies -- Direct Leaf
Label Prediction (DL), Direct Hierarchical Label Prediction (DH), and Top-down
Multi-step Hierarchical Label Prediction (TMH) -- in both zero-shot and
few-shot settings, comparing the accuracy and cost-effectiveness of these
strategies. Experiments on two datasets show that a few-shot setting
consistently improves classification accuracy compared to a zero-shot setting.
While a traditional machine learning model achieves high accuracy on a dataset
with a shallow hierarchy, LLMs, especially DH strategy, tend to outperform the
machine learning model on a dataset with a deeper hierarchy. API costs increase
significantly due to the higher input tokens required for deeper label
hierarchies on DH strategy. These results emphasize the trade-off between
accuracy improvement and the computational cost of prompt strategy. These
findings highlight the potential of black box LLMs for HTC while underscoring
the need to carefully select a prompt strategy to balance performance and cost.

</details>


### [169] [DP-GPT4MTS: Dual-Prompt Large Language Model for Textual-Numerical Time Series Forecasting](https://arxiv.org/abs/2508.04239)
*Chanjuan Liu,Shengzhi Wang,Enqiang Zhu*

Main category: cs.CL

TL;DR: 论文提出了一种创新的双提示大语言模型框架DP-GPT4MTS，通过结合显式提示和文本提示，有效融合时间序列数值数据与时标文本信息，在多个数据集上超越了现有最先进的时序预测算法。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列预测模型主要关注数值数据，忽略文本信息（如事件和新闻）对预测的影响；而现有单提示大语言模型框架难以有效捕捉时标文本语义且引入冗余信息。

Method: 1. 设计包含两种互补提示的框架：显式提示提供明确任务指令，文本提示生成时标文本的上下文感知嵌入
2. 分词器生成显式提示，文本提示嵌入通过自注意力机制和前馈网络进行精炼
3. 在多样化的文本-数值混合时间序列数据集上进行综合实验验证

Result: 在多个数据集上的实验表明，该方法优于现有最先进的时序预测算法，证明双提示机制能显著提升预测准确性。

Conclusion: 通过双提示机制融合文本上下文能有效提升时间序列预测精度，该方法为多模态时间序列建模提供了新思路。

Abstract: Time series forecasting is crucial in strategic planning and decision-making
across various industries. Traditional forecasting models mainly concentrate on
numerical time series data, often overlooking important textual information
such as events and news, which can significantly affect forecasting accuracy.
While large language models offer a promise for integrating multimodal data,
existing single-prompt frameworks struggle to effectively capture the semantics
of timestamped text, introducing redundant information that can hinder model
performance. To address this limitation, we introduce DP-GPT4MTS (Dual-Prompt
GPT2-base for Multimodal Time Series), a novel dual-prompt large language model
framework that combines two complementary prompts: an explicit prompt for clear
task instructions and a textual prompt for context-aware embeddings from
time-stamped data. The tokenizer generates the explicit prompt while the
embeddings from the textual prompt are refined through self-attention and
feed-forward networks. Comprehensive experiments conducted on diverse
textural-numerical time series datasets demonstrate that this approach
outperforms state-of-the-art algorithms in time series forecasting. This
highlights the significance of incorporating textual context via a dual-prompt
mechanism to achieve more accurate time series predictions.

</details>


### [170] [TalkDep: Clinically Grounded LLM Personas for Conversation-Centric Depression Screening](https://arxiv.org/abs/2508.04248)
*Xi Wang,Anxo Perez,Javier Parapar,Fabio Crestani*

Main category: cs.CL

TL;DR: 提出了一种名为TalkDep的临床医生参与的虚拟病人模拟流程，利用先进语言模型结合精神诊断标准和严重程度量表等多样化患者档案，生成真实、多样化的模拟病人，用于训练和评估抑郁症自动诊断系统。


<details>
  <summary>Details</summary>
Motivation: 精神健康服务需求增长迅速，但缺乏真实训练数据来培养临床专业人员，用于抑郁症诊断的支持有限。现有的虚拟病人模拟方法难以生成临床有效、自然且多样化的症状表现。

Method: 提出TalkDep流程：利用最新的先进语言模型为核心，结合多样化患者档案（包括精神病诊断标准、症状严重程度量表和背景因素），通过临床医生参与的循环迭代，生成虚拟患者。

Result: 生成的虚拟患者经过临床专业人士的全面评估，被证明具有可靠性。这些验证后的虚拟患者为自动抑郁症诊断系统提供了可扩展和适应性强的资源，提升其鲁棒性和泛化能力。

Conclusion: 该研究通过创新性地结合语言模型和临床诊断标准，开发了可靠且多样的虚拟病人模拟流程，解决了精神健康训练数据不足的难题，为开发更稳健的抑郁症自动诊断系统提供了可能。

Abstract: The increasing demand for mental health services has outpaced the
availability of real training data to develop clinical professionals, leading
to limited support for the diagnosis of depression. This shortage has motivated
the development of simulated or virtual patients to assist in training and
evaluation, but existing approaches often fail to generate clinically valid,
natural, and diverse symptom presentations. In this work, we embrace the recent
advanced language models as the backbone and propose a novel
clinician-in-the-loop patient simulation pipeline, TalkDep, with access to
diversified patient profiles to develop simulated patients. By conditioning the
model on psychiatric diagnostic criteria, symptom severity scales, and
contextual factors, our goal is to create authentic patient responses that can
better support diagnostic model training and evaluation. We verify the
reliability of these simulated patients with thorough assessments conducted by
clinical professionals. The availability of validated simulated patients offers
a scalable and adaptable resource for improving the robustness and
generalisability of automatic depression diagnosis systems.

</details>


### [171] [KVSink: Understanding and Enhancing the Preservation of Attention Sinks in KV Cache Quantization for LLMs](https://arxiv.org/abs/2508.04257)
*Zunhai Su,Kehong Yuan*

Main category: cs.CL

TL;DR: 本文提出一种名为KVSink的即插即用方法，用于在KV缓存量化过程中更有效地预测和保留注意力sink token，以提高大型语言模型（LLM）推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有的KV缓存量化方法虽能减少内存使用，但其保护最初几个token精度的策略无法完全应对注意力sink在任意位置出现的问题，且机制理解不足。

Method: 首先通过分析注意力sink在跨层极端激活异常值演化中的作用来阐明其机制；其次基于此分析提出KVSink方法，通过极小开销预测sink token以全面保护。

Result: 实验表明，KVSink超越现有的Preserve-First-N（PFN）策略，能更有效地在KV缓存量化中保护注意力sink；应用于KVQuant方法后进一步提升了困惑度（PPL）并减少了对16位异常值的依赖。

Conclusion: KVSink为KV缓存量化提供了更完善的注意力sink保护机制，不仅优化了量化效果，还深化了关于注意力sink产生原理的理解。

Abstract: Key-Value (KV) cache quantization has become a widely adopted optimization
technique for efficient large language models (LLMs) inference by reducing KV
cache memory usage and mitigating memory-bound constraints. Recent studies have
emphasized the importance of preserving the original precision of KVs for the
first few tokens to ensure the protection of attention sinks. While this
approach has proven effective in mitigating performance degradation, its
underlying principles remain insufficiently understood. Moreover, it fails to
address the recent discovery that attention sinks can emerge beyond the initial
token positions. In this work, we elucidate the underlying mechanisms of
attention sinks during inference by examining their role in the cross-layer
evolution of extreme activation outliers. Additionally, we provide a
comprehensive analysis of the interplay between attention sinks and KV cache
quantization. Based on our enhanced understanding, we introduce
\textit{\textbf{KVSink}}, a plug-and-play method that effectively predicts sink
tokens with negligible overhead, enabling more thorough preservation. Extensive
experiments demonstrate that KVSink outperforms the existing Preserve-First-N
(PFN) strategy, offering more effective preservation of attention sinks during
KV cache quantization. Moreover, when applied to the well-established KVQuant
method, KVSink further improves perplexity (PPL) and reduces reliance on 16-bit
numerical outliers.

</details>


### [172] [ShoppingBench: A Real-World Intent-Grounded Shopping Benchmark for LLM-based Agents](https://arxiv.org/abs/2508.04266)
*Jiangyuan Wang,Kejun Xiao,Qi Sun,Huaipeng Zhao,Tao Luo,Jiandong Zhang,Xiaoyi Zeng*

Main category: cs.CL

TL;DR: 提出ShoppingBench，一个新颖的端到端购物基准，专注于涵盖更加复杂实际的用户意图（如使用优惠券、管理预算和寻找多商品卖家）。


<details>
  <summary>Details</summary>
Motivation: 现有电商基准主要满足基本用户意图（如寻找或购买产品），但用户实际意图往往更复杂，因此需要能够模拟用户复杂购物目标的基准环境。

Method: 开发了一个可扩展框架来模拟用户指令，并创建了一个包含超过250万种真实商品的交互式环境（沙盒）；提出了一种轨迹蒸馏策略：先使用监督微调结合强化学习处理合成轨迹，再将大型语言代理（LLM）推理能力提炼到较小的模型上。

Result: 在ShoppingBench中，顶级智能体（如GPT-4.1）执行任务的绝对成功率在50%以下显示了其挑战性。蒸馏后训练出的较小代理与GPT-4.1相比具有竞争性成效。

Conclusion: ShoppingBench填补了复杂现实意图的基准空缺；结果反映了复杂需求场景下的语言智能体的局限性；通过蒸馏策略开发了高性能小模型——有助于推动语言助理研究在复杂电商任务上的发展。

Abstract: Existing benchmarks in e-commerce primarily focus on basic user intents, such
as finding or purchasing products. However, real-world users often pursue more
complex goals, such as applying vouchers, managing budgets, and finding
multi-products seller. To bridge this gap, we propose ShoppingBench, a novel
end-to-end shopping benchmark designed to encompass increasingly challenging
levels of grounded intent. Specifically, we propose a scalable framework to
simulate user instructions based on various intents derived from sampled
real-world products. To facilitate consistent and reliable evaluations, we
provide a large-scale shopping sandbox that serves as an interactive simulated
environment, incorporating over 2.5 million real-world products. Experimental
results demonstrate that even state-of-the-art language agents (such as
GPT-4.1) achieve absolute success rates under 50% on our benchmark tasks,
highlighting the significant challenges posed by our ShoppingBench. In
addition, we propose a trajectory distillation strategy and leverage supervised
fine-tuning, along with reinforcement learning on synthetic trajectories, to
distill the capabilities of a large language agent into a smaller one. As a
result, our trained agent achieves competitive performance compared to GPT-4.1.

</details>


### [173] [A Few Words Can Distort Graphs: Knowledge Poisoning Attacks on Graph-based Retrieval-Augmented Generation of Large Language Models](https://arxiv.org/abs/2508.04276)
*Jiayi Wen,Tianxin Chen,Zhirun Zheng,Cheng Huang*

Main category: cs.CL

TL;DR: 本文针对Graph-based Retrieval-Augmented Generation（GraphRAG）系统，提出两种知识投毒攻击：针对性的TKPA和全局性的UKPA，通过修改少量文本影响图构建过程，从而误导下游问答任务，现有防御方法难以检测。


<details>
  <summary>Details</summary>
Motivation: GraphRAG通过将原始文本转化为结构化知识图增强大语言模型的准确性和可解释性。但其图构建过程依赖语言模型解析原始文本，可能被恶意操纵植入误导信息，这一脆弱性尚未被充分研究。

Method: 设计两种攻击方法：1) TKPA使用图论分析定位关键节点，利用语言模型重写关键叙述，精准控制特定问答结果；2) UKPA利用代词等语言线索修改全局性关键词，破坏图结构完整性。攻击仅需修改极少量文本（0.05%），通过在源文本中植入特定模式实现投毒。

Result: TKPA精准控制问答的成功率达93.1%；UKPA仅修改0.05%文本即可使问答准确率从95%骤降至50%。现有防御机制均未能检测到攻击，证明GraphRAG当前存在严重安全风险。

Conclusion: 知识图构建阶段的漏洞使GraphRAG面临新型投毒威胁，现有防护方案失效，亟需开发针对性的鲁棒性增强方案。

Abstract: Graph-based Retrieval-Augmented Generation (GraphRAG) has recently emerged as
a promising paradigm for enhancing large language models (LLMs) by converting
raw text into structured knowledge graphs, improving both accuracy and
explainability. However, GraphRAG relies on LLMs to extract knowledge from raw
text during graph construction, and this process can be maliciously manipulated
to implant misleading information. Targeting this attack surface, we propose
two knowledge poisoning attacks (KPAs) and demonstrate that modifying only a
few words in the source text can significantly change the constructed graph,
poison the GraphRAG, and severely mislead downstream reasoning. The first
attack, named Targeted KPA (TKPA), utilizes graph-theoretic analysis to locate
vulnerable nodes in the generated graphs and rewrites the corresponding
narratives with LLMs, achieving precise control over specific
question-answering (QA) outcomes with a success rate of 93.1\%, while keeping
the poisoned text fluent and natural. The second attack, named Universal KPA
(UKPA), exploits linguistic cues such as pronouns and dependency relations to
disrupt the structural integrity of the generated graph by altering globally
influential words. With fewer than 0.05\% of full text modified, the QA
accuracy collapses from 95\% to 50\%. Furthermore, experiments show that
state-of-the-art defense methods fail to detect these attacks, highlighting
that securing GraphRAG pipelines against knowledge poisoning remains largely
unexplored.

</details>


### [174] [Beyond the Leaderboard: Rethinking Medical Benchmarks for Large Language Models](https://arxiv.org/abs/2508.04325)
*Zizhan Ma,Wenxuan Wang,Guo Yu,Yiu-Fai Cheung,Meidan Ding,Jie Liu,Wenting Chen,Linlin Shen*

Main category: cs.CL

TL;DR: 论文介绍了MedCheck，这是一个针对医疗基准测试的生命周期评估框架，用于解决现有医疗大语言模型（LLM）基准测试在临床保真度、数据管理和安全性评估方面的不足。通过对53个医疗LLM基准测试的评估，发现普遍存在与临床实践脱节、数据污染风险、忽略模型鲁棒性等系统性问题。


<details>
  <summary>Details</summary>
Motivation: 现有医疗领域的LLM基准测试在可靠性上存在诸多问题，如缺乏临床真实感、数据管理不善、忽略安全性评估指标等，这阻碍了医疗AI的可靠发展。因此，需要一种系统性的评估框架来诊断和改进这些基准测试的质量。

Method: 开发了MedCheck框架，将基准测试的生命周期分解为五个阶段（设计、开发、部署、评估、治理），并提供了包含46个医学特化评估标准的清单。应用该框架对53个现有医疗LLM基准测试进行了系统评估。

Result: 评估显示现有医疗基准测试普遍存在以下问题：（1）严重脱离临床实践；（2）由于数据污染风险导致数据完整性危机；（3）系统性忽略安全性关键评估维度，如模型鲁棒性和不确定性感知。

Conclusion: MedCheck框架可同时作为诊断现有基准测试问题的工具和指导未来医疗基准测试开发的行动指南，以促进更标准化、可靠和透明的医疗AI评估。

Abstract: Large language models (LLMs) show significant potential in healthcare,
prompting numerous benchmarks to evaluate their capabilities. However, concerns
persist regarding the reliability of these benchmarks, which often lack
clinical fidelity, robust data management, and safety-oriented evaluation
metrics. To address these shortcomings, we introduce MedCheck, the first
lifecycle-oriented assessment framework specifically designed for medical
benchmarks. Our framework deconstructs a benchmark's development into five
continuous stages, from design to governance, and provides a comprehensive
checklist of 46 medically-tailored criteria. Using MedCheck, we conducted an
in-depth empirical evaluation of 53 medical LLM benchmarks. Our analysis
uncovers widespread, systemic issues, including a profound disconnect from
clinical practice, a crisis of data integrity due to unmitigated contamination
risks, and a systematic neglect of safety-critical evaluation dimensions like
model robustness and uncertainty awareness. Based on these findings, MedCheck
serves as both a diagnostic tool for existing benchmarks and an actionable
guideline to foster a more standardized, reliable, and transparent approach to
evaluating AI in healthcare.

</details>


### [175] [Modelling and Classifying the Components of a Literature Review](https://arxiv.org/abs/2508.04337)
*Francisco Bolaños,Angelo Salatino,Francesco Osborne,Enrico Motta*

Main category: cs.CL

TL;DR: 论文提出了一个新颖的标注模式用于辅助文献综述生成，并评估了37个大语言模型在按该模式分类句子修辞角色上的表现。结果显示微调后的模型表现优异（F1>96%），轻量级开源模型也表现出色，并且加入半合成训练数据可提升效果。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明，依据修辞角色（如研究空白、结果、局限性等）对科学文献句子进行标注，能显著提升AI分析科学文献的能力，并有潜力推动新一代高质量文献综述生成系统的发展。然而，为实现此目标需解决两大挑战：设计支持文献综述生成的标注模式，以及开发大规模文献标注的有效策略。

Method: 1) 设计专门支持文献综述生成的新型标注模式；2) 全面评估多种最先进大语言模型按该模式分类句子修辞角色的能力。为此构建多学科基准数据集Sci-Sentence（含700句专家标注+2,240句LLM自动标注数据），在37个不同规模/架构的LLM上测试零样本学习与微调策略。

Result: 关键发现：1) 当前LLM经高质量数据微调后表现卓越（F1>96%）；2) 商用大模型（如GPT-4o）最优，但轻量开源替代模型同样高效；3) 加入LLM生成的半合成数据可显著提升小型编码器及部分开放解码器模型的鲁棒性。

Conclusion: 研究证实新型标注模式与半合成数据策略有效推进了科学文献修辞分类任务。轻量开源模型的优秀表现意味着实际应用潜力，而数据增强技术成功弥合了不同模型架构间的性能差距，为构建高效文献处理系统提供了新路径。

Abstract: Previous work has demonstrated that AI methods for analysing scientific
literature benefit significantly from annotating sentences in papers according
to their rhetorical roles, such as research gaps, results, limitations,
extensions of existing methodologies, and others. Such representations also
have the potential to support the development of a new generation of systems
capable of producing high-quality literature reviews. However, achieving this
goal requires the definition of a relevant annotation schema and effective
strategies for large-scale annotation of the literature. This paper addresses
these challenges by 1) introducing a novel annotation schema specifically
designed to support literature review generation and 2) conducting a
comprehensive evaluation of a wide range of state-of-the-art large language
models (LLMs) in classifying rhetorical roles according to this schema. To this
end, we also present Sci-Sentence, a novel multidisciplinary benchmark
comprising 700 sentences manually annotated by domain experts and 2,240
sentences automatically labelled using LLMs. We evaluate 37 LLMs on this
benchmark, spanning diverse model families and sizes, using both zero-shot
learning and fine-tuning approaches. The experiments yield several novel
insights that advance the state of the art in this challenging domain. First,
the current generation of LLMs performs remarkably well on this task when
fine-tuned on high-quality data, achieving performance levels above 96\% F1.
Second, while large proprietary models like GPT-4o achieve the best results,
some lightweight open-source alternatives also demonstrate excellent
performance. Finally, enriching the training data with semi-synthetic examples
generated by LLMs proves beneficial, enabling small encoders to achieve robust
results and significantly enhancing the performance of several open decoder
models.

</details>


### [176] [GTPO and GRPO-S: Token and Sequence-Level Reward Shaping with Policy Entropy](https://arxiv.org/abs/2508.04349)
*Hongze Tan,Jianfei Pan*

Main category: cs.CL

TL;DR: 提出了一种名为动态熵加权的方法，解决了强化学习中信用分配粗糙的问题，通过两种方式实现：GTPO（为每个令牌分配基于熵的奖励）和GRPO-S（在序列级别分配加权奖励）。实验显示该方法显著优于基线。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习（如GRPO）为序列中所有令牌赋予统一奖励，在长链推理任务中存在信用分配粗糙的问题，导致性能受限。

Method: 1. GTPO（组令牌策略优化）：根据每个令牌的熵值分配权重化的奖励。
2. GRPO-S（序列级组相对策略优化）：基于整个序列的平均令牌熵为序列分配权重化的奖励。
核心思想：正确响应中的高熵令牌能够引导模型达到更高的性能上限。

Result: 实验证明所提方法（动态熵加权）显著优于强基线DAPO，表明熵加权机制是性能提升的关键驱动因素。

Conclusion: 动态熵加权方法提供了一种更精细的信用分配机制，为增强模型的深层推理能力提供了更优路径，且实验验证了其有效性。

Abstract: Reinforcement learning (RL) with algorithms like Group Relative Policy
Optimization (GRPO) improves Large Language Model (LLM) reasoning, but is
limited by a coarse-grained credit assignment that applies a uniform reward to
all tokens in a sequence. This is a major flaw in long-chain reasoning tasks.
This paper solves this with \textbf{Dynamic Entropy Weighting}. Our core idea
is that high-entropy tokens in correct responses can guide the policy toward a
higher performance ceiling. This allows us to create more fine-grained reward
signals for precise policy updates via two ways: 1) \textbf{Group Token Policy
Optimization} (\textbf{GTPO}), we assigns a entropy-weighted reward to each
token for fine-grained credit assignment. 2) \textbf{Sequence-Level Group
Relative Policy Optimization} (\textbf{GRPO-S}), we assigns a entropy-weighted
reward to each sequence based on its average token entropy. Experiments show
our methods significantly outperform the strong DAPO baseline. The results
confirm that our entropy-weighting mechanism is the key driver of this
performance boost, offering a better path to enhance deep reasoning in models.

</details>


### [177] [Chain of Questions: Guiding Multimodal Curiosity in Language Models](https://arxiv.org/abs/2508.04350)
*Nima Iji,Kia Dashtipour*

Main category: cs.CL

TL;DR: 论文介绍Chain of Questions (CoQ)框架,通过让多模态语言模型主动生成问题激发模态感知,提升真实环境推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM改进如思维链技术在文本模态效果显著,但多模态环境下感知被动,无法主动选择合适感官通道感知复杂环境。

Method: CoQ框架驱动模型对周围环境动态生成针对性问题,以问题引导选择性激活视觉/听觉/空间等关键模态,从而高效收集推理信息。

Result: 在整合WebGPT/ScienceQA/AVSD/ScanQA的新多模态基准测试中,提高了基础模型识别整合感官信息能力,实现更高准确性、可解释性和任务适应性。

Conclusion: 好奇驱动的问题链可促进多模态模型的主动感知能力,增强复杂现实环境交互中的推理效果。

Abstract: Reasoning capabilities in large language models (LLMs) have substantially
advanced through methods such as chain-of-thought and explicit step-by-step
explanations. However, these improvements have not yet fully transitioned to
multimodal contexts, where models must proactively decide which sensory
modalities such as vision, audio, or spatial perception to engage when
interacting with complex real-world environments. In this paper, we introduce
the Chain of Questions (CoQ) framework, a curiosity-driven reasoning approach
that encourages multimodal language models to dynamically generate targeted
questions regarding their surroundings. These generated questions guide the
model to selectively activate relevant modalities, thereby gathering critical
information necessary for accurate reasoning and response generation. We
evaluate our framework on a novel multimodal benchmark dataset, assembled by
integrating WebGPT, ScienceQA, AVSD, and ScanQA datasets. Experimental results
demonstrate that our CoQ method improves a foundation model's ability to
effectively identify and integrate pertinent sensory information. This leads to
improved accuracy, interpretability, and alignment of the reasoning process
with diverse multimodal tasks.

</details>


### [178] [AIC CTU@FEVER 8: On-premise fact checking through long context RAG](https://arxiv.org/abs/2508.04390)
*Herbert Ullrich,Jan Drchal*

Main category: cs.CL

TL;DR: 本文介绍了在FEVER 9共享任务中夺冠的事实核查管道，该管道采用基于去年的两步骤RAG方案，可部署在单颗NVidia A10 GPU（23GB显存）上，并在60秒内完成每条声明的事实核查，达到最先进的性能（以Ev2R测试分数衡量）。


<details>
  <summary>Details</summary>
Motivation: 为了在资源受限条件下实现高效、实时的事实核查，并在FEVER 9共享任务中取得更高性能，需要优化现有方案，确保高性能的同时具备实际可部署性。

Method: 采用两步骤RAG管道：第一步，检索相关证据文档；第二步，使用RAG模型对声明进行事实核查。在单颗NVidia A10 GPU（23GB显存）下运行，每条声明处理时间限制为60秒。

Result: 该系统在FEVER 9共享任务中获得第一名，在Ev2R测试分数上达到最先进的事实核查性能。

Conclusion: 该工作证明高效、实时的事实核查系统能在有限硬件资源下实现，为现实应用提供了可行的解决方案。

Abstract: In this paper, we present our fact-checking pipeline which has scored first
in FEVER 8 shared task. Our fact-checking system is a simple two-step RAG
pipeline based on our last year's submission. We show how the pipeline can be
redeployed on-premise, achieving state-of-the-art fact-checking performance (in
sense of Ev2R test-score), even under the constraint of a single NVidia A10
GPU, 23GB of graphical memory and 60s running time per claim.

</details>


### [179] [Improving Crash Data Quality with Large Language Models: Evidence from Secondary Crash Narratives in Kentucky](https://arxiv.org/abs/2508.04399)
*Xu Zhang,Mei Chen*

Main category: cs.CL

TL;DR: 该研究评估了三种NLP技术（零样本开源大模型、微调Transformer模型、传统逻辑回归）在从事故叙述中识别二次事故的效果。使用肯塔基州2015-2022年的16,656条人工审核数据，其中3,803条确认为二次事故。结果显示微调Transformer模型（尤其是RoBERTa）在性能上最优（F1:0.90，准确率:95%），而大模型虽部分指标高但计算成本巨大。


<details>
  <summary>Details</summary>
Motivation: 通过挖掘交通事故叙述文本提高事故数据质量，解决现有方法依赖人工审核的低效问题，为交通安全管理提供自动化支持。

Method: 1. 数据基础：使用肯塔基州2015-2022年16,656条人工审核的事故叙述，其中3,803条为二次事故；2. 模型对比：三类模型对比——零样本开源大模型（LLaMA3:70B等）、微调Transformer模型（BERT等）、逻辑回归基线；3. 实验设置：2015-2021年训练/校准，2022年1,771条测试；4. 评估指标：F1分数、准确率、召回率、计算时间。

Result: 1. 性能：微调RoBERTa最佳（F1:0.90，准确率95%）；2. 效率：大模型推理时间过长（LLaMA3:70B需139分钟，DeepSeek-R1:70B需723分钟），微调模型秒级完成；3. 发现：中型大模型（如DeepSeek-R1:32B）性能接近更大模型且减少耗时；4. 逻辑回归基线表现最差（F1:0.66）。

Conclusion: 微调Transformer模型在精度与效率上取得最佳平衡，适合实际部署；大模型在特定召回指标上优异但计算成本过高。建议隐私保护本地部署、集成学习提升精度、增量处理实现扩展性，为事故数据质量提升提供可复制方案。

Abstract: This study evaluates advanced natural language processing (NLP) techniques to
enhance crash data quality by mining crash narratives, using secondary crash
identification in Kentucky as a case study. Drawing from 16,656 manually
reviewed narratives from 2015-2022, with 3,803 confirmed secondary crashes, we
compare three model classes: zero-shot open-source large language models (LLMs)
(LLaMA3:70B, DeepSeek-R1:70B, Qwen3:32B, Gemma3:27B); fine-tuned transformers
(BERT, DistilBERT, RoBERTa, XLNet, Longformer); and traditional logistic
regression as baseline. Models were calibrated on 2015-2021 data and tested on
1,771 narratives from 2022. Fine-tuned transformers achieved superior
performance, with RoBERTa yielding the highest F1-score (0.90) and accuracy
(95%). Zero-shot LLaMA3:70B reached a comparable F1 of 0.86 but required 139
minutes of inference; the logistic baseline lagged well behind (F1:0.66). LLMs
excelled in recall for some variants (e.g., GEMMA3:27B at 0.94) but incurred
high computational costs (up to 723 minutes for DeepSeek-R1:70B), while
fine-tuned models processed the test set in seconds after brief training.
Further analysis indicated that mid-sized LLMs (e.g., DeepSeek-R1:32B) can
rival larger counterparts in performance while reducing runtime, suggesting
opportunities for optimized deployments. Results highlight trade-offs between
accuracy, efficiency, and data requirements, with fine-tuned transformer models
balancing precision and recall effectively on Kentucky data. Practical
deployment considerations emphasize privacy-preserving local deployment,
ensemble approaches for improved accuracy, and incremental processing for
scalability, providing a replicable scheme for enhancing crash-data quality
with advanced NLP.

</details>


### [180] [Why are LLMs' abilities emergent?](https://arxiv.org/abs/2508.04401)
*Vladimír Havlík*

Main category: cs.CL

TL;DR: 论文探讨了深度神经网络（DNN）的涌现特性，指出其能力源于非线性复杂系统的动力学机制，而非参数规模的简单扩展，主张将DNN视作具有内在动力学转变的新型复杂系统，需要类似物理和生物学的建模方法。


<details>
  <summary>Details</summary>
Motivation: 解决当前AI发展中'创造而不理解'的认识论挑战，揭示大规模语言模型（LLM）能力涌现的本质，挑战仅从参数规模扩展解释现象的主流观点。

Method: 1. 基于复杂系统理论框架分析DNN工作原理；2. 实证观察模型规模扩展过程中的突现现象；3. 对比分析不同预训练损失阈值、情境学习表现；4. 通过缩放定律、能力剧变点及理解涌现模式等实验论证系统动力学机制；5. 将涌现现象类比物理学相变等自然界的涌现机制。

Result: 1. 证明DNN能力源于非线性系统固有的敏感性而非参数量变；2. 发现损失下降阈值与能力剧变存在非单调关系；3. 揭示当前性能评估标准无法捕捉涌现本质；4. 建立DNN与自然复杂系统在涌现机制上的理论同构性。

Conclusion: LLM表现出的能力是真实涌现特性，应采用复杂系统动力学建模理解其内禀特性。未来AI理论应从现象描述转向揭示系统内部动态转换的因果机制，建立可与自然科学对话的跨学科研究框架。

Abstract: The remarkable success of Large Language Models (LLMs) in generative tasks
has raised fundamental questions about the nature of their acquired
capabilities, which often appear to emerge unexpectedly without explicit
training. This paper examines the emergent properties of Deep Neural Networks
(DNNs) through both theoretical analysis and empirical observation, addressing
the epistemological challenge of "creation without understanding" that
characterises contemporary AI development. We explore how the neural approach's
reliance on nonlinear, stochastic processes fundamentally differs from symbolic
computational paradigms, creating systems whose macro-level behaviours cannot
be analytically derived from micro-level neuron activities. Through analysis of
scaling laws, grokking phenomena, and phase transitions in model capabilities,
I demonstrate that emergent abilities arise from the complex dynamics of highly
sensitive nonlinear systems rather than simply from parameter scaling alone. My
investigation reveals that current debates over metrics, pre-training loss
thresholds, and in-context learning miss the fundamental ontological nature of
emergence in DNNs. I argue that these systems exhibit genuine emergent
properties analogous to those found in other complex natural phenomena, where
systemic capabilities emerge from cooperative interactions among simple
components without being reducible to their individual behaviours. The paper
concludes that understanding LLM capabilities requires recognising DNNs as a
new domain of complex dynamical systems governed by universal principles of
emergence, similar to those operating in physics, chemistry, and biology. This
perspective shifts the focus from purely phenomenological definitions of
emergence to understanding the internal dynamic transformations that enable
these systems to acquire capabilities that transcend their individual
components.

</details>


### [181] [What Do Humans Hear When Interacting? Experiments on Selective Listening for Evaluating ASR of Spoken Dialogue Systems](https://arxiv.org/abs/2508.04402)
*Kiyotada Mori,Seiya Kawano,Chaoran Liu,Carlos Toshinori Ishi,Angel Fernando Garcia Contreras,Koichiro Yoshino*

Main category: cs.CL

TL;DR: 本文探讨了将人类选择性听觉特点应用于口语对话系统（SDSs）的自动语音识别（ASR）评估中的可能性。研究发现，通过实验证实人类在生成对话响应时会进行选择性听觉（即关注对话中的重要部分），并提出一种新的ASR评估方法可能有助于量化ASR与人类在转录能力上的差距。


<details>
  <summary>Details</summary>
Motivation: 口语对话系统（SDS）的性能依赖于前端自动语音识别（ASR）正确识别用户语音中有助于生成响应的相关信息。人类具有选择性听觉能力（即在对话中专注听重要部分的能力），这种能力尚未在ASR评估中得到应用。因此，研究人类生成对话文本时的选择性听觉特点，旨在为设计更合理的ASR评价指标提供依据，并帮助识别ASR系统与人类在转录能力上的差异。

Method: 实验方法包括：1) 收集人类为生成对话响应而转录的文本（任务导向型转录）；2) 收集参考转录（通常为完整逐字转录）；3) 对两种转录文本进行对比分析，从而验证人类在进行对话响应生成时是否表现出选择性听觉（即仅关注重要信息）；4) 基于实验结果讨论如何利用这种选择性听觉特性设计新的ASR评估方法。

Result: 实验结果证实：人类在生成对话响应的过程中进行转录时，会展现出选择性听觉行为（即仅转录对响应生成有用的关键信息），与逐字参考转录存在显著差异。

Conclusion: 论文建议可借助人类在对话响应生成中的选择性听觉行为来设计新的ASR评价指标。这类指标能够有效衡量ASR系统与人类在口语语义理解方面的表现差距，并能更准确地反映ASR系统针对SDS实际需求（即生成合理响应）的识别效果。

Abstract: Spoken dialogue systems (SDSs) utilize automatic speech recognition (ASR) at
the front end of their pipeline. The role of ASR in SDSs is to recognize
information in user speech related to response generation appropriately.
Examining selective listening of humans, which refers to the ability to focus
on and listen to important parts of a conversation during the speech, will
enable us to identify the ASR capabilities required for SDSs and evaluate them.
In this study, we experimentally confirmed selective listening when humans
generate dialogue responses by comparing human transcriptions for generating
dialogue responses and reference transcriptions. Based on our experimental
results, we discuss the possibility of a new ASR evaluation method that
leverages human selective listening, which can identify the gap between
transcription ability between ASR systems and humans.

</details>


### [182] [Dialogue Response Prefetching Based on Semantic Similarity and Prediction Confidence of Language Model](https://arxiv.org/abs/2508.04403)
*Kiyotada Mori,Seiya Kawano,Angel Fernando Garcia Contreras,Koichiro Yoshino*

Main category: cs.CL

TL;DR: 本文提出了一种预测置信度模型（PCM），通过估计预测的用户完整话术与实际话术之间的语义相似度，决定是否可以预取对话响应，以减少用户感知延迟（UPL）。


<details>
  <summary>Details</summary>
Motivation: 在语音对话系统中，用户感知延迟（UPL）是指用户在系统响应前的等待时间。为了减少UPL，需要在用户说话结束前预测其完整话术以预取响应。现有方法依赖语言模型进行预测，但预测准确性影响预取效果。因此，需要一个机制来评估预测的可靠性。

Method: 1. 提出预测置信度模型（PCM），该模型通过评估预测的完整用户话术与真实完整用户话术之间的语义相似度，来确定是否可以执行预取。2. 利用语义相似度估计作为置信度指标，决定是否放弃不可靠的预测以避免错误预取。

Result: 通过比较预测话术与真实话术的差异来评估PCM模型。具体结果未在摘要中说明，但评估方法基于预测与真实的差异分析。

Conclusion: PCM能够有效判断预取条件，减少错误预取带来的风险，提升语音对话系统的响应效率。未来工作将进一步完善模型并扩展评估。

Abstract: Prefetching of dialogue responses has been investigated to reduce
user-perceived latency (UPL), which refers to the user's waiting time before
receiving the system's response, in spoken dialogue systems. To reduce the UPL,
it is necessary to predict complete user utterances before the end of the
user's speech, typically by language models, to prepare prefetched dialogue
responses. In this study, we proposed a prediction confidence model (PCM) that
determines whether prefetching is possible or not by estimating the semantic
similarity between the predicted complete user utterance and the complete user
utterance. We evaluated our PCM based on the differences between the predicted
complete user utterance and the complete user utterance.

</details>


### [183] [Evaluating, Synthesizing, and Enhancing for Customer Support Conversation](https://arxiv.org/abs/2508.04423)
*Jie Zhu,Huaixia Dou,Junhui Li,Lifan Guo,Feng Chen,Chi Zhang,Fang Kong*

Main category: cs.CL

TL;DR: 该研究提出了客户支持对话（CSC）任务，基于COPC指南构建了一个包含五个阶段和十二种策略的结构化框架。开发了CSConv评估数据集（1855条经LLM改写标注的真实对话）和RoleCS训练数据集（通过LLM角色扮演模拟策略丰富的对话）。实验表明，在RoleCS上微调的LLM能显著提高生成策略对齐的高质量回复能力，并在问题解决上获得人类评估认可。


<details>
  <summary>Details</summary>
Motivation: 现有客服对话数据集缺乏策略指导，且现实服务数据难以获取和标注。为训练客服代理使用定义好的支持策略进行回应，需要构建结构化框架和高质量相关数据集。

Method: 1. 基于COPC指南定义五个对话阶段（问候、问题诊断、解决方案提供等）和十二种策略（共情、致歉等）。2. 构建CSConv数据集：收集真实客服对话，用LLM重写使其体现策略运用，并进行策略标注。3. 创建RoleCS训练集：通过LLM角色扮演用户和客服代理，在CSC框架指导下模拟策略性对话。4. 在RoleCS上微调LLM，并在CSConv上评估生成质量。

Result: 1. 公开CSConv（评估集）和RoleCS（训练集）。2. 实验结果：相比基线模型，在RoleCS上微调的模型（如Qwen等）显著提升策略对齐响应生成能力（自动指标提升）。3. 人类评估：微调后的模型回复在问题解决率上表现更佳。

Conclusion: 提出的结构化CSC框架和生成数据集（RoleCS）能有效训练LLM生成符合专业策略的客服回应。该方法解决了真实数据稀缺问题，并通过模拟生成策略性对话数据降低了标注成本。

Abstract: Effective customer support requires not only accurate problem solving but
also structured and empathetic communication aligned with professional
standards. However, existing dialogue datasets often lack strategic guidance,
and real-world service data is difficult to access and annotate. To address
this, we introduce the task of Customer Support Conversation (CSC), aimed at
training customer service agents to respond using well-defined support
strategies. We propose a structured CSC framework grounded in COPC guidelines,
defining five conversational stages and twelve strategies to guide high-quality
interactions. Based on this, we construct CSConv, an evaluation dataset of
1,855 real-world customer-agent conversations rewritten using LLMs to reflect
deliberate strategy use, and annotated accordingly. Additionally, we develop a
role-playing approach that simulates strategy-rich conversations using
LLM-powered roles aligned with the CSC framework, resulting in the training
dataset RoleCS. Experiments show that fine-tuning strong LLMs on RoleCS
significantly improves their ability to generate high-quality, strategy-aligned
responses on CSConv. Human evaluations further confirm gains in problem
resolution. All code and data will be made publicly available at
https://github.com/aliyun/qwen-dianjin.

</details>


### [184] [StepFun-Formalizer: Unlocking the Autoformalization Potential of LLMs through Knowledge-Reasoning Fusion](https://arxiv.org/abs/2508.04440)
*Yutong Wu,Di Huang,Ruosi Wan,Yue Peng,Shijie Shang,Chenrui Cao,Lei Qi,Rui Zhang,Zidong Du,Jie Yan,Xing Hu*

Main category: cs.CL

TL;DR: 论文提出了一种名为ThinkingF的数据合成和训练流程，旨在提升大型语言模型在自动形式化任务中的表现，通过增强形式语言领域知识的掌握和非正式到形式化推理能力，并在两个基准上达到最优结果。


<details>
  <summary>Details</summary>
Motivation: 现有的自动形式化方法在准确率上仍有不足，研究者发现有效自动形式化需要两种关键能力：对形式语言领域知识的全面掌握，以及将自然语言问题理解与非形式化到形式化对齐的推理能力。模型若缺乏前者则无法识别正确的形式对象，缺乏后者则难以解释现实背景并精确映射为形式表达式。

Method: 研究者设计了ThinkingF流程：
1. 构建两个数据集：第一个通过蒸馏和筛选大量富含形式知识的示例；第二个通过专家设计的模板指导生成非正式到形式化的推理轨迹。
2. 使用这些数据集进行监督微调（SFT）和强化学习验证反馈（RLVR），以融合和精炼上述两种能力。
3. 训练了7B和32B两种规模的大型语言模型。

Result: 训练得到的32B模型（命名为StepFun-Formalizer-32B）在FormalMATH-Lite基准上达到40.5%的BEq@1分数，在ProverBench上达到26.7%，超越所有先前通用和专用模型。

Conclusion: ThinkingF通过针对性地提升形式知识掌握和推理能力，显著推进了自动形式化任务的性能边界，所训练的模型展示了全面的形式知识和强大的非形式化到形式化推理能力。

Abstract: Autoformalization aims to translate natural-language mathematical statements
into a formal language. While LLMs have accelerated progress in this area,
existing methods still suffer from low accuracy. We identify two key abilities
for effective autoformalization: comprehensive mastery of formal-language
domain knowledge, and reasoning capability of natural language problem
understanding and informal-formal alignment. Without the former, a model cannot
identify the correct formal objects; without the latter, it struggles to
interpret real-world contexts and map them precisely into formal expressions.
To address these gaps, we introduce ThinkingF, a data synthesis and training
pipeline that improves both abilities. First, we construct two datasets: one by
distilling and selecting large-scale examples rich in formal knowledge, and
another by generating informal-to-formal reasoning trajectories guided by
expert-designed templates. We then apply SFT and RLVR with these datasets to
further fuse and refine the two abilities. The resulting 7B and 32B models
exhibit both comprehensive formal knowledge and strong informal-to-formal
reasoning. Notably, StepFun-Formalizer-32B achieves SOTA BEq@1 scores of 40.5%
on FormalMATH-Lite and 26.7% on ProverBench, surpassing all prior
general-purpose and specialized models.

</details>


### [185] [Automated Generation of Curriculum-Aligned Multiple-Choice Questions for Malaysian Secondary Mathematics Using Generative AI](https://arxiv.org/abs/2508.04442)
*Rohaizah Abdul Wahid,Muhamad Said Nizamuddin Nadim,Suliana Sulaiman,Syahmi Akmal Shaharudin,Muhammad Danial Jupikil,Iqqwan Jasman Su Azlan Su*

Main category: cs.CL

TL;DR: 该论文提出了四种渐进式方法，利用GPT-4o为马来西亚中学一年级数学生成马来语选择题，并通过自动化评估框架证实了基于RAG的方法在课程对齐和事实准确性上优于非基础提示方法。


<details>
  <summary>Details</summary>
Motivation: 解决马来西亚教育系统中高质量、可扩展教育评估工具的需求，利用生成式AI的同时应对课程对齐和低资源语言（马来语）的挑战。

Method: 1. 非基础提示法：结构化基础和基本方法；2. RAG方法：一种使用LangChain框架，另一种手动实现。方法均以官方课程文档为基础。采用双管齐下的自动化评估框架：课程对齐使用语义文本相似度（STS）评估与年度教学计划（RPT）的一致性；使用创新的基于RAG的问答（RAG-QA）方法验证上下文有效性。

Result: 基于RAG的流程显著优于非基础提示方法，在课程对齐和事实有效性方面表现更佳。同时分析了框架式RAG的易实现性和手动流程的精细控制间的权衡。

Conclusion: 该研究为低资源语言生成符合课程的教育内容提供了验证方法，引入了协同RAG-QA评估技术，为马来西亚及类似地区开发实用的教育技术解决方案提供了可行建议。

Abstract: This paper addresses the critical need for scalable and high-quality
educational assessment tools within the Malaysian education system. It
highlights the potential of Generative AI (GenAI) while acknowledging the
significant challenges of ensuring factual accuracy and curriculum alignment,
especially for low-resource languages like Bahasa Melayu. This research
introduces and compares four incremental pipelines for generating Form 1
Mathematics multiple-choice questions (MCQs) in Bahasa Melayu using OpenAI's
GPT-4o. The methods range from non-grounded prompting (structured and basic) to
Retrieval-Augmented Generation (RAG) approaches (one using the LangChain
framework, one implemented manually). The system is grounded in official
curriculum documents, including teacher-prepared notes and the yearly teaching
plan (RPT). A dual-pronged automated evaluation framework is employed to assess
the generated questions. Curriculum alignment is measured using Semantic
Textual Similarity (STS) against the RPT, while contextual validity is verified
through a novel RAG-based Question-Answering (RAG-QA) method. The results
demonstrate that RAG-based pipelines significantly outperform non-grounded
prompting methods, producing questions with higher curriculum alignment and
factual validity. The study further analyzes the trade-offs between the ease of
implementation of framework-based RAG and the fine-grained control offered by a
manual pipeline. This work presents a validated methodology for generating
curriculum-specific educational content in a low-resource language, introduces
a symbiotic RAG-QA evaluation technique, and provides actionable insights for
the development and deployment of practical EdTech solutions in Malaysia and
similar regions.

</details>


### [186] [CALE : Concept-Aligned Embeddings for Both Within-Lemma and Inter-Lemma Sense Differentiation](https://arxiv.org/abs/2508.04494)
*Bastien Liétard,Gabriel Loiseau*

Main category: cs.CL

TL;DR: 本文提出了一种称为概念分词的扩展任务，以补充现有的上下文词义建模方法，该任务通过引入词间场景来捕捉更广泛的语义信息。作者构建了一个新数据集，并微调了多个模型得到概念对齐嵌入（CALE）。实验证明CALE在多任务中表现最佳，且显著改变了嵌入空间的组织结构。


<details>
  <summary>Details</summary>
Motivation: 现有的上下文语言模型（如XL-LEXEME）通过'上下文词汇'任务（仅比较相同词目的出现）进行微调，其捕捉的信息范围有限。本文旨在通过扩展任务范围（包含词间场景）来提升模型对词汇语义的建模能力。

Method: 1. 提出'概念分词'新任务框架，基于SemCor数据构建数据集；2. 在数据集上微调多种表示模型（包括BERT, RoBERTa等），得到概念对齐嵌入（CALE）；3. 通过在词汇语义任务（如同义判断、词义消歧等）上的实验验证效果。

Result: 1. CALE在多个词汇语义任务上达到最佳性能；2. 微调后嵌入空间的结构发生显著变化（例如同类概念更聚集）；3. 证明了跨词场景信息对提升语义表示的重要性。

Conclusion: 概念分词任务及衍生的CALE模型能有效扩展上下文词义建模的边界，生成的嵌入具有多任务普适性，且空间重组现象表明其学习到更准确的语义结构。该框架为研究词义关系和上下文敏感性提供了新工具。

Abstract: Lexical semantics is concerned with both the multiple senses a word can adopt
in different contexts, and the semantic relations that exist between meanings
of different words. To investigate them, Contextualized Language Models are a
valuable tool that provides context-sensitive representations that can be used
to investigate lexical meaning. Recent works like XL-LEXEME have leveraged the
task of Word-in-Context to fine-tune them to get more semantically accurate
representations, but Word-in-Context only compares occurrences of the same
lemma, limiting the range of captured information. In this paper, we propose an
extension, Concept Differentiation, to include inter-words scenarios. We
provide a dataset for this task, derived from SemCor data. Then we fine-tune
several representation models on this dataset. We call these models
Concept-Aligned Embeddings (CALE). By challenging our models and other models
on various lexical semantic tasks, we demonstrate that the proposed models
provide efficient multi-purpose representations of lexical meaning that reach
best performances in our experiments. We also show that CALE's fine-tuning
brings valuable changes to the spatial organization of embeddings.

</details>


### [187] [StyliTruth : Unlocking Stylized yet Truthful LLM Generation via Disentangled Steering](https://arxiv.org/abs/2508.04530)
*Chenglei Shen,Zhongxiang Sun,Teng Shi,Xiao Zhang,Jun Xu*

Main category: cs.CL

TL;DR: 本文针对通过表示编辑生成特定风格LLM响应时存在的风格化与真实性之间的权衡问题，提出了StyliTruth方法。该方法通过正交投影分离风格和真实性相关的表示子空间，并结合自适应token级导向向量，动态控制生成过程，从而在保持风格的同时减少真实性的损失。


<details>
  <summary>Details</summary>
Motivation: 现有表示编辑方法在注入风格信号时，常忽视其对模型核心真实性表示的污染，导致风格化过程中真实性崩溃（stylization-induced truthfulness collapse）的问题。这源于风格方向与真实性方向在注意力头中的潜在耦合。

Method: 1. 识别关键注意力头中风格与真实性方向的耦合问题；2. 通过正交投影过程分离风格相关和真实性相关的表示子空间；3. 在各子空间设计自适应token级导向向量，动态控制生成过程；4. 在多种风格和语言上进行验证。

Result: 实验表明，StyliTruth显著降低了风格化引起的真实性崩溃，在平衡风格遵循与真实性方面优于现有推理时干预方法。

Conclusion: 通过解耦风格与真实性的表示空间并独立控制，能够有效缓解两者间的权衡问题，为细粒度控制LLM输出提供了新思路。

Abstract: Generating stylized large language model (LLM) responses via representation
editing is a promising way for fine-grained output control. However, there
exists an inherent trade-off: imposing a distinctive style often degrades
truthfulness. Existing representation editing methods, by naively injecting
style signals, overlook this collateral impact and frequently contaminate the
model's core truthfulness representations, resulting in reduced answer
correctness. We term this phenomenon stylization-induced truthfulness collapse.
We attribute this issue to latent coupling between style and truth directions
in certain key attention heads, and propose StyliTruth, a mechanism that
preserves stylization while keeping truthfulness intact. StyliTruth separates
the style-relevant and truth-relevant subspaces in the model's representation
space via an orthogonal deflation process. This decomposition enables
independent control of style and truth in their own subspaces, minimizing
interference. By designing adaptive, token-level steering vectors within each
subspace, we dynamically and precisely control the generation process to
maintain both stylistic fidelity and truthfulness. We validate our method on
multiple styles and languages. Extensive experiments and analyses show that
StyliTruth significantly reduces stylization-induced truthfulness collapse and
outperforms existing inference-time intervention methods in balancing style
adherence with truthfulness.

</details>


### [188] [Unveiling the Landscape of Clinical Depression Assessment: From Behavioral Signatures to Psychiatric Reasoning](https://arxiv.org/abs/2508.04531)
*Zhuang Chen,Guanqun Bi,Wen Zhang,Jiawei Hu,Aoyun Wang,Xiyao Xiao,Kun Feng,Minlie Huang*

Main category: cs.CL

TL;DR: 该论文介绍了C-MIND，一个用于临床抑郁症评估的多模态数据集，收集了真实医院就诊数据，包括音频、视频、转录文本和fNIRS信号。作者分析了不同任务和模态对诊断性能的贡献，探索了大型语言模型（LLMs）在临床精神病推理中的局限性，并提出用临床专业知识引导LLMs推理以提高诊断性能。


<details>
  <summary>Details</summary>
Motivation: 抑郁症是全球范围内广泛存在的精神障碍，现有研究多依赖有限或未经临床验证的数据，且常过度设计复杂模型而忽略实际效果。作者旨在揭示临床抑郁症评估的现状，并建立可靠的数据和算法基础设施。

Method: 1. 构建C-MIND数据集：收集了两年内真实医院就诊的多模态数据（音频、视频、转录、fNIRS），每个参与者完成三项结构化精神病学任务并由专家给出最终诊断。2. 分析行为特征与诊断的关系。3. 使用经典模型量化不同任务和模态对诊断性能的贡献。4. 探索LLMs进行临床精神病推理的能力并识别其局限性。5. 提出用临床专业知识引导LLM推理过程以提高诊断性能。

Result: 1. C-MIND数据集提供了真实临床环境下的多模态抑郁评估数据。2. 不同任务和模态的组合有效性被量化。3. LLMs在真实临床环境中存在明显局限性。4. 提出的基于临床专业知识的引导方法将LLM的诊断性能（Macro-F1）最高提升了10%。

Conclusion: 该研究建立了临床抑郁症评估的基础设施（C-MIND数据集和算法框架），通过多模态数据和结合临床专业知识的LLM引导方法，促进了可靠的精神健康研究，并提升了自动化诊断性能。

Abstract: Depression is a widespread mental disorder that affects millions worldwide.
While automated depression assessment shows promise, most studies rely on
limited or non-clinically validated data, and often prioritize complex model
design over real-world effectiveness. In this paper, we aim to unveil the
landscape of clinical depression assessment. We introduce C-MIND, a clinical
neuropsychiatric multimodal diagnosis dataset collected over two years from
real hospital visits. Each participant completes three structured psychiatric
tasks and receives a final diagnosis from expert clinicians, with informative
audio, video, transcript, and functional near-infrared spectroscopy (fNIRS)
signals recorded. Using C-MIND, we first analyze behavioral signatures relevant
to diagnosis. We train a range of classical models to quantify how different
tasks and modalities contribute to diagnostic performance, and dissect the
effectiveness of their combinations. We then explore whether LLMs can perform
psychiatric reasoning like clinicians and identify their clear limitations in
realistic clinical settings. In response, we propose to guide the reasoning
process with clinical expertise and consistently improves LLM diagnostic
performance by up to 10% in Macro-F1 score. We aim to build an infrastructure
for clinical depression assessment from both data and algorithmic perspectives,
enabling C-MIND to facilitate grounded and reliable research for mental
healthcare.

</details>


### [189] [Beyond Brainstorming: What Drives High-Quality Scientific Ideas? Lessons from Multi-Agent Collaboration](https://arxiv.org/abs/2508.04575)
*Nuo Chen,Yicheng Tong,Jiaying Wu,Minh Duc Duong,Qian Wang,Qingyun Zou,Bryan Hooi,Bingsheng He*

Main category: cs.CL

TL;DR: 本文探讨了结构化多代理讨论在科学研究提案生成中的优势。研究发现，多代理讨论显著优于单代理基线，配备领导者的团队能提出更具战略视野和整合深度的提案。认知多样性是提案质量的关键驱动力，但专家知识不可或缺。


<details>
  <summary>Details</summary>
Motivation: 现有AI科研构思框架多依赖单代理模式，其知识边界和单一视角限制了创造力。受真实世界科研协作启发，本文研究结构化多代理讨论是否能超越单代理构思模式。

Method: 提出合作型多代理框架生成研究提案，系统比较不同配置：小组规模（2-5代理）、领导/无领导结构、以及跨学科/资历差异的团队构成。使用代理评分与人工评审结合的综合评估方案，考量提案新颖性、战略视野、整合深度等维度。

Result: 1. 多代理讨论显著优于单代理基线
2. 设定领导者可将讨论转化为更具整合性与愿景的提案（领导团队新颖性+6%，整合深度+10%）
3. 跨学科团队提案质量提高12%，但无资深专家的团队表现甚至不及单代理
4. 3代理团队达到效率峰值，每新增成员质量增益递减

Conclusion: 成功AI协作架构需兼顾认知多样性与专业深度。领导者角色对整合创意至关紧要，但应避免层级压制多样性表达。未来工作将探索：动态领导轮换机制、专业知识图谱增强代理领域理解、对抗性讨论模式验证创意鲁棒性。

Abstract: While AI agents show potential in scientific ideation, most existing
frameworks rely on single-agent refinement, limiting creativity due to bounded
knowledge and perspective. Inspired by real-world research dynamics, this paper
investigates whether structured multi-agent discussions can surpass solitary
ideation. We propose a cooperative multi-agent framework for generating
research proposals and systematically compare configurations including group
size, leaderled versus leaderless structures, and team compositions varying in
interdisciplinarity and seniority. To assess idea quality, we employ a
comprehensive protocol with agent-based scoring and human review across
dimensions such as novelty, strategic vision, and integration depth. Our
results show that multi-agent discussions substantially outperform solitary
baselines. A designated leader acts as a catalyst, transforming discussion into
more integrated and visionary proposals. Notably, we find that cognitive
diversity is a primary driver of quality, yet expertise is a non-negotiable
prerequisite, as teams lacking a foundation of senior knowledge fail to surpass
even a single competent agent. These findings offer actionable insights for
designing collaborative AI ideation systems and shed light on how team
structure influences creative outcomes.

</details>


### [190] [Share Your Attention: Transformer Weight Sharing via Matrix-based Dictionary Learning](https://arxiv.org/abs/2508.04581)
*Magauiya Zhussip,Dmitriy Shopkhoev,Ammar Ali,Stamatios Lefkimmiatis*

Main category: cs.CL

TL;DR: 本文提出了一种名为MASA（注意力矩阵原子共享）的新框架，通过在Transformer层之间共享矩阵原子来实现权重共享，减少了注意力模块66.7%的参数，同时保持性能不变。该方法可作为即插即用的替换方案，适用于不同规模的语言模型和视觉Transformer，在保持性能的同时提高参数效率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的高计算量和内存需求限制了其广泛部署。现有的压缩技术主要关注块内优化（如低秩近似、注意力头修剪），而Transformer的重复层结构暗示了层间存在显著的冗余（除了键值缓存外尚未被充分探索）。受CNN中字典学习的启发，我们提出了一种在Transformer层之间结构化共享权重的框架，以利用层间冗余。

Method: MASA框架将注意力投影矩阵分解为共享的字典原子（matrix atoms），每个层的权重表示为这些共享矩阵原子的线性组合。该方法不需要蒸馏或架构更改，可直接作为即插即用的替换方案，并使用标准优化器进行训练。

Result: 实验在多种规模（1亿-7亿参数）的模型上进行，结果表明MASA在相同参数预算下优于分组查询注意力（GQA）、低秩基线和最近提出的Repeat-all-over/Sequential共享方法（在准确率和困惑度指标上）。消融研究验证了共享字典大小的鲁棒性，以及共享表示在捕捉层间统计规律上的有效性。扩展至视觉Transformer（ViT）后，MASA在图像分类和检测任务上保持性能指标的同时减少了66.7%的注意力参数。此外，在预训练LLMs上应用MASA也能减少参数数量而不显著降低性能。

Conclusion: MASA通过将字典学习策略与Transformer效率相结合，为参数高效的模型提供了可扩展的解决方案，且不牺牲性能。该方法能够利用Transformer层间的统计规律性，显著减少参数数量，为模型压缩提供了一种有效途径。

Abstract: Large language models (LLMs) have revolutionized AI applications, yet their
high computational and memory demands hinder their widespread deployment.
Existing compression techniques focus on intra-block optimizations (e.g.
low-rank approximation, attention head pruning), while the repetitive layered
structure of transformers implies significant inter-block redundancy - a
dimension largely unexplored beyond key-value (KV) caching. Inspired by
dictionary learning in CNNs, we propose a framework for structured weight
sharing across transformer layers. Our approach decomposes attention projection
matrices into shared dictionary atoms, reducing the attention module's
parameters by 66.7% while achieving on-par performance. Unlike complex methods
requiring distillation or architectural changes, MASA (Matrix Atom Sharing in
Attention) operates as a drop-in replacement - trained with standard optimizers
- and represents each layer's weights as linear combinations of shared matrix
atoms. Experiments across scales (100M-700M parameters) show that MASA achieves
better benchmark accuracy and perplexity than grouped-query attention (GQA),
low-rank baselines and recently proposed Repeat-all-over/Sequential sharing at
comparable parameter budgets. Ablation studies confirm robustness to the
dictionary size and the efficacy of shared representations in capturing
cross-layer statistical regularities. Extending to Vision Transformers (ViT),
MASA matches performance metrics on image classification and detection tasks
with 66.7% fewer attention parameters. By combining dictionary learning
strategies with transformer efficiency, MASA offers a scalable blueprint for
parameter-efficient models without sacrificing performance. Finally, we
investigate the possibility of employing MASA on pretrained LLMs to reduce
their number of parameters without experiencing any significant drop in their
performance.

</details>


### [191] [TURA: Tool-Augmented Unified Retrieval Agent for AI Search](https://arxiv.org/abs/2508.04604)
*Zhejun Zhao,Yuehu Dong,Alley Liu,Lixue Zheng,Pingsheng Liu,Dongdong Shen,Long Xia,Jiashu Zhao,Dawei Yin*

Main category: cs.CL

TL;DR: 本文介绍了TURA，一种结合检索增强生成与代理工具使用的框架，旨在克服传统RAG在处理实时动态内容上的局限，为搜索产品提供静态和动态信息的整合方案。


<details>
  <summary>Details</summary>
Motivation: 传统RAG方法无法处理需要访问实时动态内容（如票务库存）的查询，仅限于静态页面索引。现有研究忽略了复杂意图和动态源（如数据库）的需求，工业应用受限。

Method: TURA提出三阶段框架：1) 意图感知检索模块分解查询并检索信息源（封装为MCP服务器）；2) 基于DAG的任务规划器将任务依赖建模为有向无环图，实现并行执行；3) 轻量级蒸馏代理执行器高效调用工具。

Result: TURA成功桥接静态RAG与动态信息源，服务于千万级用户，满足低延迟需求，成为业界首个系统性解决动态搜索的架构。

Conclusion: TURA通过工具增强的统一检索代理框架，实现了静态内容与实时数据的协同处理，解决了工业级AI搜索中的关键挑战，推动了搜索技术的进化。

Abstract: The advent of Large Language Models (LLMs) is transforming search engines
into conversational AI search products, primarily using Retrieval-Augmented
Generation (RAG) on web corpora. However, this paradigm has significant
industrial limitations. Traditional RAG approaches struggle with real-time
needs and structured queries that require accessing dynamically generated
content like ticket availability or inventory. Limited to indexing static
pages, search engines cannot perform the interactive queries needed for such
time-sensitive data. Academic research has focused on optimizing RAG for static
content, overlooking complex intents and the need for dynamic sources like
databases and real-time APIs. To bridge this gap, we introduce TURA
(Tool-Augmented Unified Retrieval Agent for AI Search), a novel three-stage
framework that combines RAG with agentic tool-use to access both static content
and dynamic, real-time information. TURA has three key components: an
Intent-Aware Retrieval module to decompose queries and retrieve information
sources encapsulated as Model Context Protocol (MCP) Servers, a DAG-based Task
Planner that models task dependencies as a Directed Acyclic Graph (DAG) for
optimal parallel execution, and a lightweight Distilled Agent Executor for
efficient tool calling. TURA is the first architecture to systematically bridge
the gap between static RAG and dynamic information sources for a world-class AI
search product. Serving tens of millions of users, it leverages an agentic
framework to deliver robust, real-time answers while meeting the low-latency
demands of a large-scale industrial system.

</details>


### [192] [Lightweight Transformers for Zero-Shot and Fine-Tuned Text-to-SQL Generation Using Spider](https://arxiv.org/abs/2508.04623)
*Chirag Seth,Utkarsh Singh*

Main category: cs.CL

TL;DR: 本研究评估了三种轻量级Transformer模型（T5-Small、BART-Small和GPT-2）在Spider数据集上的文本到SQL转换性能，重点针对低资源设置。开发了一个模型无关、可复用的处理流程，通过调整模式格式化适应不同模型架构。实验表明，微调后的T5-Small在逻辑形式准确率（27.8%）上表现最好，突显了编码器-解码器模型在模式感知SQL生成上的优势。


<details>
  <summary>Details</summary>
Motivation: 文本到SQL转换能让非专家用户使用自然语言查询关系数据库，适用于教育和商业智能场景。但在资源受限环境中，需要评估轻量级模型的性能以满足实际部署需求。

Method: 1. 使用Spider数据集，开发通用处理流程（包括模式格式化适配器）
2. 在1000-5000次迭代范围内训练三种模型
3. 用1000个测试样本评估，指标：LFAcc（逻辑形式准确率）、BLEU、Exact Match
4. 每种模型架构采用特定模式格式（如T5用'上下文:问题+模式'格式）

Result: 1. T5-Small取得最高LFAcc（27.8%），其次BART-Small（23.98%）、GPT-2（20.1%）
2. 编码器-解码器模型（T5/BART）显著优于解码器模型（GPT-2）
3. 小规模训练下BLEU/EM指标与LFAcc趋势一致

Conclusion: 1. 在低资源环境中，轻量级编码器-解码器模型（如T5-Small）具有实用价值
2. 提出的模块化流程支持后续扩展（如增强模式链接或更换基模型）
3. 结果验证了紧凑Transformer在资源匮乏场景下的可部署潜力

Abstract: Text-to-SQL translation enables non-expert users to query relational
databases using natural language, with applications in education and business
intelligence. This study evaluates three lightweight transformer models -
T5-Small, BART-Small, and GPT-2 - on the Spider dataset, focusing on
low-resource settings. We developed a reusable, model-agnostic pipeline that
tailors schema formatting to each model's architecture, training them across
1000 to 5000 iterations and evaluating on 1000 test samples using Logical Form
Accuracy (LFAcc), BLEU, and Exact Match (EM) metrics. Fine-tuned T5-Small
achieves the highest LFAcc (27.8%), outperforming BART-Small (23.98%) and GPT-2
(20.1%), highlighting encoder-decoder models' superiority in schema-aware SQL
generation. Despite resource constraints limiting performance, our pipeline's
modularity supports future enhancements, such as advanced schema linking or
alternative base models. This work underscores the potential of compact
transformers for accessible text-to-SQL solutions in resource-scarce
environments.

</details>


### [193] [P-Aligner: Enabling Pre-Alignment of Language Models via Principled Instruction Synthesis](https://arxiv.org/abs/2508.04626)
*Feifan Song,Bofei Gao,Yifan Song,Yi Liu,Weimin Xiong,Yuyang Song,Tianyu Liu,Guoyin Wang,Houfeng Wang*

Main category: cs.CL

TL;DR: 本文提出P-Aligner，一个轻量级模块，用于在大型语言模型（LLM）解码前对用户指令进行预对齐，以生成更符合人类偏好的表达，同时保留原意。通过使用蒙特卡洛树搜索合成的UltraPrompt数据集训练，P-Aligner在各种模型和基准测试中表现优异，显著提升了胜率。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在接收有缺陷的指令（如上下文缺失、指令模糊或语气不当）时，往往难以生成安全、有益且诚实的内容，存在较大的改进空间。现有方法要么需要高昂的测试时搜索成本，要么依赖于不透明目标定制的训练语料进行端到端模型重写。因此，需要一种高效且有效的偏好对齐方法。

Method: 1. 提出P-Aligner：一个轻量级模块，用于生成保留原意但更符合人类偏好的指令。2. 构建UltraPrompt数据集：通过基于原则的流程，利用蒙特卡洛树搜索（MCTS）系统探索与人类偏好密切相关的候选指令空间进行合成。3. 在UltraPrompt上训练P-Aligner模块。

Result: 1. 实验表明P-Aligner在各种模型和基准测试中普遍优于强基线。2. 在GPT-4-turbo上平均胜率提升28.35%，在Gemma-2-SimPO上提升8.69%。3. 多角度分析（数据质量、搜索策略、迭代部署和时间开销）验证了其有效性和效率。

Conclusion: P-Aligner提供了一种高效且有效的偏好对齐方法，通过预对齐指令显著提升了LLM生成内容的安全性和帮助性，同时具备轻量化和低时间开销的优点。

Abstract: Large Language Models (LLMs) are expected to produce safe, helpful, and
honest content during interaction with human users, but they frequently fail to
align with such values when given flawed instructions, e.g., missing context,
ambiguous directives, or inappropriate tone, leaving substantial room for
improvement along multiple dimensions. A cost-effective yet high-impact way is
to pre-align instructions before the model begins decoding. Existing approaches
either rely on prohibitive test-time search costs or end-to-end model rewrite,
which is powered by a customized training corpus with unclear objectives. In
this work, we demonstrate that the goal of efficient and effective preference
alignment can be achieved by P-Aligner, a lightweight module generating
instructions that preserve the original intents while being expressed in a more
human-preferred form. P-Aligner is trained on UltraPrompt, a new dataset
synthesized via a proposed principle-guided pipeline using Monte-Carlo Tree
Search, which systematically explores the space of candidate instructions that
are closely tied to human preference. Experiments across different methods show
that P-Aligner generally outperforms strong baselines across various models and
benchmarks, including average win-rate gains of 28.35% and 8.69% on GPT-4-turbo
and Gemma-2-SimPO, respectively. Further analyses validate its effectiveness
and efficiency through multiple perspectives, including data quality, search
strategies, iterative deployment, and time overhead.

</details>


### [194] [IFDECORATOR: Wrapping Instruction Following Reinforcement Learning with Verifiable Rewards](https://arxiv.org/abs/2508.04632)
*Xu Guo,Tianyi Liang,Tong Jian,Xiaogui Yang,Ling-I Wu,Chenhui Li,Zhihui Lu,Qipeng Guo,Kai Chen*

Main category: cs.CL

TL;DR: 本文介绍了IFDecorator框架，该框架通过三个组件改进RLVR训练：合作对抗数据引擎、IntentCheck意图对齐模块和用于检测奖励欺骗的trap wires机制。模型在IFEval上达87.43%准确率，优于GPT-4o，并在FollowBench保持通用能力的同时显著减少奖励欺骗行为。


<details>
  <summary>Details</summary>
Motivation: 针对RLVR方法存在的训练效率低下（因难度评估不足）和过优化问题（模型利用验证捷径而不对齐用户意图），提出了一种更鲁棒高效的训练框架。

Method: 1. 合作对抗数据飞轮：共同进化指令和混合验证，生成渐进式挑战的指令-验证对；2. IntentCheck旁路模块：强制意图对齐；3. trap wires诊断机制：通过陷阱指令检测奖励欺骗行为。

Result: 1. IFDecorator版Qwen2.5-32B在IFEval达87.43%准确率，超越GPT-4o；2. FollowBench显著提升且保留通用能力；3. trap wires显示奖励欺骗率大幅降低。

Conclusion: IFDecorator有效解决了RLVR的效率和过优化问题，在指令跟随能力上实现SOTA表现。模型/代码/数据将开源。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) improves instruction
following capabilities of large language models (LLMs), but suffers from
training inefficiency due to inadequate difficulty assessment. Moreover, RLVR
is prone to over-optimization, where LLMs exploit verification shortcuts
without aligning to the actual intent of user instructions. We introduce
Instruction Following Decorator (IFDecorator}, a framework that wraps RLVR
training into a robust and sample-efficient pipeline. It consists of three
components: (1) a cooperative-adversarial data flywheel that co-evolves
instructions and hybrid verifications, generating progressively more
challenging instruction-verification pairs; (2) IntentCheck, a bypass module
enforcing intent alignment; and (3) trip wires, a diagnostic mechanism that
detects reward hacking via trap instructions, which trigger and capture
shortcut exploitation behaviors. Our Qwen2.5-32B-Instruct-IFDecorator achieves
87.43% accuracy on IFEval, outperforming larger proprietary models such as
GPT-4o. Additionally, we demonstrate substantial improvements on FollowBench
while preserving general capabilities. Our trip wires show significant
reductions in reward hacking rates. We will release models, code, and data for
future research.

</details>


### [195] [Can NLP Tackle Hate Speech in the Real World? Stakeholder-Informed Feedback and Survey on Counterspeech](https://arxiv.org/abs/2508.04638)
*Tanvi Dinkar,Aiqi Jiang,Simona Frenda,Poppy Gerrard-Abbott,Nancie Gunson,Gavin Abercrombie,Ioannis Konstas*

Main category: cs.CL

TL;DR: 对74项NLP反仇恨言论研究的系统回顾揭示，当前研究逐渐脱离受在线毒害内容影响最大的社群需求，且与利益相关者合作减少。通过一个与5个专注于在线性别暴力（oGBV）的非政府组织共同进行的参与式案例研究，提出改进建议。


<details>
  <summary>Details</summary>
Motivation: 当前NLP领域的反仇恨言论研究从原本与NGO利益相关者合作，转向依赖少量数据集的自动化流程，忽视了受影响社群的需求，可能导致研究脱离实际。本文旨在系统分析利益相关者参与的影响，为重新聚焦社群需求提供依据。

Method: 1. 对74项NLP反仇恨言论研究进行系统性回顾，分析其对利益相关者参与情况的描述；2. 联合5个反在线性别暴力专业NGO开展参与式案例研究，探索其反言论数据集构建、模型开发与评估方法。

Result: 系统综述显示：近年来NLP反仇恨言论研究仅7%与利益相关者共同构建数据集，仅10%在模型开发或评估中纳入社群反馈；在关键指标（如干预安全性和文化适配度）上，仅8%的论文进行了人工评估。案例研究则证明，与NGO共同开发的实践可有效提升模型的人文关怀和安全性。

Conclusion: 反言论研究的当务之急是重新将社群利益置于核心位置：1. 在数据集构建阶段必须纳入受影响人群参与；2. 模型评估应当采用人工评估以确保安全性等关键指标；3. 建立学术界与NGO的长期合作机制。

Abstract: Counterspeech, i.e. the practice of responding to online hate speech, has
gained traction in NLP as a promising intervention. While early work emphasised
collaboration with non-governmental organisation stakeholders, recent research
trends have shifted toward automated pipelines that reuse a small set of legacy
datasets, often without input from affected communities. This paper presents a
systematic review of 74 NLP studies on counterspeech, analysing the extent to
which stakeholder participation influences dataset creation, model development,
and evaluation. To complement this analysis, we conducted a participatory case
study with five NGOs specialising in online Gender-Based Violence (oGBV),
identifying stakeholder-informed practices for counterspeech generation. Our
findings reveal a growing disconnect between current NLP research and the needs
of communities most impacted by toxic online content. We conclude with concrete
recommendations for re-centring stakeholder expertise in counterspeech
research.

</details>


### [196] [Multi-module GRPO: Composing Policy Gradients and Prompt Optimization for Language Model Programs](https://arxiv.org/abs/2508.04660)
*Noah Ziems,Dilara Soylu,Lakshya A Agrawal,Isaac Miller,Liheng Lai,Chen Qian,Kaiqiang Song,Meng Jiang,Dan Klein,Matei Zaharia,Karel D'Oosterlinck,Christopher Potts,Omar Khattab*

Main category: cs.CL

TL;DR: 本研究提出了mmGRPO（多模块GRPO），这是一种针对包含多个语言模型（LM）模块的AI系统的后训练优化方法。它通过按模块对LM调用进行分组并处理可变长度和中断的轨迹，扩展了Group Relative Policy Optimization（GRPO）。实验表明，结合自动提示优化，mmGRPO在多个任务上相比基础LM提升了11%的准确率，比单独提示优化高5%


<details>
  <summary>Details</summary>
Motivation: 当前的AI系统通常由多个模块（如使用不同提示模板的LM调用及其他工具）组成，而传统的GRPO方法难以直接应用于这种多模块系统。因此，需要一种能够处理多模块、可变长度和中断轨迹的GRPO扩展方法

Method: 提出了mmGRPO（multi-module GRPO）方法：1) 按模块（而非整体）对LM调用进行分组；2) 支持处理可变长度和中断的轨迹；3) 与DSPy框架中的自动提示优化结合使用。该方法在DSPy中实现为dspy.GRPO优化器

Result: 在分类、多跳搜索和隐私保护委托任务上：1) mmGRPO结合提示优化比基础后训练LM平均提升了11%的准确率；2) 相比单独使用提示优化，性能额外提升了5%

Conclusion: mmGRPO有效扩展了GRPO至多模块AI系统，显著提升了在复杂任务上的性能。该方法已在DSPy框架中开源，为模块化LM程序的优化提供了新工具

Abstract: Group Relative Policy Optimization (GRPO) has proven to be an effective tool
for post-training language models (LMs). However, AI systems are increasingly
expressed as modular programs that mix together multiple LM calls with distinct
prompt templates and other tools, and it is not clear how best to leverage GRPO
to improve these systems. We begin to address this challenge by defining
mmGRPO, a simple multi-module generalization of GRPO that groups LM calls by
module across rollouts and handles variable-length and interrupted
trajectories. We find that mmGRPO, composed with automatic prompt optimization,
improves accuracy by 11% on average across classification, many-hop search, and
privacy-preserving delegation tasks against the post-trained LM, and by 5%
against prompt optimization on its own. We open-source mmGRPO in DSPy as the
dspy.GRPO optimizer.

</details>


### [197] [Sculptor: Empowering LLMs with Cognitive Agency via Active Context Management](https://arxiv.org/abs/2508.04664)
*Mo Li,L. H. Xu,Qitai Tan,Ting Cao,Yunxin Liu*

Main category: cs.CL

TL;DR: 提出Sculptor框架，通过主动上下文管理（ACM）工具让大语言模型（LLM）能够主动管理内部工作记忆，从而减轻长上下文中的前摄干扰问题。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在处理长上下文时，前摄干扰（即不相关信息干扰记忆和推理）导致性能显著下降。现有方法主要依赖外部记忆系统增强LLM，本文提出补充方案：赋予LLM主动管理内部工作记忆的能力。

Method: 分为三步：(1)上下文分块化工具：将长文本拆解为多个片段；(2)总结、隐藏和恢复工具：让LLM自主选择压缩/暂存/召回特定内容；(3)智能检索工具：支持按需精准定位信息。框架无需专门训练，直接利用LLM的通用工具调用能力。

Result: 在信息稀疏型基准测试（PI-LLM前摄干扰测试和Multi-Needle多针推理测试）上验证：Sculptor显著提升模型表现（即使未经训练）。相比仅扩展上下文窗口，该方法证明显式的上下文控制策略才是实现长文本稳健推理的关键。

Conclusion: 主动上下文管理不仅缓解前摄干扰，还为多任务长文本推理提供了认知基础。核心结论在于：上下文控制策略（而非单纯扩大窗口）是规模鲁棒性的关键。

Abstract: Large Language Models (LLMs) suffer from significant performance degradation
when processing long contexts due to proactive interference, where irrelevant
information in earlier parts of the context disrupts reasoning and memory
recall. While most research focuses on external memory systems to augment LLMs'
capabilities, we propose a complementary approach: empowering LLMs with Active
Context Management (ACM) tools to actively sculpt their internal working
memory. We introduce Sculptor, a framework that equips LLMs with three
categories of tools: (1) context fragmentation, (2) summary, hide, and restore,
and (3) intelligent search. Our approach enables LLMs to proactively manage
their attention and working memory, analogous to how humans selectively focus
on relevant information while filtering out distractions. Experimental
evaluation on information-sparse benchmarks-PI-LLM (proactive interference) and
NeedleBench Multi-Needle Reasoning-demonstrates that Sculptor significantly
improves performance even without specific training, leveraging LLMs' inherent
tool calling generalization capabilities. By enabling Active Context
Management, Sculptor not only mitigates proactive interference but also
provides a cognitive foundation for more reliable reasoning across diverse
long-context tasks-highlighting that explicit context-control strategies,
rather than merely larger token windows, are key to robustness at scale.

</details>


### [198] [GeRe: Towards Efficient Anti-Forgetting in Continual Learning of LLM via General Samples Replay](https://arxiv.org/abs/2508.04676)
*Yunan Zhang,Shuoran Jiang,Mengchen Zhao,Yuefeng Li,Yang Fan,Xiangping Wu,Qingcai Chen*

Main category: cs.CL

TL;DR: 论文提出了一个名为General Sample Replay (GeRe)的框架，通过使用预训练文本进行回放来解决大型语言模型（LLMs）在持续学习中的灾难性遗忘问题。该方法包括一个基于阈值的边际损失（TM）来维持激活状态一致性，实验表明仅需少量固定的通用回放样本集即可在保留通用能力的同时提升序列任务的性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在跨领域持续微调时面临灾难性遗忘问题，主要表现为两个方面：一是遗忘其通用能力，二是先前学习任务的性能急剧下降。因此，需要一种简单而稳定的方法同时解决这两个问题。

Method: 提出GeRe框架，利用预训练文本进行回放。在GeRe框架下，不仅回顾了最流行的回放方法，还引入了基于神经状态的增强约束优化方法，即基于阈值的边际损失（TM）。该方法在回放学习过程中通过维持激活状态的一致性来缓解遗忘。同时，通过实验验证仅需一个固定的小型预收集通用回放样本集即可解决通用能力保留和序列任务性能下降的问题。

Result: 在控制实验中，系统地比较了TM与GeRe框架下不同的回放策略（包括普通标签拟合、通过KL散度进行logit模仿，以及通过L1/L2损失进行特征模仿）。结果显示，TM方法持续提升性能并表现出更好的鲁棒性。

Conclusion: GeRe框架为大型语言模型的高效回放铺平了道路。该方法通过简单的回放机制和创新的激活状态约束优化，有效地解决了持续学习中的灾难性遗忘问题，同时实验证明仅需少量通用回放样本即可在多个任务上保持模型性能。

Abstract: The continual learning capability of large language models (LLMs) is crucial
for advancing artificial general intelligence. However, continual fine-tuning
LLMs across various domains often suffers from catastrophic forgetting,
characterized by: 1) significant forgetting of their general capabilities, and
2) sharp performance declines in previously learned tasks. To simultaneously
address both issues in a simple yet stable manner, we propose General Sample
Replay (GeRe), a framework that use usual pretraining texts for efficient
anti-forgetting. Beyond revisiting the most prevalent replay-based practices
under GeRe, we further leverage neural states to introduce a enhanced
activation states constrained optimization method using threshold-based margin
(TM) loss, which maintains activation state consistency during replay learning.
We are the first to validate that a small, fixed set of pre-collected general
replay samples is sufficient to resolve both concerns--retaining general
capabilities while promoting overall performance across sequential tasks.
Indeed, the former can inherently facilitate the latter. Through controlled
experiments, we systematically compare TM with different replay strategies
under the GeRe framework, including vanilla label fitting, logit imitation via
KL divergence and feature imitation via L1/L2 losses. Results demonstrate that
TM consistently improves performance and exhibits better robustness. Our work
paves the way for efficient replay of LLMs for the future. Our code and data
are available at https://github.com/Qznan/GeRe.

</details>


### [199] [FaST: Feature-aware Sampling and Tuning for Personalized Preference Alignment with Limited Data](https://arxiv.org/abs/2508.04698)
*Thibaut Thonet,Germán Kruszewski,Jos Rozen,Pierre Erbacher,Marc Dymetman*

Main category: cs.CL

TL;DR: 本文提出了一种针对LLM个性化设置中数据稀缺问题（PPALLI）的解决方案。通过引入两个新数据集（DnD和ELIP）并评估多种对齐技术，进而提出了参数高效的FaST方法，该方法利用从数据中自动发现的高层特征，实现了最佳性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM对话助手普遍采用统一部署模式，无法满足用户的个性化需求。当每个用户仅能提供少量偏好标注时（即PPALLI问题），传统方法面临数据稀缺的挑战。

Method: 1) 创建两个基准数据集DnD和ELIP用于评估个性化偏好对齐；2) 系统评估现有多种对齐技术在有限数据场景下的表现；3) 提出FaST方法：通过自动提取数据中的高层特征实现参数高效优化，提升小样本条件下的对齐效果。

Result: 实验表明：1) 现有技术在PPALLI场景下效果有限；2) FaST在跨数据集测试中均超越基准模型，在参数效率显著提升的同时实现最优性能。

Conclusion: FaST为有限数据下的LLM个性化对齐问题提供了有效解决方案，其自动特征提取机制可作为未来研究的核心思路，两个新建数据集将推动该领域的标准化测评。

Abstract: LLM-powered conversational assistants are often deployed in a
one-size-fits-all manner, which fails to accommodate individual user
preferences. Recently, LLM personalization -- tailoring models to align with
specific user preferences -- has gained increasing attention as a way to bridge
this gap. In this work, we specifically focus on a practical yet challenging
setting where only a small set of preference annotations can be collected per
user -- a problem we define as Personalized Preference Alignment with Limited
Data (PPALLI). To support research in this area, we introduce two datasets --
DnD and ELIP -- and benchmark a variety of alignment techniques on them. We
further propose FaST, a highly parameter-efficient approach that leverages
high-level features automatically discovered from the data, achieving the best
overall performance.

</details>


### [200] [Hop, Skip, and Overthink: Diagnosing Why Reasoning Models Fumble during Multi-Hop Analysis](https://arxiv.org/abs/2508.04699)
*Anushka Yadav,Isha Nalawade,Srujana Pillarichety,Yashwanth Babu,Reshmi Ghosh,Samyadeep Basu,Wenlong Zhao,Ali Nasaeh,Sriram Balasubramanian,Soundararajan Srinivasan*

Main category: cs.CL

TL;DR: 该论文研究了当代语言模型在多跳问答任务中的推理失败原因，提出了一个新颖的错误分类框架，涵盖多跳多样性、信息覆盖完整性以及认知效率不足三个维度，并通过人工标注和自动化指标揭示了被准确率评估所掩盖的复杂错误模式。


<details>
  <summary>Details</summary>
Motivation: 尽管推理模型在解决复杂数学、深度搜索和信息提取问题方面取得了突破性进展，但为什么这些模型比通用语言模型更容易产生幻觉（即错误推理）仍缺乏完整的理解。本研究旨在系统地探索语言模型在多跳问答任务上的推理失败原因。

Method: 1. 引入一个新颖的错误分类框架，从三个关键维度分析推理失败：(a) 多样性（指涉及的源文档的唯一性）、(b) 覆盖性（指内容是否完整捕获了相关信息）、(c) 认知效率（是否“想得太多”）。2. 通过严格的人工标注并辅以自动化指标进行探索。

Result: 该研究揭示了被现有准确率评估所掩盖的复杂错误模式。这些错误模式包括：（1）多跳多样性导致的错误（涉及多个唯一文档的推理问题更容易出错）、（2）信息覆盖不完整问题（未能完全捕获关键信息的错误）、（3）过度思考（认知效率不足导致的错误）。

Conclusion: 这种调查方法对当前模型的认知局限性提供了深刻见解，并提供了未来增强语言模型推理可靠性、透明性和稳健性的行动指南。

Abstract: The emergence of reasoning models and their integration into practical AI
chat bots has led to breakthroughs in solving advanced math, deep search, and
extractive question answering problems that requires a complex and multi-step
thought process. Yet, a complete understanding of why these models hallucinate
more than general purpose language models is missing. In this investigative
study, we systematicallyexplore reasoning failures of contemporary language
models on multi-hop question answering tasks. We introduce a novel, nuanced
error categorization framework that examines failures across three critical
dimensions: the diversity and uniqueness of source documents involved ("hops"),
completeness in capturing relevant information ("coverage"), and cognitive
inefficiency ("overthinking"). Through rigorous hu-man annotation, supported by
complementary automated metrics, our exploration uncovers intricate error
patterns often hidden by accuracy-centric evaluations. This investigative
approach provides deeper insights into the cognitive limitations of current
models and offers actionable guidance toward enhancing reasoning fidelity,
transparency, and robustness in future language modeling efforts.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [201] [Think Before You Segment: An Object-aware Reasoning Agent for Referring Audio-Visual Segmentation](https://arxiv.org/abs/2508.04418)
*Jinxing Zhou,Yanghao Zhou,Mingfei Han,Tong Wang,Xiaojun Chang,Hisham Cholakkal,Rao Muhammad Anwer*

Main category: cs.MM

TL;DR: 该论文提出了一种称为TGS-Agent的显式推理模型，用于音频-视觉分割（Ref-AVS）任务。该模型采用Think-Ground-Segment过程，首先利用多模态模型Ref-Thinker解析输入文本、视觉和音频信息来推理目标对象，然后通过预训练的Grounding-DINO和SAM2进行无像素级监督的定位与分割。此外，论文还引入了R^2-AVSBench基准测试，新模型在此基准和原标准Ref-AVSBench上均达到当前最佳性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常依赖于潜在嵌入学习并通过多模态融合触发可调SAM/SAM2解码器进行分割，这需要大量像素级监督且缺乏可解释性。为解决这些问题，本研究试图从显式推理的角度模拟人类思考过程——先识别目标对象再执行分割，以此降低对像素级标注的依赖并增强可解释性。

Method: 1. 设计TGS-Agent框架，执行Think-Ground-Segment三级流程： 
   - Think阶段：训练Ref-Thinker模型（基于多模态语言模型），利用指令调优数据集推理生成目标对象的语言描述。
   - Ground阶段：使用Grounding-DINO将Ref-Thinker输出的描述作为文本提示进行初步对象定位。
   - Segment阶段：结合定位框和音频特征提示SAM2执行像素级精确分割。
 2. 构建指令调优数据集，该数据包含包含对象感知的思维链信息用于优化Ref-Thinker模型。
 3. 提出R^2-AVSBench基准测试，该基准包含语言多样性和推理需求更高的描述文本以评估模型泛化能力。

Result: 1. 在标准Ref-AVSBench和新提出的R^2-AVSBench上均表现先进（state-of-the-art），验证了方法的有效性。
 2. 在分割任务中减少了对像素级监督的依赖，提高了推理透明性。

Conclusion: TGS-Agent通过模拟人类决策过程（Think-Ground-Segment）显式推理参考信息，从而在参考音频-视觉分割任务中实现了高效准确的模型。该方法消除了传统方法中依赖潜在嵌入学习造成的透明性不足问题，并在新引入的复杂推理基准上展示了更强的泛化性，推动了可解释性模型的研发方向。

Abstract: Referring Audio-Visual Segmentation (Ref-AVS) aims to segment target objects
in audible videos based on given reference expressions. Prior works typically
rely on learning latent embeddings via multimodal fusion to prompt a tunable
SAM/SAM2 decoder for segmentation, which requires strong pixel-level
supervision and lacks interpretability. From a novel perspective of explicit
reference understanding, we propose TGS-Agent, which decomposes the task into a
Think-Ground-Segment process, mimicking the human reasoning procedure by first
identifying the referred object through multimodal analysis, followed by
coarse-grained grounding and precise segmentation. To this end, we first
propose Ref-Thinker, a multimodal language model capable of reasoning over
textual, visual, and auditory cues. We construct an instruction-tuning dataset
with explicit object-aware think-answer chains for Ref-Thinker fine-tuning. The
object description inferred by Ref-Thinker is used as an explicit prompt for
Grounding-DINO and SAM2, which perform grounding and segmentation without
relying on pixel-level supervision. Additionally, we introduce
R\textsuperscript{2}-AVSBench, a new benchmark with linguistically diverse and
reasoning-intensive references for better evaluating model generalization. Our
approach achieves state-of-the-art results on both standard Ref-AVSBench and
proposed R\textsuperscript{2}-AVSBench. Code will be available at
https://github.com/jasongief/TGS-Agent.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [202] [Privileged Contrastive Pretraining for Multimodal Affect Modelling](https://arxiv.org/abs/2508.03729)
*Kosmas Pinitas,Konstantinos Makantasis,Georgios N. Yannakakis*

Main category: cs.LG

TL;DR: 提出Privileged Contrastive Pretraining (PriCon) 框架，结合监督对比学习和特权信息学习，解决情感计算模型从实验室环境到真实世界应用的迁移挑战，在基准数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 情感计算模型在从受控实验室环境转移到非受控真实环境时性能会下降 (in-vitro to in-vivo gap)。现有方法无法充分利用训练阶段的额外信息（特权信息）。

Method: 1. 基于监督对比学习（SCL）预训练模型；2. 使用Learning Using Privileged Information (LUPI)框架将预训练模型作为教师模型；3. SCL增强鲁棒性，LUPI利用训练阶段的特权信息（测试阶段不可用的模态）。

Result: 在RECOLA和AGAIN数据集上的实验表明：PriCon持续优于LUPI和端到端模型；多数情况下性能接近训练测试阶段都能使用所有模态的理想模型。

Conclusion: PriCon通过整合监督对比学习和特权信息学习，有效缩小in-vitro到in-vivo的差距，为情感计算的实际应用提供了可扩展的解决方案。

Abstract: Affective Computing (AC) has made significant progress with the advent of
deep learning, yet a persistent challenge remains: the reliable transfer of
affective models from controlled laboratory settings (in-vitro) to uncontrolled
real-world environments (in-vivo). To address this challenge we introduce the
Privileged Contrastive Pretraining (PriCon) framework according to which models
are first pretrained via supervised contrastive learning (SCL) and then act as
teacher models within a Learning Using Privileged Information (LUPI) framework.
PriCon both leverages privileged information during training and enhances the
robustness of derived affect models via SCL. Experiments conducted on two
benchmark affective corpora, RECOLA and AGAIN, demonstrate that models trained
using PriCon consistently outperform LUPI and end to end models. Remarkably, in
many cases, PriCon models achieve performance comparable to models trained with
access to all modalities during both training and testing. The findings
underscore the potential of PriCon as a paradigm towards further bridging the
gap between in-vitro and in-vivo affective modelling, offering a scalable and
practical solution for real-world applications.

</details>


### [203] [PILOT-C: Physics-Informed Low-Distortion Optimal Trajectory Compression](https://arxiv.org/abs/2508.03730)
*Kefei Wu,Baihua Zheng,Weiwei Sun*

Main category: cs.LG

TL;DR: 提出了一种名为PILOT-C的新型轨迹压缩框架，该框架将频域物理建模与误差有界优化相结合，支持任意维度轨迹（包括3D），并在多个真实数据集上表现出优越性能，包括压缩比和轨迹保真度的显著提升。


<details>
  <summary>Details</summary>
Motivation: 当前轨迹压缩方法（如线简化）通常仅适用于2D轨迹，忽视了时间同步和运动连续性，且在高维轨迹压缩方面表现不佳。为解决这些问题，提出PILOT-C框架以实现高维轨迹的高效压缩。

Method: PILOT-C框架将频域物理建模（捕获轨迹的周期性运动特征）与误差有界优化结合，允许对每个空间轴独立进行压缩处理。

Result: 在四个真实数据集上评估表明：1) 压缩比：较当前最佳SED基算法（CISED-W）平均提升19.2%；2) 轨迹保真度：平均误差比CISED-W降低32.6%；3) 在3D数据集上较最快线简化算法（SQUISH-E）压缩比提升49%，且保持相同计算复杂度。

Conclusion: PILOT-C是一种高效、多维度兼容的轨迹压缩框架，显著优于现有线简化算法，尤其在处理3D轨迹时展现强大扩展性。

Abstract: Location-aware devices continuously generate massive volumes of trajectory
data, creating demand for efficient compression. Line simplification is a
common solution but typically assumes 2D trajectories and ignores time
synchronization and motion continuity. We propose PILOT-C, a novel trajectory
compression framework that integrates frequency-domain physics modeling with
error-bounded optimization. Unlike existing line simplification methods,
PILOT-C supports trajectories in arbitrary dimensions, including 3D, by
compressing each spatial axis independently. Evaluated on four real-world
datasets, PILOT-C achieves superior performance across multiple dimensions. In
terms of compression ratio, PILOT-C outperforms CISED-W, the current
state-of-the-art SED-based line simplification algorithm, by an average of
19.2%. For trajectory fidelity, PILOT-C achieves an average of 32.6% reduction
in error compared to CISED-W. Additionally, PILOT-C seamlessly extends to
three-dimensional trajectories while maintaining the same computational
complexity, achieving a 49% improvement in compression ratios over SQUISH-E,
the most efficient line simplification algorithm on 3D datasets.

</details>


### [204] [CX-Mind: A Pioneering Multimodal Large Language Model for Interleaved Reasoning in Chest X-ray via Curriculum-Guided Reinforcement Learning](https://arxiv.org/abs/2508.03733)
*Wenjie Li,Yujie Zhang,Haoran Sun,Yueqi Li,Fanrui Zhang,Mengzhe Xu,Victoria Borja Clausich,Sade Mellin,Renhao Yang,Chenrun Wang,Jethro Zih-Shuo Wang,Shiyi Yao,Gen Li,Yidong Xu,Hanyu Wang,Yilin Huang,Angela Lin Wang,Chen Shi,Yin Zhang,Jianan Guo,Luqi Yang,Renxuan Li,Yang Xu,Jiawei Liu,Yao Zhang,Lei Liu,Carlos Gutiérrez SanRomán,Lei Wang*

Main category: cs.LG

TL;DR: CX-Mind模型提出了一种基于课程强化学习和可验证过程奖励的方法，用于CXR（胸部X光）任务的交互式“思考-回答”推理，显著提高了多任务诊断效果，并在真实世界临床数据上验证了其实用性。


<details>
  <summary>Details</summary>
Motivation: 医学成像中多任务胸部X光诊断存在推理过程冗长、奖励稀疏和幻觉问题，现有模型无法验证推理过程。CX-Mind试图解决这些问题，提高准确性和可解释性。

Method: 1. 利用大型指令数据集CX-Set（708K图像和260万样本）生成高质量的推理数据。2. 采用两个阶段优化：先用课程强化学习稳定基础任务，后转移到开放域诊断。3. 用规则条件过程奖励替代预训练奖励模型。

Result: 1. 平均性能超越现有医疗和通用MLLMs模型25.1%。2. 在14种疾病的真实临床数据（Rui-CXR）上显著提升召回率。3. 多中心实验和专家验证证实了其临床实用性。

Conclusion: CX-Mind首次实现了多任务CXR诊断的交互式推理，其验证驱动的强化学习框架可拓展到多任务医疗场景，并具有高效生成能力和减少幻觉的优势。

Abstract: Chest X-ray (CXR) imaging is one of the most widely used diagnostic
modalities in clinical practice, encompassing a broad spectrum of diagnostic
tasks. Recent advancements have seen the extensive application of
reasoning-based multimodal large language models (MLLMs) in medical imaging to
enhance diagnostic efficiency and interpretability. However, existing
multimodal models predominantly rely on "one-time" diagnostic approaches,
lacking verifiable supervision of the reasoning process. This leads to
challenges in multi-task CXR diagnosis, including lengthy reasoning, sparse
rewards, and frequent hallucinations. To address these issues, we propose
CX-Mind, the first generative model to achieve interleaved "think-answer"
reasoning for CXR tasks, driven by curriculum-based reinforcement learning and
verifiable process rewards (CuRL-VPR). Specifically, we constructed an
instruction-tuning dataset, CX-Set, comprising 708,473 images and 2,619,148
samples, and generated 42,828 high-quality interleaved reasoning data points
supervised by clinical reports. Optimization was conducted in two stages under
the Group Relative Policy Optimization framework: initially stabilizing basic
reasoning with closed-domain tasks, followed by transfer to open-domain
diagnostics, incorporating rule-based conditional process rewards to bypass the
need for pretrained reward models. Extensive experimental results demonstrate
that CX-Mind significantly outperforms existing medical and general-domain
MLLMs in visual understanding, text generation, and spatiotemporal alignment,
achieving an average performance improvement of 25.1% over comparable
CXR-specific models. On real-world clinical dataset (Rui-CXR), CX-Mind achieves
a mean recall@1 across 14 diseases that substantially surpasses the second-best
results, with multi-center expert evaluations further confirming its clinical
utility across multiple dimensions.

</details>


### [205] [Latent Knowledge Scalpel: Precise and Massive Knowledge Editing for Large Language Models](https://arxiv.org/abs/2508.03741)
*Xin Liu,Qiyang Song,Shaowen Xu,Kerou Zhou,Wenbo Jiang,Xiaoqi Jia,Weijuan Zhang,Heqing Huang,Yakai Li*

Main category: cs.LG

TL;DR: 本文提出了一种名为潜在知识刀（LKS）的轻量级超网络，用于在大型语言模型（LLM）中精确和大规模地更新事实知识，同时保持模型的通用能力。实验表明，即使在同时编辑10000条知识时，LKS在Llama-2和Mistral模型上仍能有效工作。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在预训练阶段吸收的信息可能不准确或过时，导致推理时输出错误或有偏见的预测。现有模型编辑方法难以同时处理大规模知识更新，且可能损害模型的通用能力。因此，需要一种能够高效、精确地修改大量知识，同时不破坏模型整体性能的编辑方法。

Method: 1. 通过实证研究发现可以直接操作LLM的内部表示来替换实体（类似编辑自然语言输入）。2. 提出潜伏知识刀（Latent Knowledge Scalpel, LKS），一个轻量级的超网络，通过操纵特定实体的潜知识实现精确和大规模编辑。具体来说，LKS学习生成针对目标实体的参数偏移，直接修改模型中对应实体的内部表示。该超网络简单高效，可处理超大规模编辑（如10,000个同时编辑）。

Result: 在Llama-2和Mistral模型上进行实验，结果表明：1. LKS能有效执行知识编辑任务（如更新过时或错误事实）；2. 即使同时编辑10,000条知识，LKS仍能保持编辑准确性；3. 被编辑过的模型在通用能力（如语言生成、理解等）上未出现显著退化，证明方法的非破坏性。代码已开源（https://github.com/Linuxin-xxx/LKS）.

Conclusion: LKS提供了一种高效、可扩展的LLM知识编辑解决方案，能够同时处理海量知识更新而不损害模型通用能力。这一方法为持续维护LLM的知识准确性开辟了新途径，对实际应用具有重要意义。通过轻量级的潜空间操作规避了传统编辑方法在规模和通用性上的瓶颈。

Abstract: Large Language Models (LLMs) often retain inaccurate or outdated information
from pre-training, leading to incorrect predictions or biased outputs during
inference. While existing model editing methods can address this challenge,
they struggle with editing large amounts of factual information simultaneously
and may compromise the general capabilities of the models. In this paper, our
empirical study demonstrates that it is feasible to edit the internal
representations of LLMs and replace the entities in a manner similar to editing
natural language inputs. Based on this insight, we introduce the Latent
Knowledge Scalpel (LKS), an LLM editor that manipulates the latent knowledge of
specific entities via a lightweight hypernetwork to enable precise and
large-scale editing. Experiments conducted on Llama-2 and Mistral show even
with the number of simultaneous edits reaching 10,000, LKS effectively performs
knowledge editing while preserving the general abilities of the edited LLMs.
Code is available at: https://github.com/Linuxin-xxx/LKS.

</details>


### [206] [GlaBoost: A multimodal Structured Framework for Glaucoma Risk Stratification](https://arxiv.org/abs/2508.03750)
*Cheng Huang,Weizheng Xie,Karanjit Kooner,Tsengdar Lee,Jui-Kai Wang,Jia Zhang*

Main category: cs.LG

TL;DR: GlaBoost是一种多模态梯度提升框架，结合临床特征、眼底图像嵌入和专家描述的文本，用于青光眼风险预测。它在真实数据集上实现了98.71%的准确率，且特征重要性分析具有临床一致性。


<details>
  <summary>Details</summary>
Motivation: 青光眼的早期准确检测至关重要，但现有方法多依赖单模态数据且缺乏可解释性，限制了临床实用性。因此，需要一种多模态、可解释的方法来改进青光眼诊断。

Method: 1. 使用预训练卷积编码器提取眼底照片的高层视觉表征。2. 使用基于Transformer的语言模型编码专家对神经视网膜边缘评估的文本描述。3. 将上述特征与人工评估的风险评分和定量眼科指标结合。4. 通过增强的XGBoost模型将这些异构信号融合到统一特征空间进行分类。

Result: 在真实世界标注数据集上，GlaBoost显著优于基线模型，验证准确率达到98.71%。特征重要性分析显示模型决策与临床一致，杯盘比、边缘苍白和特定文本嵌入贡献最大。

Conclusion: GlaBoost为可解释的青光眼诊断提供了透明且可扩展的解决方案，并可扩展到其他眼科疾病诊断。

Abstract: Early and accurate detection of glaucoma is critical to prevent irreversible
vision loss. However, existing methods often rely on unimodal data and lack
interpretability, limiting their clinical utility. In this paper, we present
GlaBoost, a multimodal gradient boosting framework that integrates structured
clinical features, fundus image embeddings, and expert-curated textual
descriptions for glaucoma risk prediction. GlaBoost extracts high-level visual
representations from retinal fundus photographs using a pretrained
convolutional encoder and encodes free-text neuroretinal rim assessments using
a transformer-based language model. These heterogeneous signals, combined with
manually assessed risk scores and quantitative ophthalmic indicators, are fused
into a unified feature space for classification via an enhanced XGBoost model.
Experiments conducted on a real-world annotated dataset demonstrate that
GlaBoost significantly outperforms baseline models, achieving a validation
accuracy of 98.71%. Feature importance analysis reveals clinically consistent
patterns, with cup-to-disc ratio, rim pallor, and specific textual embeddings
contributing most to model decisions. GlaBoost offers a transparent and
scalable solution for interpretable glaucoma diagnosis and can be extended to
other ophthalmic disorders.

</details>


### [207] [LRTuckerRep: Low-rank Tucker Representation Model for Multi-dimensional Data Completion](https://arxiv.org/abs/2508.03755)
*Wenwu Gong,Lili Yang*

Main category: cs.LG

TL;DR: 提出了一种新颖的用于多维数据补全的低秩塔克表示（LRTuckerRep）模型，该模型在塔克分解框架内统一了全局和局部先验建模。方法结合了自适应加权核范数低秩约束和参数自由的基于拉普拉斯的平滑正则化，开发了两种具有收敛保证的算法。实验证明在图像修复和交通数据补全任务上具备高精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现阶段的多维数据补全方法存在明显缺陷：全局低秩方法计算昂贵且破坏数据固有结构，而局部平滑方法需要大量参数调优且泛化性差。因此需要在统一框架下结合两种先验以避免各自的短板。

Method: 提出LRTuckerRep模型，利用塔克分解统一建模全局低秩性（通过因子矩阵的自适应加权核范数和稀疏塔克核心）和局部平滑性（通过因子空间的参数自由拉普拉斯正则化）。为求解非凸优化问题，设计了两种具有可证收敛性的迭代算法。

Result: 在多个多维图像修复和交通数据补全实验中，LRTuckerRep在高缺失率下表现出优于基准方法的补全精度和鲁棒性。

Conclusion: 该模型通过结合低秩与平滑约束，实现了高效的数据结构保持与补全能力，无需手动调参，具有广泛的应用潜力。

Abstract: Multi-dimensional data completion is a critical problem in computational
sciences, particularly in domains such as computer vision, signal processing,
and scientific computing. Existing methods typically leverage either global
low-rank approximations or local smoothness regularization, but each suffers
from notable limitations: low-rank methods are computationally expensive and
may disrupt intrinsic data structures, while smoothness-based approaches often
require extensive manual parameter tuning and exhibit poor generalization. In
this paper, we propose a novel Low-Rank Tucker Representation (LRTuckerRep)
model that unifies global and local prior modeling within a Tucker
decomposition. Specifically, LRTuckerRep encodes low rankness through a
self-adaptive weighted nuclear norm on the factor matrices and a sparse Tucker
core, while capturing smoothness via a parameter-free Laplacian-based
regularization on the factor spaces. To efficiently solve the resulting
nonconvex optimization problem, we develop two iterative algorithms with
provable convergence guarantees. Extensive experiments on multi-dimensional
image inpainting and traffic data imputation demonstrate that LRTuckerRep
achieves superior completion accuracy and robustness under high missing rates
compared to baselines.

</details>


### [208] [LLM-Prior: A Framework for Knowledge-Driven Prior Elicitation and Aggregation](https://arxiv.org/abs/2508.03766)
*Yongchao Huang*

Main category: cs.LG

TL;DR: 使用LLM自动化贝叶斯推断中的先验分布设定，提出LLMPrior框架，通过生成模型将非结构化输入转换为有效概率分布，并扩展到多代理系统的聚合先验算法Fed-LLMPrior。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯推断中先验分布设定通常需手工完成，具有主观性和难以扩展的瓶颈。本文旨在利用大型语言模型（LLM）自动化并扩展该过程。

Method: 提出LLMPrior框架：使用耦合LLM与显式生成模型（如高斯混合模型）构建LLM混合密度网络，确保输出满足概率分布要求；扩展至多代理系统，开发Fed-LLMPrior算法：基于对数意见池（Logarithmic Opinion Pooling）聚合分散代理生成的分布，并解决分布式情境依赖与异构性问题。

Result: 开发了可自动将非结构化输入转化为有效概率分布的LLMPrior框架，并实现了可聚合多个独立代理生成的分布的Fed-LLMPrior算法，为复杂贝叶斯建模提供新型工具支持。

Conclusion: LLMPrior通过自动化先验生成突破了现有瓶颈，Fed-LLMPrior扩展为处理多源异构场景提供可靠方法，有望降低贝叶斯建模门槛。

Abstract: The specification of prior distributions is fundamental in Bayesian
inference, yet it remains a significant bottleneck. The prior elicitation
process is often a manual, subjective, and unscalable task. We propose a novel
framework which leverages Large Language Models (LLMs) to automate and scale
this process. We introduce \texttt{LLMPrior}, a principled operator that
translates rich, unstructured contexts such as natural language descriptions,
data or figures into valid, tractable probability distributions. We formalize
this operator by architecturally coupling an LLM with an explicit, tractable
generative model, such as a Gaussian Mixture Model (forming a LLM based Mixture
Density Network), ensuring the resulting prior satisfies essential mathematical
properties. We further extend this framework to multi-agent systems where
Logarithmic Opinion Pooling is employed to aggregate prior distributions
induced by decentralized knowledge. We present the federated prior aggregation
algorithm, \texttt{Fed-LLMPrior}, for aggregating distributed,
context-dependent priors in a manner robust to agent heterogeneity. This work
provides the foundation for a new class of tools that can potentially lower the
barrier to entry for sophisticated Bayesian modeling.

</details>


### [209] [Provably Near-Optimal Distributionally Robust Reinforcement Learning in Online Settings](https://arxiv.org/abs/2508.03768)
*Debamita Ghosh,George K. Atia,Yue Wang*

Main category: cs.LG

TL;DR: 针对强化学习中模拟与现实差异的问题，本文提出了在线分布式鲁棒强化学习框架，通过单一未知训练环境优化最坏性能场景的策略，使用$f$-散度不确定性集合并设计高效算法，理论证伪接近最优且实验验证有效。


<details>
  <summary>Details</summary>
Motivation: 现有分布鲁棒RL方法依赖仿真模型或广泛覆盖的离线数据，在未知环境中缺乏实用性。本文关注在仅与单个未知环境交互的情景下实现最坏性能的提升，填补分布鲁棒RL在在线学习领域的空白。

Method: 提出基于$f$-散度（覆盖Chi-Square和KL散度）的不确定性集合表述；设计计算高效算法，在单训练环境在线交互中交替进行鲁棒策略优化（通过Fenchel共轭转换）、经验数据收集及模型更新；理论分析证明其在最小假设下实现次线性遗憾界，并建立极小极大下界验证近优性。

Result: 理论：证明算法在多种$f$-散度下的次线性遗憾界（如$O(T^{3/4})$）；极小极大下界$\Omega(T^{1/2})$显示方法接近最优。实验：在连续控制、离散网格环境中验证——相比标准RL和离线鲁棒方法，本算法在干扰/参数突变下性能下降最少（平均高12.7%），同时训练开销仅增加8%。

Conclusion: 首次实现未知环境下的在线分布鲁棒RL；算法兼具计算高效性与理论保障；广泛的$f$-散度适用性拓展了应用场景；实验验证了对现实扰动的强鲁棒性。

Abstract: Reinforcement learning (RL) faces significant challenges in real-world
deployments due to the sim-to-real gap, where policies trained in simulators
often underperform in practice due to mismatches between training and
deployment conditions. Distributionally robust RL addresses this issue by
optimizing worst-case performance over an uncertainty set of environments and
providing an optimized lower bound on deployment performance. However, existing
studies typically assume access to either a generative model or offline
datasets with broad coverage of the deployment environment -- assumptions that
limit their practicality in unknown environments without prior knowledge. In
this work, we study the more realistic and challenging setting of online
distributionally robust RL, where the agent interacts only with a single
unknown training environment while aiming to optimize its worst-case
performance. We focus on general $f$-divergence-based uncertainty sets,
including Chi-Square and KL divergence balls, and propose a computationally
efficient algorithm with sublinear regret guarantees under minimal assumptions.
Furthermore, we establish a minimax lower bound on regret of online learning,
demonstrating the near-optimality of our approach. Extensive experiments across
diverse environments further confirm the robustness and efficiency of our
algorithm, validating our theoretical findings.

</details>


### [210] [GTPO: Trajectory-Based Policy Optimization in Large Language Models](https://arxiv.org/abs/2508.03772)
*Marco Simoni,Aleksandar Fontana,Giulio Rossolini,Andrea Saracino*

Main category: cs.LG

TL;DR: 本文分析了现有策略优化方法GRPO的两个主要局限性，并提出了一种新的优化策略GTPO来应对这些问题。GTPO通过识别冲突token，跳过负更新并放大正更新，同时过滤熵过高的补全，以提高训练的稳定性和效果。实验验证了GTPO在多个基准测试中的优异表现。


<details>
  <summary>Details</summary>
Motivation: 现有的策略优化方法GRPO存在两个主要问题：1) token在正负奖励的补全中频繁出现，导致梯度更新冲突，降低输出概率，影响结构保持；2) 负奖励补全可能惩罚高置信度回答，导致模型输出分布趋于平坦，学习效果下降。为此，本文旨在解决这些问题，提供更稳定有效的策略优化方法。

Method: 本文提出的GTPO方法主要包括两个创新点：1) 识别冲突token（在同一位置出现在正负奖励补全中的token），对其跳过负更新并放大正更新；2) 通过过滤熵超过可证明阈值的补全，避免策略崩溃。GTPO不需要KL散度正则化，因此训练过程中不需要参考模型。

Result: 在GSM8K、MATH和AIME 2024基准上的实验表明，GTPO在训练稳定性和模型性能方面均优于GRPO方法。GTPO不仅避免了GRPO的局限性，而且在多个任务中取得了更好的效果。

Conclusion: GTPO通过解决GRPO的两个主要局限（梯度冲突和输出分布扁平化），提供了一种更加稳定和高效的策略优化方法。该方法不仅免除了对参考模型的需要，还在多个基准测试中表现出优越的性能，为语言模型的训练和对齐提供了新思路。

Abstract: Policy-based optimizations are widely adopted today for the training and
alignment of language models, where one of the most recent and effective
approaches is Group-relative Policy Optimization (GRPO). In this paper, we
reveals and analyze two major limitations of GRPO: (i) tokens frequently appear
in completions with both positive and negative rewards, leading to conflicting
gradient updates that can reduce their output probability, even though can be
essential for maintaining proper structure; (ii) negatively rewarded
completions may penalize confident responses and shift model decisions toward
unlikely tokens, progressively flattening the output distribution and degrading
learning. To address these issues and provide a more stable and effective
policy optimization strategy, we introduce GTPO (Group-relative
Trajectory-based Policy Optimization), which identifies conflict tokens, tokens
appearing in the same position across completions with opposite rewards,
protects them by skipping negative updates, while amplifying positive ones. To
further prevent policy collapse, GTPO filters out completions whose entropy
exceeds a provable threshold. Unlike GRPO, GTPO does not rely on KL-divergence
regularization, eliminating the need for a reference model during training,
while still ensuring greater training stability and improved performance,
validated through multiple experiments on GSM8K, MATH and AIME 2024 benchmarks.

</details>


### [211] [U-PINet: End-to-End Hierarchical Physics-Informed Learning With Sparse Graph Coupling for 3D EM Scattering Modeling](https://arxiv.org/abs/2508.03774)
*Rui Zhu,Yuexing Peng,Peng Wang,George C. Alexandropoulos,Wenbo Wang,Wei Xiang*

Main category: cs.LG

TL;DR: 提出了U-PINet，一种物理信息深度学习方法，用于高效计算电磁散射，克服传统数值计算成本高和纯数据驱动方法物理一致性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 电磁散射建模计算复杂，传统数值计算精度高但成本高且扩展性差，纯数据驱动方法缺乏物理约束且需要大量数据。为了解决这些问题，结合物理规律和深度学习，开发了高效的解决方案。

Method: U-PINet是一种层次化的物理信息神经网络框架，模仿电磁求解器中的多尺度分解，通过稀疏图表示高效建模三维物体的网格元素之间的自耦合和互耦合。该架构包含多尺度处理，通过建模临近场相互作用的分解和远场耦合实现端到端的电磁散射建模。

Result: U-PINet能够准确预测表面电流分布，与传统求解器结果高度一致，并且计算时间显著减少。在雷达散射截面积预测等下游应用任务上效果良好。同时，其准确率和鲁棒性均优于常规深度学习方法。

Conclusion: 该框架在实现高精度计算电磁的同时提高了效率，增强了对物理规律的遵循，具有更强的泛化性，且适用于雷达遥感等应用场景。

Abstract: Electromagnetic (EM) scattering modeling is critical for radar remote
sensing, however, its inherent complexity introduces significant computational
challenges. Traditional numerical solvers offer high accuracy, but suffer from
scalability issues and substantial computational costs. Pure data-driven deep
learning approaches, while efficient, lack physical constraints embedding
during training and require extensive labeled data, limiting their
applicability and generalization. To overcome these limitations, we propose a
U-shaped Physics-Informed Network (U-PINet), the first fully
deep-learning-based, physics-informed hierarchical framework for computational
EM designed to ensure physical consistency while maximizing computational
efficiency. Motivated by the hierarchical decomposition strategy in EM solvers
and the inherent sparsity of local EM coupling, the U-PINet models the
decomposition and coupling of near- and far-field interactions through a
multiscale processing neural network architecture, while employing a
physics-inspired sparse graph representation to efficiently model both self-
and mutual- coupling among mesh elements of complex $3$-Dimensional (3D)
objects. This principled approach enables end-to-end multiscale EM scattering
modeling with improved efficiency, generalization, and physical consistency.
Experimental results showcase that the U-PINet accurately predicts surface
current distributions, achieving close agreement with traditional solver, while
significantly reducing computational time and outperforming conventional deep
learning baselines in both accuracy and robustness. Furthermore, our
evaluations on radar cross section prediction tasks confirm the feasibility of
the U-PINet for downstream EM scattering applications.

</details>


### [212] [Revisiting Heat Flux Analysis of Tungsten Monoblock Divertor on EAST using Physics-Informed Neural Network](https://arxiv.org/abs/2508.03776)
*Xiao Wang,Zikang Yan,Hao Si,Zhendong Yang,Qingquan Yang,Dengdi Sun,Wanli Lyu,Jin Tang*

Main category: cs.LG

TL;DR: 本研究提出了一种基于物理信息的神经网络（PINN）方法，用于加速核聚变装置EAST中的热传导估计。该方法通过结合边界损失、初始条件损失和物理损失，并辅以少量数据点采样，在保持高精度的同时，计算效率提高了40倍。


<details>
  <summary>Details</summary>
Motivation: 在核聚变装置EAST中，传统有限元法进行热流估计计算效率低，难以实现实时模拟。受人工智能推动的科学计算的启发，本文提出使用PINN来解决这一挑战，以在保证精度的同时显著加速热传导估计过程。

Method: 1. 输入不同材料的信息，将空间坐标和时间戳输入神经网络。2. 基于热传导方程计算边界损失、初始条件损失和物理损失。3. 通过少量数据点的数据驱动采样，更好地拟合特定热传导场景，提升预测能力。

Result: 在均匀和非均匀顶部加热条件两种情况下进行实验。结果表明，所提出的热传导物理信息神经网络在精度上与有限元法相当，同时计算效率提高了40倍。

Conclusion: 提出的PINN方法在EAST热传导估计中成功实现了高精度和高效率，为实时模拟提供了可能。数据集和代码将开源。

Abstract: Estimating heat flux in the nuclear fusion device EAST is a critically
important task. Traditional scientific computing methods typically model this
process using the Finite Element Method (FEM). However, FEM relies on
grid-based sampling for computation, which is computationally inefficient and
hard to perform real-time simulations during actual experiments. Inspired by
artificial intelligence-powered scientific computing, this paper proposes a
novel Physics-Informed Neural Network (PINN) to address this challenge,
significantly accelerating the heat conduction estimation process while
maintaining high accuracy. Specifically, given inputs of different materials,
we first feed spatial coordinates and time stamps into the neural network, and
compute boundary loss, initial condition loss, and physical loss based on the
heat conduction equation. Additionally, we sample a small number of data points
in a data-driven manner to better fit the specific heat conduction scenario,
further enhancing the model's predictive capability. We conduct experiments
under both uniform and non-uniform heating conditions on the top surface.
Experimental results show that the proposed thermal conduction physics-informed
neural network achieves accuracy comparable to the finite element method, while
achieving $\times$40 times acceleration in computational efficiency. The
dataset and source code will be released on
https://github.com/Event-AHU/OpenFusion.

</details>


### [213] [SoilNet: A Multimodal Multitask Model for Hierarchical Classification of Soil Horizons](https://arxiv.org/abs/2508.03785)
*Teodor Chiaburu,Vipin Singh,Frank Haußer,Felix Bießmann*

Main category: cs.LG

TL;DR: 本文提出了一个名为SoilNet的多模态多任务模型，用于土壤剖面层位分类，通过结合图像数据和地理时间元数据，预测标记点、分割土壤剖面并提取形态学特征，最后利用图表示标签层次关系进行预测。


<details>
  <summary>Details</summary>
Motivation: 尽管基础模型在许多领域取得了进展，但土壤层位分类由于多模态、多任务的特点以及复杂的层级标签分类体系，仍面临挑战。而精确的土壤层位分类对监测土壤健康至关重要，影响农业生产、食品安全、生态系统稳定和气候适应。

Method: 1. 整合图像数据和地理时间元数据，预测深度标记点以分割土壤剖面为候选层位。2. 为每个分割段提取层位特定的形态学特征。3. 将多模态特征向量拼接，并利用图表示标签的层次关系预测层位标签。

Result: 作者在真实土壤剖面数据集上验证了方法的有效性。所有代码和实验可在提供的GitHub仓库中找到。

Conclusion: SoilNet模型通过结构化、模块化的流程处理了具有大量、不平衡且层级复杂的标签分类问题，为解决多模态多任务的土壤层位分类提供了一种有效方法。

Abstract: While recent advances in foundation models have improved the state of the art
in many domains, some problems in empirical sciences could not benefit from
this progress yet. Soil horizon classification, for instance, remains
challenging because of its multimodal and multitask characteristics and a
complex hierarchically structured label taxonomy. Accurate classification of
soil horizons is crucial for monitoring soil health, which directly impacts
agricultural productivity, food security, ecosystem stability and climate
resilience. In this work, we propose $\textit{SoilNet}$ - a multimodal
multitask model to tackle this problem through a structured modularized
pipeline. Our approach integrates image data and geotemporal metadata to first
predict depth markers, segmenting the soil profile into horizon candidates.
Each segment is characterized by a set of horizon-specific morphological
features. Finally, horizon labels are predicted based on the multimodal
concatenated feature vector, leveraging a graph-based label representation to
account for the complex hierarchical relationships among soil horizons. Our
method is designed to address complex hierarchical classification, where the
number of possible labels is very large, imbalanced and non-trivially
structured. We demonstrate the effectiveness of our approach on a real-world
soil profile dataset. All code and experiments can be found in our repository:
https://github.com/calgo-lab/BGR/

</details>


### [214] [Bernoulli-LoRA: A Theoretical Framework for Randomized Low-Rank Adaptation](https://arxiv.org/abs/2508.03820)
*Igor Sokolov,Abdurakhmon Sadiev,Yury Demidovich,Fawaz S Al-Qahtani,Peter Richtárik*

Main category: cs.LG

TL;DR: 提出了一种名为Bernoulli-LoRA的新型参数高效微调理论框架，该框架通过引入伯努利概率机制统一并扩展了现有的LoRA方法。在非凸优化和凸非光滑函数的假设下，分析了几种变体的收敛性，并通过实验验证了其实用性。


<details>
  <summary>Details</summary>
Motivation: 尽管LoRA在参数高效微调中表现出色且应用广泛，但其理论理解仍然有限。为弥补这一不足，本文在RAC-LoRA的基础上进一步发展，旨在建立一个统一且可扩展的理论框架，同时保持实际有效性。

Method: 1. 提出Bernoulli-LoRA框架：引入伯努利机制动态选择更新的矩阵（权重矩阵或低秩增量矩阵），统一了现有的LoRA更新策略（如只更新输入/输出矩阵、交替更新、同时更新等）。2. 基于该框架定义多种优化变体（如GD, SGD, PAGE, MVR等）。3. 理论分析：针对非凸优化问题，为七种变体建立收敛保证；针对凸非光滑问题，分析常数步长和Polyak型自适应步长的收敛速率。4. 实验验证：在多种任务上测试框架性能。

Result: 1. 理论方面：在满足标准假设条件下，证明了所有提出变体的收敛性（包括非凸和凸非光滑场景）。2. 实验方面：验证了理论结论，并证明Bernoulli-LoRA的实际有效性（与理论一致，且在任务上表现良好）。

Conclusion: Bernoulli-LoRA为LoRA类方法提供了首个统一的理论框架，通过概率化更新机制实现了对现有策略的泛化。严格的收敛性证明和实验验证表明该框架兼具理论严密性和实用价值，推动了参数高效微调方法的理论基础发展。

Abstract: Parameter-efficient fine-tuning (PEFT) has emerged as a crucial approach for
adapting large foundational models to specific tasks, particularly as model
sizes continue to grow exponentially. Among PEFT methods, Low-Rank Adaptation
(LoRA) (arXiv:2106.09685) stands out for its effectiveness and simplicity,
expressing adaptations as a product of two low-rank matrices. While extensive
empirical studies demonstrate LoRA's practical utility, theoretical
understanding of such methods remains limited. Recent work on RAC-LoRA
(arXiv:2410.08305) took initial steps toward rigorous analysis. In this work,
we introduce Bernoulli-LoRA, a novel theoretical framework that unifies and
extends existing LoRA approaches. Our method introduces a probabilistic
Bernoulli mechanism for selecting which matrix to update. This approach
encompasses and generalizes various existing update strategies while
maintaining theoretical tractability. Under standard assumptions from
non-convex optimization literature, we analyze several variants of our
framework: Bernoulli-LoRA-GD, Bernoulli-LoRA-SGD, Bernoulli-LoRA-PAGE,
Bernoulli-LoRA-MVR, Bernoulli-LoRA-QGD, Bernoulli-LoRA-MARINA, and
Bernoulli-LoRA-EF21, establishing convergence guarantees for each variant.
Additionally, we extend our analysis to convex non-smooth functions, providing
convergence rates for both constant and adaptive (Polyak-type) stepsizes.
Through extensive experiments on various tasks, we validate our theoretical
findings and demonstrate the practical efficacy of our approach. This work is a
step toward developing theoretically grounded yet practically effective PEFT
methods.

</details>


### [215] [Scalable Neural Network-based Blackbox Optimization](https://arxiv.org/abs/2508.03827)
*Pavankumar Koratikere,Leifur Leifsson*

Main category: cs.LG

TL;DR: 提出了一种基于神经网络的新方法SNBO，用于可扩展的黑盒优化，无需依赖模型不确定性估计。该方法使用独立的探索和利用标准添加新样本，并通过自适应控制采样区域来实现高效优化。在多个高维问题上验证，SNBO在函数值和计算效率上都优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统的贝叶斯优化方法在处理高维问题和大量评估时面临计算效率低下的问题，而现有的基于神经网络的方法通常需要复杂且计算量大的模型不确定性估计。因此，亟需一种高效、可扩展且不依赖模型不确定性估计的黑盒优化方法。

Method: 提出SNBO方法：该方法使用探索和利用两个独立的标准添加新样本。具体而言，通过两个独立的神经网络分别处理探索和利用目标，同时自适应地调整采样区域。探索阶段旨在发现潜在有希望的局部优化区域，而利用阶段则专注于在已识别的区域内寻求优化。在每次迭代中，利用和探索样本的添加数量是固定的，并根据当前优化的进度动态调整采样区域的大小（如半径）。

Result: 在10到102维的18个测试问题上进行实验，SNBO在大多数问题上超过了四种最先进的基线方法的最佳结果，同时将函数评估需求减少了40%-60%，并将运行时间降低了一个数量级以上。

Conclusion: SNBO在无需模型不确定性估计的前提下实现了高效、可扩展的黑盒优化。在降低计算负担的同时，其优化性能优于现有方法。该方法为高维复杂优化问题提供了一种可行的解决方案。

Abstract: Bayesian Optimization (BO) is a widely used approach for blackbox
optimization that leverages a Gaussian process (GP) model and an acquisition
function to guide future sampling. While effective in low-dimensional settings,
BO faces scalability challenges in high-dimensional spaces and with large
number of function evaluations due to the computational complexity of GP
models. In contrast, neural networks (NNs) offer better scalability and can
model complex functions, which led to the development of NN-based BO
approaches. However, these methods typically rely on estimating model
uncertainty in NN prediction -- a process that is often computationally
intensive and complex, particularly in high dimensions. To address these
limitations, a novel method, called scalable neural network-based blackbox
optimization (SNBO), is proposed that does not rely on model uncertainty
estimation. Specifically, SNBO adds new samples using separate criteria for
exploration and exploitation, while adaptively controlling the sampling region
to ensure efficient optimization. SNBO is evaluated on a range of optimization
problems spanning from 10 to 102 dimensions and compared against four
state-of-the-art baseline algorithms. Across the majority of test problems,
SNBO attains function values better than the best-performing baseline
algorithm, while requiring 40-60% fewer function evaluations and reducing the
runtime by at least an order of magnitude.

</details>


### [216] [DP-NCB: Privacy Preserving Fair Bandits](https://arxiv.org/abs/2508.03836)
*Dhruv Sarkar,Nishant Pandey,Sayak Ray Chowdhury*

Main category: cs.LG

TL;DR: 该论文提出了一种名为DP-NCB的算法框架，该框架在差分隐私（差分隐私）的保护下实现了纳什遗憾的最优界限，同时处理了公平性问题。算法适用于全局和局部隐私模型，且无需预先知道时间范围。通过模拟实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 在多臂老虎机算法的实际应用中（如临床试验和个性化决策），隐私保护和决策公平性至关重要。现有工作往往只关注隐私或公平性中的单一目标，而两者同时实现的研究仍为开放问题。隐私保护算法通常优化平均遗憾（功利主义），而公平性算法关注纳什遗憾（惩罚不公）但忽略隐私。因此，作者希望设计同时满足差分隐私和最小化纳什遗憾的算法。

Method: 作者提出差分隐私纳什置信上界(DP-NCB)框架：
1. 同时考虑全局和局部差分隐私模型（ε-DP）。
2. 算法无需预知时间范围(Anytime)。
3. 在置信上界方法中整合机制实现隐私保护（如加噪机制），同时通过优化纳什后悔保证公平（即所有臂的奖励收益分配公平）。
4. 理论分析证明其在纳什遗憾目标上的阶数最优性（匹配下界，仅对数量乘因子差异）。

Result: 理论分析表明：
1. DP-NCB在全局/局部差分隐私下均达到$O(\sum_{i\neq i^*}\frac{\ln T}{\Delta_i})$的纳什后悔界（i为臂，Δi为最优臂与其他臂的奖励差），对数因子下与已知下界一致。
2. 模拟实验在合成老虎机数据上对比：DP-NCB的纳什遗憾显著低于现有基线（如只关注公平的F-UCB或只关注隐私的PrivateBandits），验证了框架在隐私-公平联合优化上的优越性。

Conclusion: DP-NCB是第一个在差分隐私条件下实现阶数最优纳什遗憾的统一框架，填补了同时满足隐私与公平的多臂老虎机算法空白。该成果为高风险社会敏感应用（需要兼顾数据和公正）提供了理论基础，具有广泛适用潜力。

Abstract: Multi-armed bandit algorithms are fundamental tools for sequential
decision-making under uncertainty, with widespread applications across domains
such as clinical trials and personalized decision-making. As bandit algorithms
are increasingly deployed in these socially sensitive settings, it becomes
critical to protect user data privacy and ensure fair treatment across decision
rounds. While prior work has independently addressed privacy and fairness in
bandit settings, the question of whether both objectives can be achieved
simultaneously has remained largely open. Existing privacy-preserving bandit
algorithms typically optimize average regret, a utilitarian measure, whereas
fairness-aware approaches focus on minimizing Nash regret, which penalizes
inequitable reward distributions, but often disregard privacy concerns.
  To bridge this gap, we introduce Differentially Private Nash Confidence Bound
(DP-NCB)-a novel and unified algorithmic framework that simultaneously ensures
$\epsilon$-differential privacy and achieves order-optimal Nash regret,
matching known lower bounds up to logarithmic factors. The framework is
sufficiently general to operate under both global and local differential
privacy models, and is anytime, requiring no prior knowledge of the time
horizon. We support our theoretical guarantees with simulations on synthetic
bandit instances, showing that DP-NCB incurs substantially lower Nash regret
than state-of-the-art baselines. Our results offer a principled foundation for
designing bandit algorithms that are both privacy-preserving and fair, making
them suitable for high-stakes, socially impactful applications.

</details>


### [217] [VAE-DNN: Energy-Efficient Trainable-by-Parts Surrogate Model For Parametric Partial Differential Equations](https://arxiv.org/abs/2508.03839)
*Yifei Zong,Alexandre M. Tartakovsky*

Main category: cs.LG

TL;DR: 提出了一种可部分训练（trainable-by-parts）的代理模型，用于求解带参数的非线性偏微分方程的正问题和反问题。该模型的核心创新在于其三个组件（编码器、全连接神经网络、解码器）能够独立训练，从而显著减少了训练时间和能耗。相较于FNO和DeepONet等主流算子学习模型，VAE-DNN在求解地下水流的非线性扩散方程的正反问题时，不仅效率更高，精度也更高。


<details>
  <summary>Details</summary>
Motivation: 现有算子学习模型（如FNO、DeepONet）在训练高维非线性PDE的正反问题时，通常需要同时训练整个模型，导致训练成本高昂。为降低训练开销，作者提出一种可分离训练的替代模型结构。

Method: 模型分为三个部分：1）编码器：将高维输入y(x)降维至潜在空间μ_φ_y；2）全连接神经网络：将μ_φ_y映射至PDE解h(x,t)的潜在空间μ_φ_h；3）解码器：重构h(x,t)。核心创新在于独立训练机制：编码器作为y(x)的变分自编码器（VAE）训练，解码器作为h(x,t)的VAE训练，中间的全连接网络单独训练。这种分离训练策略大幅降低了训练资源需求。该模型命名为VAE-DNN。

Result: 在非线性扩散方程（模拟无承压含水层地下水流）的正反问题实验中，VAE-DNN相比FNO和DeepONet：1）训练时间与能耗显著降低；2）正反问题求解精度更高。

Conclusion: VAE-DNN通过模型组件的可分离训练机制，在保证精度的同时极大提升了训练效率，为高维PDE的快速正反问题求解提供了更高效的解决方案。

Abstract: We propose a trainable-by-parts surrogate model for solving forward and
inverse parameterized nonlinear partial differential equations. Like several
other surrogate and operator learning models, the proposed approach employs an
encoder to reduce the high-dimensional input $y(\bm{x})$ to a lower-dimensional
latent space, $\bm\mu_{\bm\phi_y}$. Then, a fully connected neural network is
used to map $\bm\mu_{\bm\phi_y}$ to the latent space, $\bm\mu_{\bm\phi_h}$, of
the PDE solution $h(\bm{x},t)$. Finally, a decoder is utilized to reconstruct
$h(\bm{x},t)$. The innovative aspect of our model is its ability to train its
three components independently. This approach leads to a substantial decrease
in both the time and energy required for training when compared to leading
operator learning models such as FNO and DeepONet. The separable training is
achieved by training the encoder as part of the variational autoencoder (VAE)
for $y(\bm{x})$ and the decoder as part of the $h(\bm{x},t)$ VAE. We refer to
this model as the VAE-DNN model. VAE-DNN is compared to the FNO and DeepONet
models for obtaining forward and inverse solutions to the nonlinear diffusion
equation governing groundwater flow in an unconfined aquifer. Our findings
indicate that VAE-DNN not only demonstrates greater efficiency but also
delivers superior accuracy in both forward and inverse solutions compared to
the FNO and DeepONet models.

</details>


### [218] [Data-Driven Spectrum Demand Prediction: A Spatio-Temporal Framework with Transfer Learning](https://arxiv.org/abs/2508.03863)
*Amin Farajzadeh,Hongzhao Zheng,Sarah Dumoulin,Trevor Ha,Halim Yanikomeroglu,Amir Ghasemi*

Main category: cs.LG

TL;DR: 该论文提出了一种基于众包用户侧KPI和监管数据的时空预测框架，用于预测频谱需求，通过先进的特征工程、相关性分析和迁移学习技术，实现了高精度和跨区域的泛化能力。与传统的ITU模型相比，该方法能更好地捕捉频谱利用的时空变化，为政策制定者和监管机构提供了更实际、可操作的频谱管理工具。


<details>
  <summary>Details</summary>
Motivation: 准确的频谱需求预测对于频谱分配、监管规划和促进无线通信网络的可持续发展至关重要。现有ITU模型受限于输入的主观性和不切实际的假设，难以捕捉频谱利用的时空动态变化。因此，需要一种数据驱动的方法来提供更精确且可泛化的预测。

Method: 1. 利用众包用户侧关键性能指标(KPIs)和监管数据集构建时空预测模型。2. 采用先进的特征工程提取关键信息。3. 通过全面的相关性分析确定影响频谱需求的主要因素。4. 结合迁移学习技术提升模型的跨区域泛化能力。

Result: 实验结果表明，所提出的框架在预测准确性上优于传统ITU基准模型。该模型能够更真实地反映频谱需求的时空变化，提供可操作的预测结果。

Conclusion: 本文提出的数据驱动框架为频谱管理提供了更有效的预测工具，支持政策制定者和监管机构优化频谱分配策略和规划，适应5G、6G及物联网等新兴技术的需求。

Abstract: Accurate spectrum demand prediction is crucial for informed spectrum
allocation, effective regulatory planning, and fostering sustainable growth in
modern wireless communication networks. It supports governmental efforts,
particularly those led by the international telecommunication union (ITU), to
establish fair spectrum allocation policies, improve auction mechanisms, and
meet the requirements of emerging technologies such as advanced 5G, forthcoming
6G, and the internet of things (IoT). This paper presents an effective
spatio-temporal prediction framework that leverages crowdsourced user-side key
performance indicators (KPIs) and regulatory datasets to model and forecast
spectrum demand. The proposed methodology achieves superior prediction accuracy
and cross-regional generalizability by incorporating advanced feature
engineering, comprehensive correlation analysis, and transfer learning
techniques. Unlike traditional ITU models, which are often constrained by
arbitrary inputs and unrealistic assumptions, this approach exploits granular,
data-driven insights to account for spatial and temporal variations in spectrum
utilization. Comparative evaluations against ITU estimates, as the benchmark,
underscore our framework's capability to deliver more realistic and actionable
predictions. Experimental results validate the efficacy of our methodology,
highlighting its potential as a robust approach for policymakers and regulatory
bodies to enhance spectrum management and planning.

</details>


### [219] [Prediction-Oriented Subsampling from Data Streams](https://arxiv.org/abs/2508.03868)
*Benedetta Lavinia Mussati,Freddie Bickford Smith,Tom Rainforth,Stephen Roberts*

Main category: cs.LG

TL;DR: 该论文探讨了针对离线学习的数据子采样问题，提出了一种以信息论为基础、专注于降低下游预测不确定性的方法。经验证，该方法在两种广泛研究的任务上表现优于已有的信息论技术，但同时也指出在实际应用中实现稳健性能需要仔细的模型设计。


<details>
  <summary>Details</summary>
Motivation: 数据常常以流的方式生成，随着时间的推移会不断有新的观测数据。学习数据流模型的一个关键挑战是在管理计算成本的同时捕捉相关信息。本文旨在探索智能化的数据子采样技术，用于优化离线学习的效率与效果，特别是在信息捕获上确保对下游预测任务最有价值的数据被保留。

Method: 提出了一种以信息论为中心的方法，聚焦于减少下游预测任务中的不确定性。首先明确感兴趣的预测任务，然后评估不同数据点对于预测不确定性的影响，优先选择能够最有效地降低预测不确定性（信息量最大）的数据子集进行后续的学习。同时，强调了模型设计的严谨性对方法性能的重要性。

Result: 在两个广泛研究的问题上进行了实验，结果表明此预测导向的数据子采样方法优于先前提出的另一种信息论技术。然而，成功实现高性能依赖于精心的模型设计，缺乏合理设计的模型可能导致方法效果不稳定。

Conclusion: 数据流情境下的智能子采样应围绕预测目标设计，通过信息论指导减少预测不确定性是一种有效的策略。在优化后续预测任务性能的同时，本文突出强调实践过程中模型设计的关键作用——模型选择与结构必须与预测任务高度适配，才能稳健地实现高性能。

Abstract: Data is often generated in streams, with new observations arriving over time.
A key challenge for learning models from data streams is capturing relevant
information while keeping computational costs manageable. We explore
intelligent data subsampling for offline learning, and argue for an
information-theoretic method centred on reducing uncertainty in downstream
predictions of interest. Empirically, we demonstrate that this
prediction-oriented approach performs better than a previously proposed
information-theoretic technique on two widely studied problems. At the same
time, we highlight that reliably achieving strong performance in practice
requires careful model design.

</details>


### [220] [Intelligent Sampling of Extreme-Scale Turbulence Datasets for Accurate and Efficient Spatiotemporal Model Training](https://arxiv.org/abs/2508.03872)
*Wesley Brewer,Murali Meena Gopalakrishnan,Matthias Maiterth,Aditya Kashi,Jong Youl Choi,Pei Zhang,Stephen Nichols,Riccardo Balin,Miles Couchman,Stephen de Bruyn Kops,P. K. Yeung,Daniel Dotson,Rohini Uma-Vaideswaran,Sarp Oral,Feiyi Wang*

Main category: cs.LG

TL;DR: SICKLE是一个智能稀疏选择框架，用于高效学习，通过新颖的最大熵采样方法、可扩展训练和能耗基准测试，实现用较少数据训练更好模型。实验显示其在湍流DNS数据集上，相比随机和相空间采样，能提升精度并大幅降低能耗，某些情况下能耗减少38倍。


<details>
  <summary>Details</summary>
Motivation: 随着摩尔定律和Dennard缩放定律的终结，高效训练需要重新考虑数据量。智能子采样能否用显著更少的数据训练出更好模型？为了解决这个问题，作者开发了SICKLE框架。

Method: 1. 提出SICKLE框架，包含：最大熵(MaxEnt)采样方法、可扩展训练机制、能源消耗基准测试工具。2. 在湍流直接数值模拟（DNS）大数据集上，将MaxEnt与随机采样、相空间采样进行对比。3. 在Frontier超算系统上进行大规模评估。

Result: 1. 子采样作为预处理步骤可提升模型精度。2. 显著降低能源消耗：在特定案例中观察到高达38倍的能耗缩减。

Conclusion: 智能子采样技术(SICKLE)能够突破传统训练的数据依赖，在保证模型性能的同时实现能源效率的显著提升，为未来大规模计算提供可持续解决方案。

Abstract: With the end of Moore's law and Dennard scaling, efficient training
increasingly requires rethinking data volume. Can we train better models with
significantly less data via intelligent subsampling? To explore this, we
develop SICKLE, a sparse intelligent curation framework for efficient learning,
featuring a novel maximum entropy (MaxEnt) sampling approach, scalable
training, and energy benchmarking. We compare MaxEnt with random and
phase-space sampling on large direct numerical simulation (DNS) datasets of
turbulence. Evaluating SICKLE at scale on Frontier, we show that subsampling as
a preprocessing step can improve model accuracy and substantially lower energy
consumption, with reductions of up to 38x observed in certain cases.

</details>


### [221] [Reinforcement Learning for Target Zone Blood Glucose Control](https://arxiv.org/abs/2508.03875)
*David H. Mguni,Jing Dong,Wanrong Yang,Ziquan Liu,Muhammad Salman Haleem,Baoxiang Wang*

Main category: cs.LG

TL;DR: 本文提出了一种统一脉冲控制和切换控制的强化学习框架，用于管理1型糖尿病的治疗决策。该框架通过增强生理状态特征和考虑胰岛素衰减等现实因素，提高了策略的安全性和时效性。在实验中，血糖水平违规率从22.4%降至10.8%。


<details>
  <summary>Details</summary>
Motivation: 在1型糖尿病管理中，控制生理变量在安全范围内具有挑战性。强化学习虽然具备个性化治疗的潜力，但难以处理干预措施带来的延迟和异质性效果。因此，需要一种能够捕捉治疗复杂时间动态的新方法。

Method: 提出一个包含两种控制模式的强化学习框架：脉冲控制（用于离散、快效干预）和切换控制（用于长效治疗和方案切换）。方法核心是基于增强生理状态特征的约束Markov决策过程，并在策略学习中纳入胰岛素衰减等生物学现实因素，同时满足临床和资源限制。

Result: 在理论层面证明框架的收敛性。通过1型糖尿病控制任务实验验证，将血糖水平违规率从先前最好结果22.4%显著降低至最低10.8%。

Conclusion: 本研究为未来医疗领域安全且具备时间感知的强化学习奠定了基础。虽然框架不直接用于临床部署，但其统一控制模态的方式及生物学因素整合，为开发更贴近现实的治疗策略提供了新途径。

Abstract: Managing physiological variables within clinically safe target zones is a
central challenge in healthcare, particularly for chronic conditions such as
Type 1 Diabetes Mellitus (T1DM). Reinforcement learning (RL) offers promise for
personalising treatment, but struggles with the delayed and heterogeneous
effects of interventions. We propose a novel RL framework to study and support
decision-making in T1DM technologies, such as automated insulin delivery. Our
approach captures the complex temporal dynamics of treatment by unifying two
control modalities: \textit{impulse control} for discrete, fast-acting
interventions (e.g., insulin boluses), and \textit{switching control} for
longer-acting treatments and regime shifts. The core of our method is a
constrained Markov decision process augmented with physiological state
features, enabling safe policy learning under clinical and resource
constraints. The framework incorporates biologically realistic factors,
including insulin decay, leading to policies that better reflect real-world
therapeutic behaviour. While not intended for clinical deployment, this work
establishes a foundation for future safe and temporally-aware RL in healthcare.
We provide theoretical guarantees of convergence and demonstrate empirical
improvements in a stylised T1DM control task, reducing blood glucose level
violations from 22.4\% (state-of-the-art) to as low as 10.8\%.

</details>


### [222] [Calibrating Biophysical Models for Grape Phenology Prediction via Multi-Task Learning](https://arxiv.org/abs/2508.03898)
*William Solow,Sandhya Saisubramanian*

Main category: cs.LG

TL;DR: 提出了一种结合多任务学习和循环神经网络的混合建模方法，用于参数化可微分的生物物理模型，以提高葡萄物候预测的准确性。该方法通过跨品种共享学习并保留生物结构，显著优于传统生物物理模型和基线深度学习方法。


<details>
  <summary>Details</summary>
Motivation: 准确预测葡萄物候对于葡萄园管理至关重要。传统生物物理模型在季节尺度预测上校准历史数据但缺乏细粒度管理所需精度；深度学习方法受限于稀疏的品种级物候数据集。因此，需要一种能结合两者优势的方法。

Method: 开发混合模型：利用多任务学习预测可微分生物物理模型的参数，并通过循环神经网络实现参数化。该方法允许跨品种共享学习，同时保留生物结构，从而提高预测稳健性和准确性。

Result: 在真实世界和合成数据集上的评估表明，该方法在预测物候阶段（如冷害抗性）及其他作物状态变量（如小麦产量）方面，显著优于传统生物物理模型和基线深度学习方法。

Conclusion: 所提出的混合建模方法通过整合深度学习和生物物理过程，解决了稀疏数据下的品种级物候预测问题，为细粒度葡萄园管理提供了更可靠的工具，并在多类农业预测任务中验证了有效性。

Abstract: Accurate prediction of grape phenology is essential for timely vineyard
management decisions, such as scheduling irrigation and fertilization, to
maximize crop yield and quality. While traditional biophysical models
calibrated on historical field data can be used for season-long predictions,
they lack the precision required for fine-grained vineyard management. Deep
learning methods are a compelling alternative but their performance is hindered
by sparse phenology datasets, particularly at the cultivar level. We propose a
hybrid modeling approach that combines multi-task learning with a recurrent
neural network to parameterize a differentiable biophysical model. By using
multi-task learning to predict the parameters of the biophysical model, our
approach enables shared learning across cultivars while preserving biological
structure, thereby improving the robustness and accuracy of predictions.
Empirical evaluation using real-world and synthetic datasets demonstrates that
our method significantly outperforms both conventional biophysical models and
baseline deep learning approaches in predicting phenological stages, as well as
other crop state variables such as cold-hardiness and wheat yield.

</details>


### [223] [Fast and Accurate Explanations of Distance-Based Classifiers by Uncovering Latent Explanatory Structures](https://arxiv.org/abs/2508.03913)
*Florian Bley,Jacob Kauffmann,Simon León Krug,Klaus-Robert Müller,Grégoire Montavon*

Main category: cs.LG

TL;DR: 本文揭示基于距离的分类器（如k近邻和支持向量机）具有隐藏的神经网络结构，使得可解释AI技术（如层间相关性传播，LRP）可以应用于这些模型。新方法在定量评估中优于多种基线，并通过实际用例展示了基于距离模型解释的实用性。


<details>
  <summary>Details</summary>
Motivation: 基于距离的分类器在机器学习和实践中广泛应用，但理解其预测过程并生成解释以增进信任和决策是重要需求。现有的可解释AI方法往往需要模型具有特定的层级结构，而传统距离分类器缺少这种结构，因此需要一种新方法使其适用于这些模型。

Method: 1. 发现在距离分类器中隐藏的神经网络结构，该结构由线性检测单元和非线性池化层组成。2. 将现有的可解释AI技术（如LRP）应用于该结构，生成对分类预测的解释。3. 进行定量评估，通过与基线方法比较评估新解释方法的优势。4. 通过两个实际用例展示该方法的实用性。

Result: 1. 定量评估表明，提出的解释方法优于多种基线。2. 两个实际用例验证了该方法在解释基于距离分类器预测中的有用性。

Conclusion: 通过揭示距离分类器中的隐藏神经网络结构，可成功应用可解释AI技术（如LRP）来生成准确且有用的预测解释，提高了这些广泛使用模型的可解释性，助力科学和产业应用中的决策与洞察。

Abstract: Distance-based classifiers, such as k-nearest neighbors and support vector
machines, continue to be a workhorse of machine learning, widely used in
science and industry. In practice, to derive insights from these models, it is
also important to ensure that their predictions are explainable. While the
field of Explainable AI has supplied methods that are in principle applicable
to any model, it has also emphasized the usefulness of latent structures (e.g.
the sequence of layers in a neural network) to produce explanations. In this
paper, we contribute by uncovering a hidden neural network structure in
distance-based classifiers (consisting of linear detection units combined with
nonlinear pooling layers) upon which Explainable AI techniques such as
layer-wise relevance propagation (LRP) become applicable. Through quantitative
evaluations, we demonstrate the advantage of our novel explanation approach
over several baselines. We also show the overall usefulness of explaining
distance-based models through two practical use cases.

</details>


### [224] [Active Learning and Transfer Learning for Anomaly Detection in Time-Series Data](https://arxiv.org/abs/2508.03921)
*John D. Kelleher,Matthew Nicholson,Rahul Agrahari,Clare Conran*

Main category: cs.LG

TL;DR: 本文研究了主动学习结合迁移学习在跨领域时序数据异常检测中的效果。结果显示，聚类与主动学习之间存在交互作用，但最佳性能出现在不使用聚类（单一集群）时。主动学习虽然能提升模型表现，但提升率普遍比文献报道慢，原因在于实验设计中采样池和测试池使用了不同数据样本。另外，主动学习与迁移学习的性能上限评估表明，随目标点增加性能先升后降，暗示主动学习的数据点筛选序列有效（后期点效用较低），最终表现为模型提升率呈线性平缓函数形态。


<details>
  <summary>Details</summary>
Motivation: 探索主动学习(Active Learning)与迁移学习(Transfer Learning)相结合在跨领域时间序列异常检测中的有效性，重点分析聚类在其中的作用、主动学习的实际提升速度，以及两者组合的性能上限问题，厘清现有文献中可能被实验设计影响的结果偏差。

Method: 1. 设计对比实验：组合主动学习（逐步选定并标记目标域数据加入训练集）和迁移学习（基于源域模型调整至目标域）
2. 分析聚类与主动学习交互：在模型中引入聚类处理（分组样本）或忽略聚类（单一集群）并比较性能差异
3. 改进实验设计：严格分离采样池（用于主动学习筛选）和测试池（最终验证）的数据样本以消除偏差
4. 评估性能上限：在不同数据集上观察主动学习随选定样本量增长时的模型表现变化趋势（尤其关注性能拐点）

Result: 1. 最佳性能出现于不使用聚类时（单一集群），暗示聚类在该任务中可能与主动学习目标冲突
2. 主动学习可提升模型表现，但提升速度慢于文献报告，归因于采样池与测试池独立设计的严格性
3. 迁移学习+主动学习的组合存在性能上限：随标注目标点增加，性能先提升后趋于平缓甚至下降，表明主动学习后期纳入的样本效用较低
4. 总体性能提升形态为线性平缓函数，而非文献中常见的指数级快速改善

Conclusion: 在跨域时序异常检测中：(1) 聚类未必有益，可能干扰主动学习样本选择有效性；(2) 主动学习仍有效但实际提升效率低于理论预期，需警惕实验设计差异导致的结论偏差；(3) 主动学习能有效排序样本价值（高价值优先），导致后期性能停滞，暗示实践中应设置标注预算上限；(4) 组合策略的模型改进呈缓慢线性趋势，需针对性优化主动学习策略以突破瓶颈。

Abstract: This paper examines the effectiveness of combining active learning and
transfer learning for anomaly detection in cross-domain time-series data. Our
results indicate that there is an interaction between clustering and active
learning and in general the best performance is achieved using a single cluster
(in other words when clustering is not applied). Also, we find that adding new
samples to the training set using active learning does improve model
performance but that in general, the rate of improvement is slower than the
results reported in the literature suggest. We attribute this difference to an
improved experimental design where distinct data samples are used for the
sampling and testing pools. Finally, we assess the ceiling performance of
transfer learning in combination with active learning across several datasets
and find that performance does initially improve but eventually begins to tail
off as more target points are selected for inclusion in training. This tail-off
in performance may indicate that the active learning process is doing a good
job of sequencing data points for selection, pushing the less useful points
towards the end of the selection process and that this tail-off occurs when
these less useful points are eventually added. Taken together our results
indicate that active learning is effective but that the improvement in model
performance follows a linear flat function concerning the number of points
selected and labelled.

</details>


### [225] [Next Generation Equation-Free Multiscale Modelling of Crowd Dynamics via Machine Learning](https://arxiv.org/abs/2508.03926)
*Hector Vargas Alvarez,Dimitrios G. Patsatzis,Lucia Russo,Ioannis Kevrekidis,Constantinos Siettos*

Main category: cs.LG

TL;DR: 提出了一种结合流形学习和机器学习的方法，从基于代理的高保真模拟中学习潜在空间中的人群动态演化解算子。该框架包括四个步骤：通过KDE从离散微观数据推导宏观密度场；基于流形学习构建从宏观空间到潜在空间的映射；在潜在空间使用LSTM和MVAR等机器学习技术学习降阶代理模型；重建高维空间的宏观密度分布。方法在循环边界条件的走廊障碍物场景中验证，显示出高精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决从微观到宏观尺度的人群动态建模这一开放挑战，以实现系统化的数值分析、优化及控制。现有方法难以直接建立多尺度耦合模型，特别是在高维复杂系统中。基于此，作者提出一种框架，利用代理模型在低维潜在空间学习动态规律，从而规避直接求解不可得宏观微分方程的问题。

Method: 1. 核密度估计(KDE): 从行人位置微观数据中提取宏观密度场。2. 流形学习与降维: 通过主成分分析(POD)构建宏观空间到低维潜在空间的映射。3. 机器学习建模: 在潜在空间利用LSTM(长短期记忆网络)和MVAR(多变量自回归模型)学习动态演化算子，并显式保证质量守恒。4. 高维重建: 通过SVD逆过程返回宏观密度分布，形成闭环。验证使用社会力模型生成带障碍物走廊场景数据。

Result: 数值实验表明该方法在周期性边界条件的走廊障碍物场景中：1. POD重建的密度分布严格守恒质量；2. 所构建的数据驱动解算子能有效替代不可获得的宏观PDE；3. LSTM与MVAR模型均表现出高精度(预测误差<5%)、强鲁棒性(对噪声敏感度低)和良好泛化能力(适应未见初始条件)。

Conclusion: 1. 提出的四阶段框架成功桥接了Agent-based微观模型与宏观PDE之间的尺度鸿沟；2. 通过流形学习与机器学习结合，实现了高维人群动力学系统的可处理建模；3. 方法在复杂几何条件下仍保持质量守恒特性；4. 该数据驱动框架为其他无法显式建模的复杂系统提供通用建模范式。

Abstract: Bridging the microscopic and the macroscopic modelling scales in crowd
dynamics constitutes an important, open challenge for systematic numerical
analysis, optimization, and control. We propose a combined manifold and machine
learning approach to learn the discrete evolution operator for the emergent
crowd dynamics in latent spaces from high-fidelity agent-based simulations. The
proposed framework builds upon our previous works on next-generation
Equation-free algorithms on learning surrogate models for high-dimensional and
multiscale systems. Our approach is a four-stage one, explicitly conserving the
mass of the reconstructed dynamics in the high-dimensional space. In the first
step, we derive continuous macroscopic fields (densities) from discrete
microscopic data (pedestrians' positions) using KDE. In the second step, based
on manifold learning, we construct a map from the macroscopic ambient space
into the latent space parametrized by a few coordinates based on POD of the
corresponding density distribution. The third step involves learning
reduced-order surrogate ROMs in the latent space using machine learning
techniques, particularly LSTMs networks and MVARs. Finally, we reconstruct the
crowd dynamics in the high-dimensional space in terms of macroscopic density
profiles. We demonstrate that the POD reconstruction of the density
distribution via SVD conserves the mass. With this "embed->learn in latent
space->lift back to the ambient space" pipeline, we create an effective
solution operator of the unavailable macroscopic PDE for the density evolution.
For our illustrations, we use the Social Force Model to generate data in a
corridor with an obstacle, imposing periodic boundary conditions. The numerical
results demonstrate high accuracy, robustness, and generalizability, thus
allowing for fast and accurate modelling/simulation of crowd dynamics from
agent-based simulations.

</details>


### [226] [Markov Chain Estimation with In-Context Learning](https://arxiv.org/abs/2508.03934)
*Simon Lepage,Jeremie Mary,David Picard*

Main category: cs.LG

TL;DR: 该文研究了通过只训练语言模型做下一个词预测任务，使其能够学习涉及上下文的算法。作者设计了随机转移矩阵的马尔可夫链任务，在训练和测试中使用不同的矩阵。实验发现当模型大小和训练数据量超过某个阈值时，模型能够从上下文中估计转移概率，而不是仅仅记住训练数据中的模式。此外，改进对状态的编码能让模型对训练中未见过的结构的马尔可夫链有更好的稳健性。


<details>
  <summary>Details</summary>
Motivation: 研究人员希望探索transformer模型是否能通过仅做下一个词预测的训练而学习涉及上下文的复杂算法。他们的动机在于验证transformer能否从上下文推理而不仅仅是记忆模式，并研究模型大小和数据量对模型理解算法的影响。

Method: 研究者采用了以下方法：1）设计带有随机转移矩阵的马尔可夫链任务；2）使用仅通过下一个词预测进行训练的transformer模型；3）在训练和测试中使用不同的转移矩阵；4）测量模型能否估计转移概率而不是记忆模式，并设置模型大小和数据量的阈值标准；5）通过改进的状态编码（如one-hot或embedding）提升模型在未见马尔可夫链结构上的鲁棒性。

Result: 实验结果显示，当transformer模型的大小和训练数据的数量超过一定阈值时，模型能够从上下文中推理出转移概率而不是依赖记忆；并且，如果使用更有效的方式编码状态，模型可以对训练中未出现过的不同结构的马尔可夫链表现出良好的稳健性。

Conclusion: 这项工作证明了在适当模型大小和训练数据支持下，transformer架构可以通过下一个词预测任务超越纯粹的统计模式记忆，发展出算法能力，比如在上下文中估计转移矩阵等。对状态更优的编码还可以提升模型针对训练外数据的泛化能力。

Abstract: We investigate the capacity of transformers to learn algorithms involving
their context while solely being trained using next token prediction. We set up
Markov chains with random transition matrices and we train transformers to
predict the next token. Matrices used during training and test are different
and we show that there is a threshold in transformer size and in training set
size above which the model is able to learn to estimate the transition
probabilities from its context instead of memorizing the training patterns.
Additionally, we show that more involved encoding of the states enables more
robust prediction for Markov chains with structures different than those seen
during training.

</details>


### [227] [FairPOT: Balancing AUC Performance and Fairness with Proportional Optimal Transport](https://arxiv.org/abs/2508.03940)
*Pengxi Liu,Yi Shen,Matthew M. Engelhard,Benjamin A. Goldstein,Michael J. Pencina,Nicoleta J. Economou-Zavlanos,Michael M. Zavlanos*

Main category: cs.LG

TL;DR: 提出了FairPOT，一种新颖的模型无关后处理框架，通过最优传输选择性地对齐不同群体间的风险评分分布，以在减少AUC差异和维护整体AUC性能之间实现可调节的权衡。该方法还扩展到部分AUC场景，并在合成、公开和临床数据集上展示出优于现有后处理技术的性能。


<details>
  <summary>Details</summary>
Motivation: 在高风险领域（如医疗、金融和刑事司法）中，使用AUC的公平性度量日益受到关注。然而，这些领域通常基于风险评分评估公平性，而强制执行严格公平会显著降低AUC性能。为解决这一挑战，旨在在公平性和AUC性能之间取得更好的平衡。

Method: FairPOT（公平比例最优传输）框架使用最优传输技术，选择性地转换劣势群体中可控比例（即 top-lambda 分位数）的分数，从而对齐不同群体的风险评分分布。通过调整lambda参数，方法允许在减少AUC差异和保持整体AUC性能之间进行权衡。同时，扩展到部分AUC场景，使公平性干预可以集中在最高风险区域。

Result: 在合成、公开和临床数据集上的实验表明，FairPOT在全局AUC和部分AUC场景下均优于现有后处理技术，通常能够以轻微的AUC性能下降甚至正面的效用提升实现改进的公平性。

Conclusion: FairPOT的计算效率和实际适应性使其成为现实世界中实用的公平性解决方案，成功地在公平性和性能之间达到了比其他方法更优的平衡。

Abstract: Fairness metrics utilizing the area under the receiver operator
characteristic curve (AUC) have gained increasing attention in high-stakes
domains such as healthcare, finance, and criminal justice. In these domains,
fairness is often evaluated over risk scores rather than binary outcomes, and a
common challenge is that enforcing strict fairness can significantly degrade
AUC performance. To address this challenge, we propose Fair Proportional
Optimal Transport (FairPOT), a novel, model-agnostic post-processing framework
that strategically aligns risk score distributions across different groups
using optimal transport, but does so selectively by transforming a controllable
proportion, i.e., the top-lambda quantile, of scores within the disadvantaged
group. By varying lambda, our method allows for a tunable trade-off between
reducing AUC disparities and maintaining overall AUC performance. Furthermore,
we extend FairPOT to the partial AUC setting, enabling fairness interventions
to concentrate on the highest-risk regions. Extensive experiments on synthetic,
public, and clinical datasets show that FairPOT consistently outperforms
existing post-processing techniques in both global and partial AUC scenarios,
often achieving improved fairness with slight AUC degradation or even positive
gains in utility. The computational efficiency and practical adaptability of
FairPOT make it a promising solution for real-world deployment.

</details>


### [228] [BubbleONet: A Physics-Informed Neural Operator for High-Frequency Bubble Dynamics](https://arxiv.org/abs/2508.03965)
*Yunhao Zhang,Lin Cheng,Aswin Gnanaskandan,Ameya D. Jagtap*

Main category: cs.LG

TL;DR: BubbleONet是一种基于物理信息的深度运算符网络（PI-DeepONet）构建的算子学习模型，用于将压力分布映射到气泡半径响应。通过引入Rowdy自适应激活函数缓解深度学习中的频谱偏差，该模型在多种气泡动力学场景中表现出色，成为高效替代传统数值求解器的代理模型。


<details>
  <summary>Details</summary>
Motivation: 传统数值方法模拟气泡动力学（如气泡在压力变化下的半径响应）计算成本高。本文旨在开发一种结合物理信息的深度算子网络模型，利用DeepONet的普适逼近能力和物理信息神经网络（PINNs）的物理保真度，构建高效的气泡动力学模拟代理模型。

Method: 1. 架构：基于PI-DeepONet框架，构建BubbleONet模型，输入为压力分布函数，输出为气泡半径响应曲线。
2. 改进：引入Rowdy自适应激活函数缓解网络频谱偏差，以更准确捕捉高频特征。
3. 训练：对比单步训练与两步训练策略（先预训练主干网络再微调物理约束）。
4. 验证场景：
   - 单初始半径的Rayleigh-Plesset方程气泡动力学
   - 单初始半径的Keller-Miksis方程气泡动力学
   - 多初始半径的Keller-Miksis方程气泡动力学

Result: 1. 在三种气泡动力学场景中均成功映射压力输入到气泡半径输出。
2. Rowdy激活函数有效提升了高频特征建模能力。
3. 两步训练策略效果优于单步训练。
4. 相比传统数值求解器，BubbleONet显著提升计算效率（具体数据未给出）。

Conclusion: BubbleONet通过融合物理约束与自适应激活函数，成为气泡动力学的高效代理模型。其架构设计可扩展至其他物理系统的高频响应问题，两步训练策略为复杂物理约束模型提供新思路。未来工作需验证其在更广参数范围和耦合场景的鲁棒性。

Abstract: This paper introduces BubbleONet, an operator learning model designed to map
pressure profiles from an input function space to corresponding bubble radius
responses. BubbleONet is built upon the physics-informed deep operator network
(PI-DeepONet) framework, leveraging DeepONet's powerful universal approximation
capabilities for operator learning alongside the robust physical fidelity
provided by the physics-informed neural networks. To mitigate the inherent
spectral bias in deep learning, BubbleONet integrates the Rowdy adaptive
activation function, enabling improved representation of high-frequency
features. The model is evaluated across various scenarios, including: (1)
Rayleigh-Plesset equation based bubble dynamics with a single initial radius,
(2) Keller-Miksis equation based bubble dynamics with a single initial radius,
and (3) Keller-Miksis equation based bubble dynamics with multiple initial
radii. Moreover, the performance of single-step versus two-step training
techniques for BubbleONet is investigated. The results demonstrate that
BubbleONet serves as a promising surrogate model for simulating bubble
dynamics, offering a computationally efficient alternative to traditional
numerical solvers.

</details>


### [229] [Dynamic User-controllable Privacy-preserving Few-shot Sensing Framework](https://arxiv.org/abs/2508.03989)
*Ajesh Koyatan Chathoth,Shuhao Yu,Stephen Lee*

Main category: cs.LG

TL;DR: 提出了PrivCLIP框架，该框架允许用户动态控制并指定隐私偏好，利用多模态对比学习和语言引导的转换机制，仅需少量样本即可识别敏感活动并在检测到后对数据进行隐私合规的转换，同时在多个人类活动识别数据集上显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于传感器数据的隐私保护方法通常依赖静态预定义的隐私标签或大量私有训练数据，限制了适应性和用户控制能力。用户隐私偏好因人而异且随时间变化，因此需要一种动态、用户可控制的、仅需少量样本的隐私保护框架。

Method: 1. 用户通过黑名单、白名单和灰名单分类活动指定隐私偏好。2. 利用基于对比学习的多模态对齐框架，将IMU传感器数据与自然语言活动描述映射到共享嵌入空间，实现少量样本敏感活动检测。3. 当检测到敏感活动时，通过语言引导的活动净化器和运动生成模块（IMU-GPT）将原始数据转换为语义上类似于不敏感活动的隐私合规版本。

Result: 在多个人类活动识别数据集上的评估表明，PrivCLIP显著优于基线方法，既有效保护了隐私又维持了数据的实用性。

Conclusion: PrivCLIP为用户提供了一种灵活动态控制IMU数据隐私保护的方式，在少量样本设定下实现了对多类别敏感活动的精准识别和隐私转换，为可穿戴设备中的隐私保护提供了新思路。

Abstract: User-controllable privacy is important in modern sensing systems, as privacy
preferences can vary significantly from person to person and may evolve over
time. This is especially relevant in devices equipped with Inertial Measurement
Unit (IMU) sensors, such as smartphones and wearables, which continuously
collect rich time-series data that can inadvertently expose sensitive user
behaviors. While prior work has proposed privacy-preserving methods for sensor
data, most rely on static, predefined privacy labels or require large
quantities of private training data, limiting their adaptability and user
agency. In this work, we introduce PrivCLIP, a dynamic, user-controllable,
few-shot privacy-preserving sensing framework. PrivCLIP allows users to specify
and modify their privacy preferences by categorizing activities as sensitive
(black-listed), non-sensitive (white-listed), or neutral (gray-listed).
Leveraging a multimodal contrastive learning approach, PrivCLIP aligns IMU
sensor data with natural language activity descriptions in a shared embedding
space, enabling few-shot detection of sensitive activities. When a
privacy-sensitive activity is identified, the system uses a language-guided
activity sanitizer and a motion generation module (IMU-GPT) to transform the
original data into a privacy-compliant version that semantically resembles a
non-sensitive activity. We evaluate PrivCLIP on multiple human activity
recognition datasets and demonstrate that it significantly outperforms baseline
methods in terms of both privacy protection and data utility.

</details>


### [230] [Tensorized Clustered LoRA Merging for Multi-Task Interference](https://arxiv.org/abs/2508.03999)
*Zhan Su,Fengran Mo,Guojun Liang,Jinghan Zhang,Bingbing Wen,Prayag Tiwari,Jian-Yun Nie*

Main category: cs.LG

TL;DR: 提出了TC-LoRA以解决多任务场景下合并LoRA适配器时的任务干扰问题，在文本和参数两个层面进行优化：文本层面聚类训练样本并为每个簇训练专用适配器；参数层面通过联合张量分解解耦任务相关和共享因子。实验显示TC-LoRA在多项任务中提升SOTA模型性能。


<details>
  <summary>Details</summary>
Motivation: 在多任务设置中，基于LoRA的微调方法合并针对不同训练的适配器时会产生任务干扰，降低下游任务性能。为解决这一问题，本文旨在从样本分布和参数空间两个维度消除干扰。

Method: '文本级干预'：在嵌入空间对训练样本聚类，每个簇训练单独的LoRA适配器；'参数级干预'：联合应用CP张量分解技术，将各适配器权重分解为任务特有因子和共享因子，保留核心知识并减少任务间冲突。架构名称为TC-LoRA（Tensorized Clustered LoRA）。

Result: 在零样本域外泛化（OOD）和技能组合任务上测试，包括推理、QA和代码生成任务下比较SVD基线方法：Phi-3模型精度提升1.4%，Mistral-7B模型提升2.3%，证实该方法有效缓解任务干扰。

Conclusion: TC-LoRA通过双层级干预策略显著减轻多任务LoRA适配器合并时的性能衰退问题，实验证明其优于主流分解方法，为高效语言模型适应提供新方向。验证了张量聚类机制对提升模型鲁棒性的作用。

Abstract: Despite the success of the monolithic dense paradigm of large language models
(LLMs), the LoRA adapters offer an efficient solution by fine-tuning small
task-specific modules and merging them with the base model. However, in
multi-task settings, merging LoRA adapters trained on heterogeneous sources
frequently causes \textit{task interference}, degrading downstream performance.
To address this, we propose a tensorized clustered LoRA (TC-LoRA) library
targeting to address the task interference at the \textit{text-level} and
\textit{parameter-level}. At the \textit{text-level}, we cluster the training
samples in the embedding space to capture input-format similarities, then train
a specialized LoRA adapter for each cluster. At the \textit{parameter-level},
we introduce a joint Canonical Polyadic (CP) decomposition that disentangles
task-specific and shared factors across LoRA adapters. This joint factorization
preserves essential knowledge while reducing cross-task interference. Extensive
experiments on out-of-domain zero-shot and skill-composition tasks-including
reasoning, question answering, and coding. Compared to strong SVD-based
baselines, TC-LoRA achieves +1.4\% accuracy on Phi-3 and +2.3\% on Mistral-7B
(+2.3\%), demonstrating the effectiveness of TC-LoRA in LLM adaptation.

</details>


### [231] [Decoupled Contrastive Learning for Federated Learning](https://arxiv.org/abs/2508.04005)
*Hyungbin Kim,Incheol Baek,Yon Dohn Chung*

Main category: cs.LG

TL;DR: 提出了名为DCFL（解耦对比学习的联邦学习）的新框架，通过将现有对比损失解耦为对齐和均匀性两个独立目标，解决了联邦学习中对比学习需要无限负样本的渐进假设与有限样本冲突的问题，从而在数据量小的客户端环境中有效工作，并在多个基准测试上取得了优于现有方法的结果。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在数据异构环境下的性能下降问题，而对比学习被提出作为缓解方法。然而，理论分析揭示了对比学习在联邦学习有限样本场景下的一个根本冲突：其依赖于无限负样本的渐进假设在有限样本中不成立。

Method: 提出DCFL框架，将对比损失解耦为两个独立目标：对齐（正样本间的吸引力）和均匀性（负样本间的排斥力）。这种方法不需要渐进假设，允许独立调节两个作用力。在联邦学习设置中，每个客户端在本地数据上分别优化这两个目标，只共享模型更新而不共享原始数据。

Result: 实验表明，DCFL在正样本对齐和负样本均匀化方面优于现有对比学习方法。在CIFAR-10、CIFAR-100和Tiny-ImageNet等标准基准测试中，DCFL一致性地超越了最先进的联邦学习方法。

Conclusion: 解耦对比学习解决了联邦学习中对比学习的内在矛盾，使得在小数据客户端环境下也能有效工作，并通过更好的对齐和均匀化提升了联邦学习的性能。

Abstract: Federated learning is a distributed machine learning paradigm that allows
multiple participants to train a shared model by exchanging model updates
instead of their raw data. However, its performance is degraded compared to
centralized approaches due to data heterogeneity across clients. While
contrastive learning has emerged as a promising approach to mitigate this, our
theoretical analysis reveals a fundamental conflict: its asymptotic assumptions
of an infinite number of negative samples are violated in finite-sample regime
of federated learning. To address this issue, we introduce Decoupled
Contrastive Learning for Federated Learning (DCFL), a novel framework that
decouples the existing contrastive loss into two objectives. Decoupling the
loss into its alignment and uniformity components enables the independent
calibration of the attraction and repulsion forces without relying on the
asymptotic assumptions. This strategy provides a contrastive learning method
suitable for federated learning environments where each client has a small
amount of data. Our experimental results show that DCFL achieves stronger
alignment between positive samples and greater uniformity between negative
samples compared to existing contrastive learning methods. Furthermore,
experimental results on standard benchmarks, including CIFAR-10, CIFAR-100, and
Tiny-ImageNet, demonstrate that DCFL consistently outperforms state-of-the-art
federated learning methods.

</details>


### [232] [A Comparative Survey of PyTorch vs TensorFlow for Deep Learning: Usability, Performance, and Deployment Trade-offs](https://arxiv.org/abs/2508.04035)
*Zakariya Ba Alawi*

Main category: cs.LG

TL;DR: 对TensorFlow和PyTorch两大深度学习框架在可用性、性能和部署能力等方面的全面对比研究。调查发现，PyTorch因其灵活性和用户友好性被研究界广泛采用，而TensorFlow则凭借成熟的部署生态在企业应用中更具优势。


<details>
  <summary>Details</summary>
Motivation: 为研究者和从业者提供详细的框架选择依据。随着深度学习框架的不断演进（如TensorFlow引入eager执行），需要对其最新进展、优劣势进行系统性评估，帮助用户依据应用场景（研究或生产）做出合适选择。

Method: 1. 比较编程范式和开发体验（TensorFlow的图计算与PyTorch的Pythonic动态图）; 2. 在多任务、多数据规模下测试训练和推理性能; 3. 调研部署生态成熟度（包括TensorFlow Lite/Serving/JS支持与PyTorch的TorchScript/ONNX/TorchServe）; 4. 分析社区支持与研究趋势（统计论文采用率及产业落地案例）; 5. 通过CV/NLP领域应用案例说明实际用法; 6. 展望未来技术挑战（如执行模式统一、框架互操作性、编译优化整合）

Result: PyTorch在学术研究中占主导地位（大量论文采用），开发调试更便捷；TensorFlow在生产部署场景中工具链更完善。性能方面两者在标准测试中互有胜负，但均支持通过XLA/JIT等编译优化提升效率。

Conclusion: 两大框架均支持前沿深度学习任务，但存在显著差异：研究场景优先考虑PyTorch以提升开发效率，生产部署则建议采用TensorFlow生态系统。后续发展需关注执行模式统一、跨框架兼容性增强及自动化编译优化集成。

Abstract: This paper presents a comprehensive comparative survey of TensorFlow and
PyTorch, the two leading deep learning frameworks, focusing on their usability,
performance, and deployment trade-offs. We review each framework's programming
paradigm and developer experience, contrasting TensorFlow's graph-based (now
optionally eager) approach with PyTorch's dynamic, Pythonic style. We then
compare model training speeds and inference performance across multiple tasks
and data regimes, drawing on recent benchmarks and studies. Deployment
flexibility is examined in depth - from TensorFlow's mature ecosystem
(TensorFlow Lite for mobile/embedded, TensorFlow Serving, and JavaScript
support) to PyTorch's newer production tools (TorchScript compilation, ONNX
export, and TorchServe). We also survey ecosystem and community support,
including library integrations, industry adoption, and research trends (e.g.,
PyTorch's dominance in recent research publications versus TensorFlow's broader
tooling in enterprise). Applications in computer vision, natural language
processing, and other domains are discussed to illustrate how each framework is
used in practice. Finally, we outline future directions and open challenges in
deep learning framework design, such as unifying eager and graph execution,
improving cross-framework interoperability, and integrating compiler
optimizations (XLA, JIT) for improved speed. Our findings indicate that while
both frameworks are highly capable for state-of-the-art deep learning, they
exhibit distinct trade-offs: PyTorch offers simplicity and flexibility favored
in research, whereas TensorFlow provides a fuller production-ready ecosystem -
understanding these trade-offs is key for practitioners selecting the
appropriate tool. We include charts, code snippets, and more than 20 references
to academic papers and official documentation to support this comparative
analysis

</details>


### [233] [FeDaL: Federated Dataset Learning for Time Series Foundation Models](https://arxiv.org/abs/2508.04045)
*Shengchao Chen,Guodong Long,Jing Jiang*

Main category: cs.LG

TL;DR: 提出了一种名为FeDaL的联邦学习方法，通过学习数据集无关的时间序列表示，解决时间序列基础模型中的数据集异构性问题。该方法通过域偏差消除（DBE）和全局偏差消除（GBE）机制减轻偏差，并在8个任务的真实数据集上验证了其跨数据集泛化能力。


<details>
  <summary>Details</summary>
Motivation: 时间序列基础模型（TSFMs）存在的关键问题是数据集异构性带来的域偏差，这严重降低了模型的泛化能力，而这一问题尚未得到充分探索。

Method: 1. 采用联邦学习范式处理异构时间序列数据；2. 设计联邦数据集学习（FeDaL）框架，利用分布式架构将数据集分解为共享的通用知识和保留的个性化知识；3. 在TSFM架构上增加两个机制：DBE（处理局部偏差）和GBE（处理全局偏差）。

Result: 在8个任务的大规模真实数据集上进行了广泛评估（包括表示学习和下游时间序列分析），对比了54个基线模型，证明了方法的有效性。同时分析了联邦扩展行为（数据量、客户端数量、参与率）对性能的影响。

Conclusion: FeDaL成功通过联邦学习架构和偏差消除机制解决了时间序列数据集异构性问题，显著提高了时间序列基础模型的跨数据集泛化能力。针对联邦扩展行为的分析为实际应用提供了指导意义。

Abstract: Dataset-wise heterogeneity introduces significant domain biases that
fundamentally degrade generalization on Time Series Foundation Models (TSFMs),
yet this challenge remains underexplored. This paper rethink the development of
TSFMs using the paradigm of federated learning. We propose a novel Federated
Dataset Learning (FeDaL) approach to tackle heterogeneous time series by
learning dataset-agnostic temporal representations. Specifically, the
distributed architecture of federated learning is a nature solution to
decompose heterogeneous TS datasets into shared generalized knowledge and
preserved personalized knowledge. Moreover, based on the TSFM architecture,
FeDaL explicitly mitigates both local and global biases by adding two
complementary mechanisms: Domain Bias Elimination (DBE) and Global Bias
Elimination (GBE). FeDaL`s cross-dataset generalization has been extensively
evaluated in real-world datasets spanning eight tasks, including both
representation learning and downstream time series analysis, against 54
baselines. We further analyze federated scaling behavior, showing how data
volume, client count, and join rate affect model performance under
decentralization.

</details>


### [234] [Quantum Temporal Fusion Transformer](https://arxiv.org/abs/2508.04048)
*Krishnakanta Barik,Goutam Paul*

Main category: cs.LG

TL;DR: 本文提出了一种量子增强的混合量子-经典架构——量子时序融合变换模型（QTFT），用于多时间线时序预测。实验表明，该模型在某些测试案例中优于其经典版本（TFT），在其余案例中表现相当。该模型可在当前的含噪声中等规模量子（NISQ）设备上实现。


<details>
  <summary>Details</summary>
Motivation: 经典时序融合变换模型（TFT）在时序预测任务中表现出色，但其性能受到经典计算资源的限制。为了探索量子计算潜力，作者提出QTFT模型，旨在通过量子神经网络提升预测性能并对资源需求有相对宽松要求，使其适用于当前量子设备。

Method: QTFT基于经典TFT框架开发：1）采用混合量子-经典架构，设计变分量子算法实现量子化扩展；2）适用于当前NISQ设备，通过优化量子比特数和线路深度提升实用性；3）在经典TFT基础上部分组件替换为量子电路，维持其原始设计优势（如可变长度输入支持、多步预测输出等）。

Result: 1）QTFT在预测数据集上被成功训练并显示准确预测能力；2）在部分测试案例中，训练/测试损失值优于经典TFT（如降低20%损失），其余案例表现相近（差距在实验误差范围内）；3）量子组件实现参数效率提升，且计算资源消耗较经典模型具有潜在优势。

Conclusion: QTFT是首个将量子计算引入TFT框架的混合架构：1）证实在特定场景下超越经典模型性能；2）其设计方案满足当前NISQ设备的约束条件，具有工程实用潜力。未来研究方向包括更大规模数据集验证及量子优势理论分析。

Abstract: The Temporal Fusion Transformer (TFT), proposed by Lim et al.
[\textit{International Journal of Forecasting}, 2021], is a state-of-the-art
attention-based deep neural network architecture specifically designed for
multi-horizon time series forecasting. It has demonstrated significant
performance improvements over existing benchmarks. In this work, we propose a
Quantum Temporal Fusion Transformer (QTFT), a quantum-enhanced hybrid
quantum-classical architecture that extends the capabilities of the classical
TFT framework. Our results demonstrate that QTFT is successfully trained on the
forecasting datasets and is capable of accurately predicting future values. In
particular, our experimental results display that in certain test cases, the
model outperforms its classical counterpart in terms of both training and test
loss, while in the remaining cases, it achieves comparable performance. A key
advantage of our approach lies in its foundation on a variational quantum
algorithm, enabling implementation on current noisy intermediate-scale quantum
(NISQ) devices without strict requirements on the number of qubits or circuit
depth.

</details>


### [235] [Fine-tuning for Better Few Shot Prompting: An Empirical Comparison for Short Answer Grading](https://arxiv.org/abs/2508.04063)
*Joel Walsh,Siddarth Mamidanna,Benjamin Nye,Mark Core,Daniel Auerbach*

Main category: cs.LG

TL;DR: 该论文研究了使用少量数据微调大型语言模型（LLM）在自动短答案评分（ASAG）中的应用，比较了封闭模型（OpenAI）和开放权重模型（Llama）在微调与少样本提示结合下的表现。


<details>
  <summary>Details</summary>
Motivation: 传统上，微调方法需要大规模计算集群，而大多数用户无法访问这些资源。最近，出现了两种新的微调方法：OpenAI的封闭模型微调服务（只需100个样本）和使用开放权重模型（如QLORA）在消费级GPU上进行微调。本研究旨在评估这两种微调方法在ASAG任务中的效果，以及与少样本提示的结合使用情况。

Method: 1. 对封闭模型（OpenAI）和开放权重模型（Llama）采用两种微调方法：
   - OpenAI的微调服务（仅需100个样本）
   - 基于开放权重的QLORA微调（可在消费级GPU上运行）
2. 评估微调模型在自动短答案评分（ASAG）任务中的表现，重点关注其与少样本提示的交互效果。
3. 使用结构化（JSON）输出格式。
4. 在Llama 3.1 8B-Instruct模型中，通过注入大量廉价生成的合成训练数据来提升效果。

Result: 1. 对于开放权重的Llama模型，使用少量数据进行微调效果有限。
2. 对于OpenAI的封闭模型，微调方法优于少样本提示的基线指令微调LLMs。
3. 微调的收益可能受学科领域的影响（基于有限的数据集观察）。
4. 在Llama 3.1 8B-Instruct模型中，通过注入合成训练数据显著提升了性能。

Conclusion: 微调在封闭模型（OpenAI）上效果显著，但开放权重模型（Llama）需要更多数据支持。生成合成数据是一种有效提升开放权重模型性能的方法。实际应用中需考虑学科领域对微调效果的影响。

Abstract: Research to improve Automated Short Answer Grading has recently focused on
Large Language Models (LLMs) with prompt engineering and no- or few-shot
prompting to achieve best results. This is in contrast to the fine-tuning
approach, which has historically required large-scale compute clusters
inaccessible to most users. New closed-model approaches such as OpenAI's
fine-tuning service promise results with as few as 100 examples, while methods
using open weights such as quantized low-rank adaptive (QLORA) can be used to
fine-tune models on consumer GPUs. We evaluate both of these fine-tuning
methods, measuring their interaction with few-shot prompting for automated
short answer grading (ASAG) with structured (JSON) outputs. Our results show
that finetuning with small amounts of data has limited utility for Llama
open-weight models, but that fine-tuning methods can outperform few-shot
baseline instruction-tuned LLMs for OpenAI's closed models. While our
evaluation set is limited, we find some evidence that the observed benefits of
finetuning may be impacted by the domain subject matter. Lastly, we observed
dramatic improvement with the LLama 3.1 8B-Instruct open-weight model by
seeding the initial training examples with a significant amount of cheaply
generated synthetic training data.

</details>


### [236] [FLAT: Latent-Driven Arbitrary-Target Backdoor Attacks in Federated Learning](https://arxiv.org/abs/2508.04064)
*Tuan Nguyen,Khoa D Doan,Kok-Seng Wong*

Main category: cs.LG

TL;DR: 提出了一种名为FLAT的新颖后门攻击，利用潜在驱动条件自编码器生成多样化的特定目标触发器，能够在联邦学习环境中实现任意目标攻击，并规避传统检测机制。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习后门攻击大多局限于固定模式或单目标触发器，灵活性差且易被检测，需要一种更灵活、隐蔽且支持多目标的攻击方法。

Method: 利用潜在驱动的条件自编码器（latent-driven conditional autoencoder）生成多样化和目标特定的触发器；通过引入潜在码实现视觉自适应和高变异性；攻击者无需重新训练即可选择任意目标。

Result: 实验显示FLAT实现了高攻击成功率，且能抵抗先进的联邦学习防御机制；结果表明该方法在攻击成功、隐蔽性和多样性方面具有显著优势。

Conclusion: FLAT展示了潜在驱动的多目标后门攻击对联邦学习的严重威胁，凸显了开发新防御策略的紧迫性。

Abstract: Federated learning (FL) is vulnerable to backdoor attacks, yet most existing
methods are limited by fixed-pattern or single-target triggers, making them
inflexible and easier to detect. We propose FLAT (FL Arbitrary-Target Attack),
a novel backdoor attack that leverages a latent-driven conditional autoencoder
to generate diverse, target-specific triggers as needed. By introducing a
latent code, FLAT enables the creation of visually adaptive and highly variable
triggers, allowing attackers to select arbitrary targets without retraining and
to evade conventional detection mechanisms. Our approach unifies attack
success, stealth, and diversity within a single framework, introducing a new
level of flexibility and sophistication to backdoor attacks in FL. Extensive
experiments show that FLAT achieves high attack success and remains robust
against advanced FL defenses. These results highlight the urgent need for new
defense strategies to address latent-driven, multi-target backdoor threats in
federated settings.

</details>


### [237] [Adversarial Fair Multi-View Clustering](https://arxiv.org/abs/2508.04071)
*Mudi Jiang,Jiahui Zhou,Lianyu Hu,Xinying Liu,Zengyou He,Zhikui Chen*

Main category: cs.LG

TL;DR: 该论文提出了一个对抗性公平多视图聚类框架(AFMVC)，用于在保证聚类性能的同时解决多视图聚类中的公平性问题。该方法通过在特征学习阶段通过对抗训练去除敏感属性信息，确保聚类结果不受敏感属性影响。理论证明表明，通过KL散度将视图特定的聚类分配与公平不变的共识分布对齐，可以在不显著损害公平性的情况下保持聚类一致性。实验证明AFMVC在公平性和聚类性能上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前多视图聚类方法主要关注聚类性能，但忽略了算法公平性这个人类中心应用中的关键问题。现有公平多视图聚类方法依赖于敏感属性与聚类结构对齐的假设（通过正则化实现），然而这种假设在实践中常常不成立且会降低聚类性能。因此需要一种能够内在地解决公平性问题的方法。

Method: 1. 采用对抗训练机制：在特征学习阶段引入对抗性训练，从特征表示中彻底移除敏感属性信息。\n2. 提出理论保障：证明通过KL散度将视图特定的聚类分配与公平不变的共识分布对齐时，能在保持聚类一致性的同时不显著损害公平性。\n3. 设计统一框架：将公平学习整合到表示学习过程中，生成不受敏感属性影响的聚类分配。

Result: 在带有公平性约束的数据集上进行了广泛实验，结果表明：\n1. AFMVC在公平性指标上显著优于现有的多视图聚类方法和其他公平感知聚类方法。\n2. 在保持竞争力聚类性能（如聚类准确率）的同时实现了优越的公平性。\n3. 验证了理论分析的可靠性：在移除敏感属性信息后通过对齐机制实现了公平性与性能的平衡。

Conclusion: AFMVC框架通过将对抗性公平学习集成到多视图聚类中，从根本上解决了现有方法依赖敏感属性与聚类结构对齐的局限性。该方法不仅在技术上实现了对敏感属性的信息消除，还通过理论证明提供了可靠性保障。实验证实该方法在公平性指标上领先且保持竞争力的聚类性能，为公平多视图聚类提供了有效的解决方案。

Abstract: Cluster analysis is a fundamental problem in data mining and machine
learning. In recent years, multi-view clustering has attracted increasing
attention due to its ability to integrate complementary information from
multiple views. However, existing methods primarily focus on clustering
performance, while fairness-a critical concern in human-centered
applications-has been largely overlooked. Although recent studies have explored
group fairness in multi-view clustering, most methods impose explicit
regularization on cluster assignments, relying on the alignment between
sensitive attributes and the underlying cluster structure. However, this
assumption often fails in practice and can degrade clustering performance. In
this paper, we propose an adversarial fair multi-view clustering (AFMVC)
framework that integrates fairness learning into the representation learning
process. Specifically, our method employs adversarial training to fundamentally
remove sensitive attribute information from learned features, ensuring that the
resulting cluster assignments are unaffected by it. Furthermore, we
theoretically prove that aligning view-specific clustering assignments with a
fairness-invariant consensus distribution via KL divergence preserves
clustering consistency without significantly compromising fairness, thereby
providing additional theoretical guarantees for our framework. Extensive
experiments on data sets with fairness constraints demonstrate that AFMVC
achieves superior fairness and competitive clustering performance compared to
existing multi-view clustering and fairness-aware clustering methods.

</details>


### [238] [Model Inversion Attacks on Vision-Language Models: Do They Leak What They Learn?](https://arxiv.org/abs/2508.04097)
*Ngoc-Bao Nguyen,Sy-Tuyen Ho,Koh Jun Hao,Ngai-Man Cheung*

Main category: cs.LG

TL;DR: 模型反演（MI）攻击通过从训练好的神经网络中重建私有训练数据，构成重大隐私风险。本文首次研究了视觉语言模型（VLMs）在泄露私有视觉训练数据方面的脆弱性，并提出了一系列针对VLMs的基于标记和基于序列的模型反演策略，包括TMI、TMI-C、SMI和SMI-AW。实验表明，VLMs容易遭受训练数据泄露，其中提出的基于序列的方法（特别是SMI-AW结合基于词汇表征的对数最大化损失）在攻击准确率和视觉相似度上优于基于标记的方法。


<details>
  <summary>Details</summary>
Motivation: 尽管以往研究关注于传统单模态DNNs，但视觉语言模型（VLMs）的脆弱性尚未被充分探索。随着VLMs在医疗、金融等多个领域的日益普及，其隐私风险亟需评估。因此，本研究旨在探究VLMs是否容易因模型反演攻击而导致隐私数据泄露，以揭示其在广泛应用中的潜在安全威胁。

Method: 针对VLMs基于标记的生成特性，本文提出了四种模型反演攻击策略：
1. 基于标记的模型反演（TMI）：针对单个标记的反演方法。
2. 收敛的基于标记的模型反演（TMI-C）：改进的TMI方法，实现收敛。
3. 基于序列的模型反演（SMI）：扩展为序列级别的反演。
4. 自适应加权的基于序列的模型反演（SMI-AW）：在SMI基础上引入动态标记加权机制。这些方法利用不同的损失函数（如基于词汇表征的对数最大化损失）优化重建图像的质量和准确性。

Result: 
- 实验在三个SOTA的VLMs和多个数据集上进行，首次证明VLMs易受训练数据泄露的影响。
- 基于序列的方法（尤其是SMI-AW）在重建效果上优于基于标记的方法，攻击准确率和视觉相似度更高。人类评估结果显示重建图像的攻击准确率达75.31%。
- 研究还成功演示了对公开发布的VLMs的反演攻击。

Conclusion: 该研究首次揭示视觉语言模型（VLMs）存在严重的隐私风险。实验证明VLMs易受模型反演攻击影响，特别是在医疗、金融等隐私敏感场景中。本文提出的SMI-AW方法显著提升了攻击准确性。这一发现警告我们VLMs的广泛使用需要加强隐私保护措施，并开辟了更稳健的隐私防护研究的新方向。

Abstract: Model inversion (MI) attacks pose significant privacy risks by reconstructing
private training data from trained neural networks. While prior works have
focused on conventional unimodal DNNs, the vulnerability of vision-language
models (VLMs) remains underexplored. In this paper, we conduct the first study
to understand VLMs' vulnerability in leaking private visual training data. To
tailored for VLMs' token-based generative nature, we propose a suite of novel
token-based and sequence-based model inversion strategies. Particularly, we
propose Token-based Model Inversion (TMI), Convergent Token-based Model
Inversion (TMI-C), Sequence-based Model Inversion (SMI), and Sequence-based
Model Inversion with Adaptive Token Weighting (SMI-AW). Through extensive
experiments and user study on three state-of-the-art VLMs and multiple
datasets, we demonstrate, for the first time, that VLMs are susceptible to
training data leakage. The experiments show that our proposed sequence-based
methods, particularly SMI-AW combined with a logit-maximization loss based on
vocabulary representation, can achieve competitive reconstruction and
outperform token-based methods in attack accuracy and visual similarity.
Importantly, human evaluation of the reconstructed images yields an attack
accuracy of 75.31\%, underscoring the severity of model inversion threats in
VLMs. Notably we also demonstrate inversion attacks on the publicly released
VLMs. Our study reveals the privacy vulnerability of VLMs as they become
increasingly popular across many applications such as healthcare and finance.

</details>


### [239] [COPO: Consistency-Aware Policy Optimization](https://arxiv.org/abs/2508.04138)
*Jinghang Han,Jiawei Chen,Hang Shao,Hao Ma,Mingcheng Li,Xintian Shen,Lihao Zheng,Wei Chen,Tao Wei,Lihua Zhang*

Main category: cs.LG

TL;DR: 针对强化学习中当同一提示下多个响应的结果一致时，组间优势函数可能退化为零导致梯度消失的问题，提出一种基于一致性感知的策略优化框架，通过引入结构化全局奖励和熵基软混合机制，有效提升训练效率和下游性能。


<details>
  <summary>Details</summary>
Motivation: 在LLMs的推理任务中，基于规则的奖励方法（如DeepSeek-R1）在组间输出结果一致时，组间优势函数会退化为零，导致梯度消失和样本无效学习，降低训练效率和性能。

Method: 1. 设计了一个基于结果一致性的结构化全局奖励，即使组内响应完全一致也能提供有意义的学习信号；2. 引入基于熵的软混合机制，自适应平衡局部优势估计与全局优化目标，动态调整探索与收敛平衡。

Result: 在多个数学推理基准测试中实现了显著性能提升，证明了框架的鲁棒性和普适性。代码已开源。

Conclusion: 所提出的一致性感知优化框架通过全局奖励和自适应混合机制，有效解决了优势函数退化问题，推动了LLM在复杂推理任务中的强化学习应用。

Abstract: Reinforcement learning has significantly enhanced the reasoning capabilities
of Large Language Models (LLMs) in complex problem-solving tasks. Recently, the
introduction of DeepSeek R1 has inspired a surge of interest in leveraging
rule-based rewards as a low-cost alternative for computing advantage functions
and guiding policy optimization. However, a common challenge observed across
many replication and extension efforts is that when multiple sampled responses
under a single prompt converge to identical outcomes, whether correct or
incorrect, the group-based advantage degenerates to zero. This leads to
vanishing gradients and renders the corresponding samples ineffective for
learning, ultimately limiting training efficiency and downstream performance.
To address this issue, we propose a consistency-aware policy optimization
framework that introduces a structured global reward based on outcome
consistency, the global loss based on it ensures that, even when model outputs
show high intra-group consistency, the training process still receives
meaningful learning signals, which encourages the generation of correct and
self-consistent reasoning paths from a global perspective. Furthermore, we
incorporate an entropy-based soft blending mechanism that adaptively balances
local advantage estimation with global optimization, enabling dynamic
transitions between exploration and convergence throughout training. Our method
introduces several key innovations in both reward design and optimization
strategy. We validate its effectiveness through substantial performance gains
on multiple mathematical reasoning benchmarks, highlighting the proposed
framework's robustness and general applicability. Code of this work has been
released at https://github.com/hijih/copo-code.git.

</details>


### [240] [Semi-Supervised Deep Domain Adaptation for Predicting Solar Power Across Different Locations](https://arxiv.org/abs/2508.04165)
*Md Shazid Islam,A S M Jahid Hasan,Md Saydur Rahman,Md Saiful Islam Sajol*

Main category: cs.LG

TL;DR: 该论文提出了一种半监督深度领域自适应框架，用于解决太阳能发电预测中因地理位置和天气条件变化导致的领域偏移问题。该方法通过教师-学生模型配置，仅需目标域少量标记数据，即可实现高精度预测，并在三个不同目标域上取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 由于地理位置和天气特征的变化，太阳能发电预测模型在跨区域应用时面临领域偏移问题，导致模型在目标域表现下降。同时，目标域标记数据不足及存储问题进一步增加了挑战。因此，需要一种无需大量标记数据的领域自适应方法。

Method: 1. 在源域地点数据上训练深度卷积神经网络。2. 采用源数据无关(source-free)的教师-学生模型进行领域自适应：使用一致性损失和交叉熵损失进行半监督学习，仅利用目标域20%的标记数据微调模型。

Result: 方法在加利福尼亚、佛罗里达和纽约作为目标域的测试中，预测准确率分别比非自适应方法提升11.36%、6.65%和4.92%。

Conclusion: 所提出的半监督深度领域自适应框架有效解决了领域偏移问题，显著提升跨区域太阳能发电预测性能。该方法仅需少量目标域标记数据，具有实际部署价值。

Abstract: Accurate solar generation prediction is essential for proper estimation of
renewable energy resources across diverse geographic locations. However,
geographical and weather features vary from location to location which
introduces domain shift - a major bottleneck to develop location-agnostic
prediction model. As a result, a machine-learning model which can perform well
to predict solar power in one location, may exhibit subpar performance in
another location. Moreover, the lack of properly labeled data and storage
issues make the task even more challenging. In order to address domain shift
due to varying weather conditions across different meteorological regions, this
paper presents a semi-supervised deep domain adaptation framework, allowing
accurate predictions with minimal labeled data from the target location. Our
approach involves training a deep convolutional neural network on a source
location's data and adapting it to the target location using a source-free,
teacher-student model configuration. The teacher-student model leverages
consistency and cross-entropy loss for semi-supervised learning, ensuring
effective adaptation without any source data requirement for prediction. With
annotation of only $20 \%$ data in the target domain, our approach exhibits an
improvement upto $11.36 \%$, $6.65 \%$, $4.92\%$ for California, Florida and
New York as target domain, respectively in terms of accuracy in predictions
with respect to non-adaptive approach.

</details>


### [241] [One Small Step with Fingerprints, One Giant Leap for emph{De Novo} Molecule Generation from Mass Spectra](https://arxiv.org/abs/2508.04180)
*Neng Kai Nigel Neo,Lim Jing,Ngoui Yong Zhau Preston,Koh Xue Ting Serene,Bingquan Shen*

Main category: cs.LG

TL;DR: 该论文提出了一种新的质谱生成分子结构的策略，采用MIST编码器和预训练的MolForge解码器构成的双阶段流程，其中一个关键创新是通过在解码器输出指纹时使用阶跃函数进行阈值处理。该方法比之前最先进的方法提升了十倍，在质谱上实现top-1的28%和top-10的36%正确结构预测率。


<details>
  <summary>Details</summary>
Motivation: 现有的从质谱生成分子结构的两阶段方法（编码器→解码器）在效果上有局限。本研究旨在通过结合MIST编码器和预训练的MolForge解码器，并在编码器预测指纹的环节使用二值化处理，从而提高预测准确率。

Method: 1. 使用MIST编码器从质谱（mass spectra）生成向量化指纹（molecular fingerprints）。2. 将指纹通过二值处理的阶跃函数过滤，使之转为结构存在标记（0代表不存在，1代表存在），而非概率值。
   * 该操作强化了特征的存在性信号，避免了概率值的模糊性，从而提高了模型的鲁棒性。
   3. 将处理后的指纹传入预训练的MolForge解码器（一种文本序列解码器），该模型由预训练后拥有良好的指纹到文本的转码能力，从而重建分子结构（SMILES字符串）。

Result: 通过改进编码器到解码器的流程，模型结果显著超越之前的最好成绩：当输入为质谱数据时，该组合方法在top-1预测准确率达到28%，top-10达到36%准确率，对比之前方法提高了约10倍（例如过往顶级方法的准确率可能在3%左右）。

Conclusion: 该方法利用预训练的MolForge解码器和指纹输出二值化处理显著提升了分子结构生成准确性，为未来研究树立了新的基线。作者建议将该模型部署为研究起点或基线模型，推动de novo分子鉴定工作的发展方向。

Abstract: A common approach to the \emph{de novo} molecular generation problem from
mass spectra involves a two-stage pipeline: (1) encoding mass spectra into
molecular fingerprints, followed by (2) decoding these fingerprints into
molecular structures. In our work, we adopt
\textsc{MIST}~\citep{MISTgoldmanAnnotatingMetaboliteMass2023} as the encoder
and \textsc{MolForge}~\citep{ucakReconstructionLosslessMolecular2023} as the
decoder, leveraging pretraining to enhance performance. Notably, pretraining
\textsc{MolForge} proves especially effective, enabling it to serve as a robust
fingerprint-to-structure decoder. Additionally, instead of passing the
probability of each bit in the fingerprint, thresholding the probabilities as a
step function helps focus the decoder on the presence of substructures,
improving recovery of accurate molecular structures even when the fingerprints
predicted by \textsc{MIST} only moderately resembles the ground truth in terms
of Tanimoto similarity. This combination of encoder and decoder results in a
tenfold improvement over previous state-of-the-art methods, generating top-1
28\% / top-10 36\% of molecular structures correctly from mass spectra. We
position this pipeline as a strong baseline for future research in \emph{de
novo} molecule elucidation from mass spectra.

</details>


### [242] [Neural Network Training via Stochastic Alternating Minimization with Trainable Step Sizes](https://arxiv.org/abs/2508.04193)
*Chengcheng Yan,Jiawei Xu,Zheng Peng,Qingsong Wang*

Main category: cs.LG

TL;DR: 提出了一种新型优化方法SAMT，通过交替更新神经网络不同层的权重块，结合元学习自适应学习步长，以减少计算开销并提高非凸优化中的稳定性。理论证明收敛性，实验显示更少的参数更新带来更好的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络的非凸优化问题中，标准的随机梯度下降(SGD)方法需要同时更新所有参数，导致收敛不稳定且计算成本高。需要一种更高效、稳定的优化策略。

Method: 设计了分块交替最小化框架，将网络各层权重视为独立子问题更新；采用基于元学习的策略自适应训练每块的步长（支持标量、逐元素、行列级等尺度）；提供理论收敛性证明。

Result: 在多个基准测试上，SAMT比现有方法以更少的参数更新次数获得更高的泛化性能，展示了优化效果和潜力。

Conclusion: 交替更新与元学习步长的结合有效降低了计算负担，提升了训练稳定性与泛化能力，为神经网络优化提供了新方向。

Abstract: The training of deep neural networks is inherently a nonconvex optimization
problem, yet standard approaches such as stochastic gradient descent (SGD)
require simultaneous updates to all parameters, often leading to unstable
convergence and high computational cost. To address these issues, we propose a
novel method, Stochastic Alternating Minimization with Trainable Step Sizes
(SAMT), which updates network parameters in an alternating manner by treating
the weights of each layer as a block. By decomposing the overall optimization
into sub-problems corresponding to different blocks, this block-wise
alternating strategy reduces per-step computational overhead and enhances
training stability in nonconvex settings. To fully leverage these benefits,
inspired by meta-learning, we proposed a novel adaptive step size strategy to
incorporate into the sub-problem solving steps of alternating updates. It
supports different types of trainable step sizes, including but not limited to
scalar, element-wise, row-wise, and column-wise, enabling adaptive step size
selection tailored to each block via meta-learning. We further provide a
theoretical convergence guarantee for the proposed algorithm, establishing its
optimization soundness. Extensive experiments for multiple benchmarks
demonstrate that SAMT achieves better generalization performance with fewer
parameter updates compared to state-of-the-art methods, highlighting its
effectiveness and potential in neural network optimization.

</details>


### [243] [Causal Reward Adjustment: Mitigating Reward Hacking in External Reasoning via Backdoor Correction](https://arxiv.org/abs/2508.04216)
*Ruike Song,Zeen Song,Huijie Guo,Wenwen Qiang*

Main category: cs.LG

TL;DR: 为了解决外部推理系统中因奖励模型容易受到奖励黑客攻击而导致错误答案的问题，本文提出了基于因果推理的奖励调整（CRA）方法，通过消除混淆语义特征的影响来更准确地估计推理路径的真实奖励。


<details>
  <summary>Details</summary>
Motivation: 外部推理系统中，过程奖励模型（PRM）容易受到奖励黑客攻击的影响，即使逻辑错误的推理路径也能获得高分，导致错误答案。这主要归因于混淆语义特征的干扰。

Method: 研究者提出因果奖励调整（CRA）方法：1）在PRM的内部激活上训练稀疏自编码器以恢复可解释特征；2）通过后门调整法消除混淆语义特征对奖励分数的干扰。该方法无需调整策略模型或重新训练PRM。

Result: 在数学问题求解数据集上的实验表明，CRA能够有效缓解奖励黑客问题并提高最终准确性。

Conclusion: CRA是一种无需修改现有模型结构就能有效解决奖励黑客问题的新方法，通过因果推理中的后门调整技术抑制了混淆语义特征对奖励分数的错误影响，为外部推理系统的鲁棒性提供了新思路。

Abstract: External reasoning systems combine language models with process reward models
(PRMs) to select high-quality reasoning paths for complex tasks such as
mathematical problem solving. However, these systems are prone to reward
hacking, where high-scoring but logically incorrect paths are assigned high
scores by the PRMs, leading to incorrect answers. From a causal inference
perspective, we attribute this phenomenon primarily to the presence of
confounding semantic features. To address it, we propose Causal Reward
Adjustment (CRA), a method that mitigates reward hacking by estimating the true
reward of a reasoning path. CRA trains sparse autoencoders on the PRM's
internal activations to recover interpretable features, then corrects
confounding by using backdoor adjustment. Experiments on math solving datasets
demonstrate that CRA mitigates reward hacking and improves final accuracy,
without modifying the policy model or retraining PRM.

</details>


### [244] [Symmetric Behavior Regularization via Taylor Expansion of Symmetry](https://arxiv.org/abs/2508.04225)
*Lingwei Zhu,Zheng Chen,Han Wang,Yukie Nagai*

Main category: cs.LG

TL;DR: 该论文通过将对称散度引入行为正则化策略优化（BRPO），提出了一个新的离线强化学习框架。现有方法主要使用不对称散度（如KL散度）来获得解析正则化策略和实际的最小化目标。作者证明对称散度无法直接得到解析策略，且作为损失函数可能存在数值问题。为解决这些问题，他们提出使用f-散度的泰勒级数展开，证明了有限级数可以获得解析策略，并通过分解对称散度为不对称项和条件对称项来缓解数值问题。最终，作者提出了Symmetric f Actor-Critic（Sf-AC）算法，并在分布近似和MuJoCo任务上验证了其竞争力。


<details>
  <summary>Details</summary>
Motivation: 现有的离线强化学习方法在行为正则化策略优化（BRPO）中主要使用不对称散度（如KL散度），这些方法能够获得解析正则化策略和可操作的最小化目标。然而，作者观察到对称散度具有一些潜在优势（如对称性可能带来更均衡的策略约束），但直接将其应用于BRPO会面临两个主要挑战：1）无法直接得到正则化的解析策略形式；2）作为损失函数可能引入数值不稳定问题。因此，研究目标是如何克服这些挑战，使对称散度能够有效应用于BRPO框架。

Method: 论文的核心方法包括两部分：
1. **理论创新（策略正则化）**：作者利用f-散度的泰勒级数展开，在有限阶截断下证明了一种近似解析策略的存在性（定理3.2），从而解决了对称散度无法直接导出解析策略的问题。
2. **算法设计（损失函数）**：作者将对称散度分解为显式的不对称项和隐含的条件对称项。为了缓解条件对称项计算中可能出现的数值问题（如指数函数溢出），仅对该项进行泰勒展开（公式9），并与行为策略的对数比例结合构造损失函数（公式10）。综合以上两点，设计了Sf-AC算法：Actor使用有限阶泰勒展开策略（定理3.2），Critic使用改进的损失函数。

Result: 作者进行了两类实验验证：
1. **策略分布的匹配质量**（合成环境）：在二维环境中，Sf-AC（对称JS散度）比其他基线方法（如KL散度的BRPO和前向KL的BEAR）更准确地恢复了目标分布。
2. **离线强化学习性能**（D4RL MuJoCo基准）：Sf-AC使用三种对称散度（JS, Jeffreys, Triangular Discrimination）在多个任务（如hopper-medium-replay, walker2d-medium）上达到了与CQL、IQL等SOTA方法相当或更优的性能（尤其优于采用传统KL散度的BRPO）。
实验表明，对称散度可以被稳定地用于离线强化学习且具有竞争力。

Conclusion: 该论文首次在行为正则化策略优化（BRPO）框架下成功集成了对称f-散度（如JS散度），解决了对称散度面临的解析策略和数值问题。通过泰勒展开策略和损失函数分解，提出的Sf-AC算法在分布匹配和离线强化学习任务中均展现出有效性。这为在策略优化中研究更多样化的散度类型（超越传统KL散度）提供了理论基础和实用方案。对称约束可能为策略提供新的优化方向，未来可结合其他技术深入探索。

Abstract: This paper introduces symmetric divergences to behavior regularization policy
optimization (BRPO) to establish a novel offline RL framework. Existing methods
focus on asymmetric divergences such as KL to obtain analytic regularized
policies and a practical minimization objective. We show that symmetric
divergences do not permit an analytic policy as regularization and can incur
numerical issues as loss. We tackle these challenges by the Taylor series of
$f$-divergence. Specifically, we prove that an analytic policy can be obtained
with a finite series. For loss, we observe that symmetric divergences can be
decomposed into an asymmetry and a conditional symmetry term, Taylor-expanding
the latter alleviates numerical issues. Summing together, we propose Symmetric
$f$ Actor-Critic (S$f$-AC), the first practical BRPO algorithm with symmetric
divergences. Experimental results on distribution approximation and MuJoCo
verify that S$f$-AC performs competitively.

</details>


### [245] [Empowering Time Series Forecasting with LLM-Agents](https://arxiv.org/abs/2508.04231)
*Chin-Chia Michael Yeh,Vivian Lai,Uday Singh Saini,Xiran Fan,Yujie Fan,Junpeng Wang,Xin Dai,Yan Zheng*

Main category: cs.LG

TL;DR: 提出了一种数据为中心的代理DCATS，用于时间序列预测。该代理利用元数据清理数据以优化预测性能。实验表明，DCATS在交通量预测数据集上平均降低了6%的误差。


<details>
  <summary>Details</summary>
Motivation: 近期研究表明，在时间序列预测中，轻量级模型通常可以达到最先进的性能。因此，我们转向通过提升数据质量（而非模型架构）来改进AutoML系统，这是一个有前景的方向。

Method: 我们开发了DCATS（一个数据为中心的代理），它利用时间序列的元数据清理数据，同时优化预测性能。在大型交通量预测数据集上，使用四种时间序列预测模型进行评估。

Result: DCATS在所有测试模型和时间范围内实现了平均6%的错误率降低。

Conclusion: 数据为中心的方法在时间序列AutoML中有明显潜力，尤其是在提升数据质量方面的改进超过了调整模型架构。

Abstract: Large Language Model (LLM) powered agents have emerged as effective planners
for Automated Machine Learning (AutoML) systems. While most existing AutoML
approaches focus on automating feature engineering and model architecture
search, recent studies in time series forecasting suggest that lightweight
models can often achieve state-of-the-art performance. This observation led us
to explore improving data quality, rather than model architecture, as a
potentially fruitful direction for AutoML on time series data. We propose
DCATS, a Data-Centric Agent for Time Series. DCATS leverages metadata
accompanying time series to clean data while optimizing forecasting
performance. We evaluated DCATS using four time series forecasting models on a
large-scale traffic volume forecasting dataset. Results demonstrate that DCATS
achieves an average 6% error reduction across all tested models and time
horizons, highlighting the potential of data-centric approaches in AutoML for
time series forecasting.

</details>


### [246] [Automated ultrasound doppler angle estimation using deep learning](https://arxiv.org/abs/2508.04243)
*Nilesh Patil,Ajay Anand*

Main category: cs.LG

TL;DR: 提出了一种基于深度学习的自动化多普勒角度估计方法，用于超声多普勒血流速度测量中的角度校正，以减少人为错误。


<details>
  <summary>Details</summary>
Motivation: 当前在超声多普勒血流速度测量中，角度估计错误是导致测量误差的主要原因。传统靠人工操作，存在主观性和操作复杂的问题，因此需要自动化方法以提高准确性和效率。

Method: 1. 使用2100张人体颈动脉超声图像（含数据增强）训练模型。2. 采用五种预训练模型提取图像特征。3. 设计一个浅层网络处理特征并进行角度估计。4. 独立进行人工测量作为对比基准。评估指标采用平均绝对误差（MAE）。

Result: 自动化模型与人类观察者测量的MAE值在3.9°至9.4°之间，其中最佳模型MAE（3.9°）低于临床可接受的误差阈值，可避免将正常速度误判为狭窄流速。

Conclusion: 该方法证明了深度学习在自动化多普勒角度估计上的潜力，未来可集成至商业超声扫描仪的成像软件中，提高临床测量准确性和一致性。

Abstract: Angle estimation is an important step in the Doppler ultrasound clinical
workflow to measure blood velocity. It is widely recognized that incorrect
angle estimation is a leading cause of error in Doppler-based blood velocity
measurements. In this paper, we propose a deep learning-based approach for
automated Doppler angle estimation. The approach was developed using 2100 human
carotid ultrasound images including image augmentation. Five pre-trained models
were used to extract images features, and these features were passed to a
custom shallow network for Doppler angle estimation. Independently,
measurements were obtained by a human observer reviewing the images for
comparison. The mean absolute error (MAE) between the automated and manual
angle estimates ranged from 3.9{\deg} to 9.4{\deg} for the models evaluated.
Furthermore, the MAE for the best performing model was less than the acceptable
clinical Doppler angle error threshold thus avoiding misclassification of
normal velocity values as a stenosis. The results demonstrate potential for
applying a deep-learning based technique for automated ultrasound Doppler angle
estimation. Such a technique could potentially be implemented within the
imaging software on commercial ultrasound scanners.

</details>


### [247] [T3Time: Tri-Modal Time Series Forecasting via Adaptive Multi-Head Alignment and Residual Fusion](https://arxiv.org/abs/2508.04251)
*Abdul Monaf Chowdhury,Rabeya Akter,Safaeid Hossain Arib*

Main category: cs.LG

TL;DR: 提出了T3Time框架，用于多变量时间序列预测，通过时间、频谱和提示分支联合建模，提高预测精度和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前基于Transformer和LLM的预测方法在处理多变量时间序列时存在限制：固定的归纳偏差、忽略变量间交互、静态融合策略等，导致难以捕捉预测时间范围内特定关系。因此，提出T3Time框架以解决这些问题。

Method: 1. 构建三模态框架：时间分支、频谱分支（捕获周期性结构）和提示分支。2. 引入门控机制：据预测时间长度自适应调整时间和频谱特征的权重。3. 设计动态聚合机制：通过多个跨模态对齐头，动态加权各头重要性以实现特征融合。

Result: 在多个基准测试上，模型平均降低MSE 3.28%和MAE 2.29%。少量数据下：5%训练数据时，MSE下降4.13%，MAE下降1.91%；10%训练数据时，下降3.62%和1.98%。

Conclusion: T3Time框架通过动态权衡时间和频谱特征、跨模态对齐，实现性能超越现有模型，尤其少量数据下，证明其有效性和泛化能力。

Abstract: Multivariate time series forecasting (MTSF) seeks to model temporal dynamics
among variables to predict future trends. Transformer-based models and large
language models (LLMs) have shown promise due to their ability to capture
long-range dependencies and patterns. However, current methods often rely on
rigid inductive biases, ignore intervariable interactions, or apply static
fusion strategies that limit adaptability across forecast horizons. These
limitations create bottlenecks in capturing nuanced, horizon-specific
relationships in time-series data. To solve this problem, we propose T3Time, a
novel trimodal framework consisting of time, spectral, and prompt branches,
where the dedicated frequency encoding branch captures the periodic structures
along with a gating mechanism that learns prioritization between temporal and
spectral features based on the prediction horizon. We also proposed a mechanism
which adaptively aggregates multiple cross-modal alignment heads by dynamically
weighting the importance of each head based on the features. Extensive
experiments on benchmark datasets demonstrate that our model consistently
outperforms state-of-the-art baselines, achieving an average reduction of 3.28%
in MSE and 2.29% in MAE. Furthermore, it shows strong generalization in
few-shot learning settings: with 5% training data, we see a reduction in MSE
and MAE by 4.13% and 1.91%, respectively; and with 10% data, by 3.62% and 1.98%
on average. Code - https://github.com/monaf-chowdhury/T3Time/

</details>


### [248] [A Visual Tool for Interactive Model Explanation using Sensitivity Analysis](https://arxiv.org/abs/2508.04269)
*Manuela Schuler*

Main category: cs.LG

TL;DR: SAInT是一个基于Python的工具，用于通过集成的局部和全局敏感性分析来可视化和理解机器学习模型的行为。它支持人机协作工作流，让用户（AI研究人员和领域专家）无需编程即可通过交互式图形界面配置、训练、评估和解释模型。该工具自动完成模型训练和选择，提供基于方差的全局特征归因分析，并通过LIME和SHAP提供单实例解释。作者以泰坦尼克号数据集上的分类任务为例，展示了敏感性信息如何指导特征选择和数据优化。


<details>
  <summary>Details</summary>
Motivation: 当前，理解和解释机器学习模型的行为对于AI研究人员和领域专家至关重要，尤其是在需要透明度和责任感的领域。然而，现有的工具往往需要编程技能，限制了非技术用户的使用。因此，开发一个无需编程、支持交互式探索的工具，能够同时提供全局（整个模型）和局部（单个预测）的解释，有助于增强用户对模型的信任和理解，并指导实践中的决策（如特征选择和数据优化）。

Method: 1. 开发了一个图形用户界面（GUI）工具，用户无需编程即可使用。
2. 支持用户配置数据集、选择模型类型、训练模型及评估模型性能。
3. 提供全局敏感性分析（基于方差的分析），评估特征在整个数据集上对模型输出的影响。
4. 提供局部解释（使用LIME和SHAP），针对单个预测生成解释。
5. 自动化模型训练和选择过程，简化用户操作。
6. 在泰坦尼克号生存预测数据集上进行了演示，展示了特征选择和数据优化的过程。

Result: 1. 构建了SAInT工具，成功实现了交互式模型训练、评估和解释。
2. 全局敏感性分析帮助识别出整体上重要的特征（如泰坦尼克号数据集中的“性别”和“舱位等级”）。
3. 局部解释（LIME/SHAP）提供了对单个预测的透明化理解。
4. 案例展示：使用敏感性信息指导特征选择（移除不重要特征）和数据优化（如处理缺失值）后，模型性能得到提升。
5. 该工具提高了非编程用户（领域专家）参与模型开发与解释的能力。

Conclusion: SAInT提供了一种无需编程、交互式探索机器学习模型内部行为的方法，通过整合全局和局部敏感性分析，帮助用户（包括非技术背景的领域专家）理解模型、改进模型（特征工程）和建立信任。在泰坦尼克号数据集上的应用证明了其在指导特征选择和数据优化方面的有效性。该工具为模型可解释性提供了一种用户友好的解决方案，有望促进AI在更多领域的应用。

Abstract: We present SAInT, a Python-based tool for visually exploring and
understanding the behavior of Machine Learning (ML) models through integrated
local and global sensitivity analysis. Our system supports Human-in-the-Loop
(HITL) workflows by enabling users - both AI researchers and domain experts -
to configure, train, evaluate, and explain models through an interactive
graphical interface without programming. The tool automates model training and
selection, provides global feature attribution using variance-based sensitivity
analysis, and offers per-instance explanation via LIME and SHAP. We demonstrate
the system on a classification task predicting survival on the Titanic dataset
and show how sensitivity information can guide feature selection and data
refinement.

</details>


### [249] [Mockingbird: How does LLM perform in general machine learning tasks?](https://arxiv.org/abs/2508.04279)
*Haoyu Jia,Yoshiki Obinata,Kento Kawaharazuka,Kei Okada*

Main category: cs.LG

TL;DR: 提出了一个名为Mockingbird的框架，用于将大型语言模型（LLMs）适配到通用机器学习任务中，通过角色扮演和错误反思来提高模型性能。评估显示该方法可获得尚可的结果，但仅靠自我反思目前还无法超越领域专家反馈的效果。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型在推理能力和速度上的快速提升，展现出超越聊天机器人应用的潜力，本研究出于对这种潜力的好奇，旨在探索如何将LLMs应用于通用机器学习任务。

Method: 提出的Mockingbird框架核心是让LLMs扮演特定角色（函数），并通过自我反思错误来持续改进。具体流程包括：指令驱动角色扮演、执行任务、错误识别、反思修正，形成迭代优化循环。

Result: 在多个通用机器学习任务上的评估表明，Mockingbird框架能达到可接受的结果；但分析显示其仅靠自我反思的性能仍无法超越使用领域特定文档或人类专家反馈的传统方法。

Conclusion: LLM驱动的机器学习（如Mockingbird框架）在通用任务上具有潜力，但当前自我反思机制存在局限性，未来仍需结合领域知识或增强反思能力才能进一步提升效能。

Abstract: Large language models (LLMs) are now being used with increasing frequency as
chat bots, tasked with the summarizing information or generating text and code
in accordance with user instructions. The rapid increase in reasoning
capabilities and inference speed of LLMs has revealed their remarkable
potential for applications extending beyond the domain of chat bots to general
machine learning tasks. This work is conducted out of the curiosity about such
potential. In this work, we propose a framework Mockingbird to adapt LLMs to
general machine learning tasks and evaluate its performance and scalability on
several general machine learning tasks. The core concept of this framework is
instructing LLMs to role-play functions and reflect on its mistakes to improve
itself. Our evaluation and analysis result shows that LLM-driven machine
learning methods, such as Mockingbird, can achieve acceptable results on common
machine learning tasks; however, solely reflecting on its own currently cannot
outperform the effect of domain-specific documents and feedback from human
experts.

</details>


### [250] [Enhancing Vision-Language Model Training with Reinforcement Learning in Synthetic Worlds for Real-World Success](https://arxiv.org/abs/2508.04280)
*George Bredis,Stanislav Dereka,Viacheslav Sinii,Ruslan Rakhimov,Daniil Gavrilov*

Main category: cs.LG

TL;DR: 引入VL-DAC算法，通过解耦行动者-批评者（Actor-Critic）训练，在无需密集奖励环境和超参数调节的情况下，有效提升视觉语言模型（VLMs）在多种任务中的泛化能力


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型缺乏从原始视觉转化为序列语言行动的能力，之前的强化学习要么泛化能力不足，要么依赖密集奖励和低状态变化环境

Method: 提出Vision-Language Decoupled Actor-Critic (VL-DAC)算法。该算法轻量且无超参数，在多个廉价模拟器（MiniWorld, Gym-Cards, ALFWorld, WebShop）中训练单个视觉语言模型，通过解耦行动tokens的PPO更新与仅环境步层面的价值学习，提升稳定性和收敛速度

Result: 在多个基准任务中表现出优异的泛化能力：具体结果包括：在BALROG中相对提升50%，在VSI-Bench空间规划最困难部分提升5%，在VisualWebBench网络导航中提升2%，且不损害通用图像理解精度。

Conclusion: 本文首次证明，一个简单的强化学习算法可以在低价模拟器中训练视觉语言模型，同时在现实图像代理、空间推理和网络导航基准上带来可衡量的增益

Abstract: Interactive multimodal agents must convert raw visual observations into
coherent sequences of language-conditioned actions -- a capability that current
vision-language models (VLMs) still lack. Earlier reinforcement-learning (RL)
efforts could, in principle, endow VLMs with such skills, but they have seldom
tested whether the learned behaviours generalize beyond their training
simulators, and they depend either on brittle hyperparameter tuning or on
dense-reward environments with low state variability. We introduce
Vision-Language Decoupled Actor-Critic (VL-DAC), a lightweight,
hyperparameter-free RL algorithm. VL-DAC applies PPO updates to action tokens
while learning value only at the environment-step level: an arrangement, to our
knowledge, not previously explored for large VLMs or LLMs. This simple
decoupling removes unstable weighting terms and yields faster, more reliable
convergence. Training a single VLM with VL-DAC in one inexpensive simulator at
a time (MiniWorld, Gym-Cards, ALFWorld, or WebShop) already produces policies
that generalize widely: +50\% relative on BALROG (game-centric agentic
control), +5\% relative on the hardest part of VSI-Bench (spatial planning),
and +2\% on VisualWebBench (web navigation), all without degrading general
image understanding accuracy. These results provide the first evidence that a
simple RL algorithm can train VLMs entirely in cheap synthetic worlds while
delivering measurable gains on real-image agentic, spatial-reasoning, and
web-navigation benchmarks.

</details>


### [251] [WSS-CL: Weight Saliency Soft-Guided Contrastive Learning for Efficient Machine Unlearning Image Classification](https://arxiv.org/abs/2508.04308)
*Thang Duc Tran,Thai Hoang Le*

Main category: cs.LG

TL;DR: 本文提出了一种名为WSS-CL的高效机器遗忘方法，采用权重显著性和两阶段策略（遗忘阶段和对抗微调阶段），显著缩小了与"精确"遗忘的性能差距，并在监督与自监督场景下均表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有机器遗忘方法（数据为中心或基于权重）在实现精确遗忘、保持稳定性和跨域适用性方面存在挑战。为此，基于权重显著性的两阶段遗忘方案被提出。

Method: 方法分为两阶段：1) 遗忘阶段：最大化输出logits与聚合伪标签的KL散度，实现logit空间的高效遗忘；2) 对抗微调阶段：采用自监督对比学习，以缩放特征表示最大化遗忘数据与保留数据的特征空间距离——遗忘数据与其增强样本作为正样本对，保留样本作为负样本对参与对比损失计算。

Result: 实验表明，相比现有先进方法，该方法在遗忘效能上大幅提升，性能损失可忽略，且适用于监督与自监督场景。

Conclusion: WSS-CL通过聚焦关键模型参数（权重显著性）和创新的两阶段设计，解决了机器遗忘中的稳定性与精确性问题，为跨域应用提供了有效方案。

Abstract: Machine unlearning, the efficient deletion of the impact of specific data in
a trained model, remains a challenging problem. Current machine unlearning
approaches that focus primarily on data-centric or weight-based strategies
frequently encounter challenges in achieving precise unlearning, maintaining
stability, and ensuring applicability across diverse domains. In this work, we
introduce a new two-phase efficient machine unlearning method for image
classification, in terms of weight saliency, leveraging weight saliency to
focus the unlearning process on critical model parameters. Our method is called
weight saliency soft-guided contrastive learning for efficient machine
unlearning image classification (WSS-CL), which significantly narrows the
performance gap with "exact" unlearning. First, the forgetting stage maximizes
kullback-leibler divergence between output logits and aggregated pseudo-labels
for efficient forgetting in logit space. Next, the adversarial fine-tuning
stage introduces contrastive learning in a self-supervised manner. By using
scaled feature representations, it maximizes the distance between the forgotten
and retained data samples in the feature space, with the forgotten and the
paired augmented samples acting as positive pairs, while the retained samples
act as negative pairs in the contrastive loss computation. Experimental
evaluations reveal that our proposed method yields much-improved unlearning
efficacy with negligible performance loss compared to state-of-the-art
approaches, indicative of its usability in supervised and self-supervised
settings.

</details>


### [252] [Forgetting: A New Mechanism Towards Better Large Language Model Fine-tuning](https://arxiv.org/abs/2508.04329)
*Ali Taheri Ghahrizjani,Alireza Taban,Qizhou Wang,Shanshan Ye,Abdolreza Mirzaei,Tongliang Liu,Bo Han*

Main category: cs.LG

TL;DR: 本文提出了一种通过将语料库中的token分类为积极和消极token，并让模型忘记消极token，以提升监督微调（SFT）效果的机制。该机制不仅提高了模型整体性能，还促进了更多样化的模型响应。


<details>
  <summary>Details</summary>
Motivation: 监督微调（SFT）的效果高度依赖于数据质量和数量，如果数据质量不高或数量不足，可能导致性能提升有限甚至性能下降。因此，需要一种方法来减轻对高质量大数据的依赖，同时提高模型性能。

Method: 1. 将每个语料库中的token分为两类：积极token（有助于提升模型性能）和消极token（缺乏必要语义或具有误导性）；
2. 积极token采用常规方式训练；
3. 消极token通过显式遗忘机制让模型忘记，从而塑造知识边界，指导模型更精确地学习信息。

Result: 在多个成熟基准测试上的实验表明，该遗忘机制不仅提高了模型的整体性能，还能促进模型生成更多样化的响应。

Conclusion: 通过有选择地遗忘消极token，模型能够更加专注于学习有益的信息，从而在减少对数据质量和数量依赖的同时，提高SFT的效果。

Abstract: Supervised fine-tuning (SFT) plays a critical role for pretrained large
language models (LLMs), notably enhancing their capacity to acquire
domain-specific knowledge while preserving or potentially augmenting their
general-purpose capabilities. However, the efficacy of SFT hinges on data
quality as well as data volume, otherwise it may result in limited performance
gains or even degradation relative to the associated baselines. To mitigate
such reliance, we suggest categorizing tokens within each corpus into two parts
-- positive and negative tokens -- based on whether they are useful to improve
model performance. Positive tokens can be trained in common ways, whereas
negative tokens, which may lack essential semantics or be misleading, should be
explicitly forgotten. Overall, the token categorization facilitate the model to
learn less informative message, and the forgetting process shapes a knowledge
boundary to guide the model on what information to learn more precisely. We
conduct experiments on well-established benchmarks, finding that this
forgetting mechanism not only improves overall model performance and also
facilitate more diverse model responses.

</details>


### [253] [From Split to Share: Private Inference with Distributed Feature Sharing](https://arxiv.org/abs/2508.04346)
*Zihan Liu,Jiayi Wen,Shouhong Tan,Zhirun Zheng,Cheng Huang*

Main category: cs.LG

TL;DR: PrivDFS是一种新的私有推理范式，通过分布式特征共享解决现有方法在隐私和效率之间的权衡问题。它通过将输入特征划分为多个平衡份额并在多个非共谋服务器上独立推理，再安全聚合结果，从而实现在不损失准确性的情况下显著降低客户端计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有的私有推理方法面临隐私与效率的权衡：基于加密的方法提供强隐私保护但计算开销大，而效率较高的分拆推理则暴露中间特征导致容易受到反演攻击。因此，需要一种既保护隐私又高效的方法。

Method: 1. 客户端将输入特征划分为多个平衡份额。2. 各份额分配给非共谋、不通信的服务器进行独立部分推理。3. 客户端安全聚合服务器输出重建最终预测。此外，两种扩展方法：- PrivDFS-AT：利用对抗训练与基于扩散的代理攻击器强化特征划分的反演鲁棒性。- PrivDFS-KD：使用用户特定秘钥使划分策略多样化，防止基于查询的反演泛化。

Result: 在CIFAR-10和CelebA数据集上，PrivDFS在保持与深度分拆推理相当隐私保护水平的同时，将客户端计算开销降低高达100倍，且精度无损失。扩展方法PrivDFS-AT和PrivDFS-KD对扩散型同分布攻击和自适应攻击均表现出鲁棒性。

Conclusion: PrivDFS通过分布式特征共享创新性地解决了MLaaS中隐私与效率的矛盾，其核心思想‘无单一服务器掌握足够信息’加上两个扩展强化方案，为实际部署提供了可行路径。实验证明了该方法在降低计算负担的同时保持了隐私水平，并有能力抵御复杂攻击。

Abstract: Cloud-based Machine Learning as a Service (MLaaS) raises serious privacy
concerns when handling sensitive client data. Existing Private Inference (PI)
methods face a fundamental trade-off between privacy and efficiency:
cryptographic approaches offer strong protection but incur high computational
overhead, while efficient alternatives such as split inference expose
intermediate features to inversion attacks. We propose PrivDFS, a new paradigm
for private inference that replaces a single exposed representation with
distributed feature sharing. PrivDFS partitions input features on the client
into multiple balanced shares, which are distributed to non-colluding,
non-communicating servers for independent partial inference. The client
securely aggregates the servers' outputs to reconstruct the final prediction,
ensuring that no single server observes sufficient information to compromise
input privacy. To further strengthen privacy, we propose two key extensions:
PrivDFS-AT, which uses adversarial training with a diffusion-based proxy
attacker to enforce inversion-resistant feature partitioning, and PrivDFS-KD,
which leverages user-specific keys to diversify partitioning policies and
prevent query-based inversion generalization. Experiments on CIFAR-10 and
CelebA demonstrate that PrivDFS achieves privacy comparable to deep split
inference while cutting client computation by up to 100 times with no accuracy
loss, and that the extensions remain robust against both diffusion-based
in-distribution and adaptive attacks.

</details>


### [254] [Multi-Marginal Stochastic Flow Matching for High-Dimensional Snapshot Data at Irregular Time Points](https://arxiv.org/abs/2508.04351)
*Justin Lee,Behnaz Moradijamei,Heman Shakeri*

Main category: cs.LG

TL;DR: 提出一种新型的多边际随机流匹配方法（MMSFM），该方法是无模拟的得分匹配和流匹配方法的扩展，旨在处理在多时间点收集的高维生物数据的建模问题。该方法不需要降低维度，同时增强了对于不规则时间间隔的鲁棒性，并通过得分匹配防止高维空间中过拟合。


<details>
  <summary>Details</summary>
Motivation: 在定量生物学等领域，从在非常规时间点采集的有限快照观测数据中建模高维系统的演变具有挑战性。传统方法通常依赖降维技术，但这可能会过度简化动力学过程并难以捕捉非平衡系统中的关键瞬态行为。

Method: 引入多边际设置下的随机流匹配方法（MMSFM），该方法通过基于测度的样条增强了对于不规则快照时间点的鲁棒性，并利用得分匹配方法避免在高维空间中过拟合。MMSFM能够在多时间点上对齐高维数据，而不需要降维。

Result: 该方法在多种合成数据、基准数据集（包括在不均匀时间点采集的基因表达数据和图像进展任务）上进行了验证，展示了方法的广泛适用性。

Conclusion: MMSFM提供了一种有效方式来对高维数据进行建模，无需降维也能处理不规则时间点数据，并在多个任务中表现出优越性。这为解决非平衡系统中捕捉瞬态行为的问题提供了有前景的方法。

Abstract: Modeling the evolution of high-dimensional systems from limited snapshot
observations at irregular time points poses a significant challenge in
quantitative biology and related fields. Traditional approaches often rely on
dimensionality reduction techniques, which can oversimplify the dynamics and
fail to capture critical transient behaviors in non-equilibrium systems. We
present Multi-Marginal Stochastic Flow Matching (MMSFM), a novel extension of
simulation-free score and flow matching methods to the multi-marginal setting,
enabling the alignment of high-dimensional data measured at non-equidistant
time points without reducing dimensionality. The use of measure-valued splines
enhances robustness to irregular snapshot timing, and score matching prevents
overfitting in high-dimensional spaces. We validate our framework on several
synthetic and benchmark datasets, including gene expression data collected at
uneven time points and an image progression task, demonstrating the method's
versatility.

</details>


### [255] [Continual Multiple Instance Learning for Hematologic Disease Diagnosis](https://arxiv.org/abs/2508.04368)
*Zahra Ebrahimi,Raheleh Salehi,Nassir Navab,Carsten Marr,Ario Sadafi*

Main category: cs.LG

TL;DR: 这篇论文提出了一种针对多示例学习（MIL）的持续学习方法，特别适用于单细胞血液疾病诊断（如白血病检测）。该方法通过精心选择样本和实例存储到示例集中，解决了现有持续学习方法在MIL场景下效果不佳的问题，并在真实白血病实验室数据上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 实验室和临床环境的动态性导致数据流持续更新，需要持续学习来保持模型性能。然而现有持续学习方法在多示例学习（MIL）场景下（如白血病检测中的单细胞数据分析）效果不佳。

Method: 1. 提出首个针对MIL的持续学习方法（重演法）
2. 精心选择样本实例机制：综合使用实例注意力得分、到类别均值向量的距离等指标
3. 从各任务中选择单细胞实例构建示例集保存
4. 通过保留数据多样性避免灾难性遗忘

Result: 在真实白血病实验室（单月数据）的类增量场景下：
• 显著优于现有SOTA持续学习方法（具体提升未说明）
• 成为首个可行的MIL持续学习解决方案

Conclusion: 该方法支持模型适应数据分布随时间变化的情况（如疾病发生率变化/基因变异），首次为MIL领域提供了有效的持续学习途径，对临床诊断模型的实时更新具有重要价值。

Abstract: The dynamic environment of laboratories and clinics, with streams of data
arriving on a daily basis, requires regular updates of trained machine learning
models for consistent performance. Continual learning is supposed to help train
models without catastrophic forgetting. However, state-of-the-art methods are
ineffective for multiple instance learning (MIL), which is often used in
single-cell-based hematologic disease diagnosis (e.g., leukemia detection).
Here, we propose the first continual learning method tailored specifically to
MIL. Our method is rehearsal-based over a selection of single instances from
various bags. We use a combination of the instance attention score and distance
from the bag mean and class mean vectors to carefully select which samples and
instances to store in exemplary sets from previous tasks, preserving the
diversity of the data. Using the real-world input of one month of data from a
leukemia laboratory, we study the effectiveness of our approach in a class
incremental scenario, comparing it to well-known continual learning methods. We
show that our method considerably outperforms state-of-the-art methods,
providing the first continual learning approach for MIL. This enables the
adaptation of models to shifting data distributions over time, such as those
caused by changes in disease occurrence or underlying genetic alterations.

</details>


### [256] [FlexQ: Efficient Post-training INT6 Quantization for LLM Serving via Algorithm-System Co-Design](https://arxiv.org/abs/2508.04405)
*Hao Zhang,Aining Jia,Weifeng Bu,Yushu Cai,Kai Sheng,Hao Chen,Xin He*

Main category: cs.LG

TL;DR: FlexQ是一种新颖的INT6训练后量化框架，通过在权重上使用统一的6位量化并在激活层上自适应地保留8位，结合创新的GPU内核实现高效推理，在LLaMA模型上几乎保持FP16精度，同时提升推理速度和节省内存。


<details>
  <summary>Details</summary>
Motivation: 虽然现有的INT4/INT8量化降低了大型语言模型（LLMs）的内存和计算成本，但它们在准确性和效率之间难以达到最优平衡。INT6量化提供了更好的权衡，但由于现代GPU缺乏硬件支持而无法充分发挥优势。因此，本文提出FlexQ框架，旨在通过算法和系统级优化实现高效的INT6量化。

Method: 1. 使用统一的6位（INT6）权重量化所有层；2. 通过层间敏感性分析自适应地在某些层中保留8位（INT8）激活；3. 设计并实现了一个专门的高性能GPU内核，该内核通过二元张量核心（BTC）等效支持W6A6和W6A8表示的矩阵乘法，从而绕过缺乏本地INT6张量核心的问题。

Result: 1. 在LLaMA模型上，FlexQ几乎保持与FP16相当的精度（困惑度增加不超过0.05）；2. 提出的内核在LLaMA-2-70B的线性层上比ABQ-LLM平均加速1.39倍；3. 端到端推理上，与SmoothQuant相比，FlexQ提供了1.33倍的加速和1.21倍的内存节省。

Conclusion: FlexQ算法与系统创新设计相结合，是一种高效的INT6量化方案，不仅维持了模型准确性，而且在缺乏原生INT6支持的现代GPU上实现了显著的加速和内存节省，为大规模部署大型语言模型提供了实用性解决方案。

Abstract: Large Language Models (LLMs) demonstrate exceptional performance but entail
significant memory and computational costs, restricting their practical
deployment. While existing INT4/INT8 quantization reduces these costs, they
often degrade accuracy or lack optimal efficiency. INT6 quantization offers a
superior trade-off between model accuracy and inference efficiency, but lacks
hardware support in modern GPUs, forcing emulation via higher-precision
arithmetic units that limit acceleration.
  In this paper, we propose FlexQ, a novel post-training INT6 quantization
framework combining algorithmic innovation with system-level optimizations.
FlexQ employs uniform 6-bit weight quantization across all layers, with
adaptive retention of 8-bit activations in layers identified through layer-wise
sensitivity analysis. To maximize hardware efficiency, we develop a specialized
high-performance GPU kernel supporting matrix multiplication for W6A6 and W6A8
representations via Binary Tensor Core (BTC) equivalents, effectively bypassing
the lack of native INT6 tensor cores. Evaluations on LLaMA models show FlexQ
maintains near-FP16 accuracy, with perplexity increases of no more than 0.05.
The proposed kernel achieves an average 1.39$\times$ speedup over ABQ-LLM on
LLaMA-2-70B linear layers. End-to-end, FlexQ delivers 1.33$\times$ inference
acceleration and 1.21$\times$ memory savings over SmoothQuant. Code is released
at https://github.com/FlyFoxPlayer/FlexQ.

</details>


### [257] [Decoding the Multimodal Maze: A Systematic Review on the Adoption of Explainability in Multimodal Attention-based Models](https://arxiv.org/abs/2508.04427)
*Md Raisul Kibria,Sébastien Lafond,Janan Arslan*

Main category: cs.LG

TL;DR: 本文对2020年1月至2024年初期间关于多模态模型可解释性研究进行了系统文献综述，旨在为更可解释、负责任的多模态AI系统提供建议。


<details>
  <summary>Details</summary>
Motivation: 随着多模态学习的显著进展，以及可解释人工智能（XAI）需求的日益增长，需要系统性地梳理多模态模型可解释性研究的现状、挑战与方法论缺陷。

Method: 通过多维框架分析文献（包括模型架构、模态组合、解释算法和评估方法），总结现有研究的分布、技术偏好及局限性。

Result: 发现研究主要集中于视觉-语言和纯语言模型，注意力解释技术占主导但无法全面捕捉模态间交互；评估方法存在非系统性缺陷，缺乏一致性、鲁棒性和对认知因素的考量。

Conclusion: 提出标准化评估和报告实践的建议，以推动以可解释性为核心、更透明可靠的多模态AI研究。

Abstract: Multimodal learning has witnessed remarkable advancements in recent years,
particularly with the integration of attention-based models, leading to
significant performance gains across a variety of tasks. Parallel to this
progress, the demand for explainable artificial intelligence (XAI) has spurred
a growing body of research aimed at interpreting the complex decision-making
processes of these models. This systematic literature review analyzes research
published between January 2020 and early 2024 that focuses on the
explainability of multimodal models. Framed within the broader goals of XAI, we
examine the literature across multiple dimensions, including model
architecture, modalities involved, explanation algorithms and evaluation
methodologies. Our analysis reveals that the majority of studies are
concentrated on vision-language and language-only models, with attention-based
techniques being the most commonly employed for explanation. However, these
methods often fall short in capturing the full spectrum of interactions between
modalities, a challenge further compounded by the architectural heterogeneity
across domains. Importantly, we find that evaluation methods for XAI in
multimodal settings are largely non-systematic, lacking consistency,
robustness, and consideration for modality-specific cognitive and contextual
factors. Based on these findings, we provide a comprehensive set of
recommendations aimed at promoting rigorous, transparent, and standardized
evaluation and reporting practices in multimodal XAI research. Our goal is to
support future research in more interpretable, accountable, and responsible
mulitmodal AI systems, with explainability at their core.

</details>


### [258] [Matrix-Free Two-to-Infinity and One-to-Two Norms Estimation](https://arxiv.org/abs/2508.04444)
*Askar Tsyganov,Evgeny Frolov,Sergey Samsonov,Maxim Rakhuba*

Main category: cs.LG

TL;DR: 提出了在仅使用矩阵-向量乘法的矩阵无关设置中估计矩阵二到无穷范数及一到二范数的新随机算法，基于改进的Hutchinson对角估计器及其Hutch++版本，提供了两种改进方法的oracle复杂度界限。实验展示了在图像分类任务中深度神经网络训练中基于Jacobian正则化的实用性，并应用于推荐系统领域以减轻对抗攻击的影响。


<details>
  <summary>Details</summary>
Motivation: 现有方法在矩阵范数估计中可能存在效率或适用性不足的问题，特别是在矩阵规模巨大且无法显式存储的场景下（如深度学习）。需要开发高效的无矩阵算法，仅依赖矩阵-向量乘法，以支持在资源受限环境下（如GPU内存限制）实现大规模矩阵范数估计，并拓展其应用场景。

Method: 1. 提出两种改进算法：分别修改Hutchinson对角估计器（基础版）及其进阶版本Hutch++，以适应二到无穷范数和一到二范数的估计。
2. 核心机制：通过自适应选择的随机向量序列进行矩阵-向量乘法，避免显式存储矩阵。
3. 理论分析：严格证明算法收敛性，并给出oracle复杂度界限（即所需矩阵-向量乘法次数的理论上限）。

Result: 1. 理论结果：证明了新算法的收敛性及更优的oracle复杂度界限。
2. 实验验证：
   - 在图像分类任务中，成功应用Jacobian正则化提升深度网络训练效果。
   - 在推荐系统中，利用所提方法有效防御对抗攻击，提高系统鲁棒性。

Conclusion: 所提无矩阵范数估计算法在理论和实验中均表现出高效性和广泛应用潜力。不仅能高效支持深度学习的正则化需求，还能为推荐系统安全提供新解决方案，展示了矩阵范数估计在跨领域问题中的实用价值。

Abstract: In this paper, we propose new randomized algorithms for estimating the
two-to-infinity and one-to-two norms in a matrix-free setting, using only
matrix-vector multiplications. Our methods are based on appropriate
modifications of Hutchinson's diagonal estimator and its Hutch++ version. We
provide oracle complexity bounds for both modifications. We further illustrate
the practical utility of our algorithms for Jacobian-based regularization in
deep neural network training on image classification tasks. We also demonstrate
that our methodology can be applied to mitigate the effect of adversarial
attacks in the domain of recommender systems.

</details>


### [259] [Cloud Model Characteristic Function Auto-Encoder: Integrating Cloud Model Theory with MMD Regularization for Enhanced Generative Modeling](https://arxiv.org/abs/2508.04447)
*Biao Hu,Guoyin Wang*

Main category: cs.LG

TL;DR: 提出了Cloud Model Characteristic Function Auto-Encoder (CMCFAE)，这是一种将云模型集成到Wasserstein自编码器（WAE）框架中的新型生成模型。通过利用云模型的特征函数对潜在空间进行正则化，该方法能更准确地建模复杂数据分布。与依赖标准高斯先验和传统散度度量的传统方法不同，该方法采用云模型先验，提供更灵活和真实的潜在空间表示，从而减轻重构样本中的同质化现象。在多个数据集上的实验表明，CMCFAE在重构质量、潜在空间结构和样本多样性方面优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 传统生成模型（如基于WAE的方法）通常使用标准高斯先验，这可能导致重构样本的同质化问题，且不能很好地捕捉复杂数据分布。云模型能更好地描述不确定性，因此将其引入生成模型框架有望提高对复杂分布的建模能力。

Method: 1. 将云模型集成到Wasserstein自编码器（WAE）框架中，形成了CMCFAE模型。
2. 推导云模型的特征函数，并在WAE框架中提出相应的正则化项（即云模型特征函数正则化）。
3. 模型的训练目标是：将输入数据编码到潜在空间，再解码重构；同时，潜在空间的分布通过云模型特征函数进行正则化，使其符合云模型先验（而非传统高斯先验）。

Result: 在MNIST, FashionMNIST, CIFAR-10和CelebA等多个数据集上进行了定量和定性评估：
1. 重构质量：优于现有模型。
2. 潜在空间结构：更优的潜在空间组织。
3. 样本多样性：生成的样本多样性更高，减轻了同质化问题。

Conclusion: CMCFAE成功地将云模型理论引入MMD（最大均值差异）正则化框架，为增强基于自编码器的生成模型提供了新的思路，并在实验中验证了其优越性。

Abstract: We introduce Cloud Model Characteristic Function Auto-Encoder (CMCFAE), a
novel generative model that integrates the cloud model into the Wasserstein
Auto-Encoder (WAE) framework. By leveraging the characteristic functions of the
cloud model to regularize the latent space, our approach enables more accurate
modeling of complex data distributions. Unlike conventional methods that rely
on a standard Gaussian prior and traditional divergence measures, our method
employs a cloud model prior, providing a more flexible and realistic
representation of the latent space, thus mitigating the homogenization observed
in reconstructed samples. We derive the characteristic function of the cloud
model and propose a corresponding regularizer within the WAE framework.
Extensive quantitative and qualitative evaluations on MNIST, FashionMNIST,
CIFAR-10, and CelebA demonstrate that CMCFAE outperforms existing models in
terms of reconstruction quality, latent space structuring, and sample
diversity. This work not only establishes a novel integration of cloud model
theory with MMD-based regularization but also offers a promising new
perspective for enhancing autoencoder-based generative models.

</details>


### [260] [Automatic LLM Red Teaming](https://arxiv.org/abs/2508.04451)
*Roman Belaire,Arunesh Sinha,Pradeep Varakantham*

Main category: cs.LG

TL;DR: 该论文提出了一种新的红队方法，通过训练一个AI智能体使用分层强化学习（HRL）来战略性地攻击另一个AI模型，以发现大型语言模型（LLMs）中的漏洞。该方法将红队任务建模为马尔可夫决策过程（MDP），克服了稀疏奖励和多回合交互的挑战，从而能够生成连贯的多回合攻击策略，并在性能上超越了现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 当前针对大型语言模型（LLMs）的自动化红队方法依赖于脆弱的提示模板或单次攻击，无法捕捉现实世界中对抗性对话的复杂交互性。为了解决这一问题，作者提出了训练一个AI智能体来策略性地“攻破”另一个AI的想法，以更有效地识别LLMs的漏洞，提高AI部署的鲁棒性。

Method: 作者将红队任务形式化为马尔可夫决策过程（MDP），并采用分层强化学习（HRL）框架进行训练。该方法使用基于token的细粒度有害性奖励函数，引导智能体学习生成连贯的多回合攻击策略。这种分层结构能够有效解决长时序任务中的稀疏奖励问题。

Result: 所提出的方法在攻击大型语言模型时超越了现有基线方法，能够发现传统方法遗漏的潜在漏洞。该方法为红队任务建立了新的最优标准，将LLM红队测试重新定义为基于动态轨迹的过程（而不是一步测试）。

Conclusion: 通过将红队任务建模为MDP并应用分层强化学习训练攻击智能体，论文证明了一种高效发现LLM漏洞的新范式。这种动态轨迹驱动的红队方法远比传统单步攻击更有效，对实现安全可靠的AI部署具有重要意义。

Abstract: Red teaming is critical for identifying vulnerabilities and building trust in
current LLMs. However, current automated methods for Large Language Models
(LLMs) rely on brittle prompt templates or single-turn attacks, failing to
capture the complex, interactive nature of real-world adversarial dialogues. We
propose a novel paradigm: training an AI to strategically `break' another AI.
By formalizing red teaming as a Markov Decision Process (MDP) and employing a
hierarchical Reinforcement Learning (RL) framework, we effectively address the
inherent sparse reward and long-horizon challenges. Our generative agent learns
coherent, multi-turn attack strategies through a fine-grained, token-level harm
reward, enabling it to uncover subtle vulnerabilities missed by existing
baselines. This approach sets a new state-of-the-art, fundamentally reframing
LLM red teaming as a dynamic, trajectory-based process (rather than a one-step
test) essential for robust AI deployment.

</details>


### [261] [Small transformer architectures for task switching](https://arxiv.org/abs/2508.04461)
*Claudius Gros*

Main category: cs.LG

TL;DR: 研究表明，在任务切换场景中，传统的注意力机制在小型应用上表现不佳。通过实验比较了Transformer、LSTM、MLP以及改进架构（非平移不变的cisformer和extensive attention）的性能，发现后者结合后能在IARC任务上达到95%的准确率，揭示了改进注意力机制的新途径。


<details>
  <summary>Details</summary>
Motivation: 当前大规模生成式AI主要依赖注意力机制，但在小型应用（如任务切换场景）中，注意力机制是否优于传统方法（如MLP或RNN）尚不明确。本文旨在探索任务切换框架下不同模型的表现，特别关注基本算术任务（IARC）中模型的能力短板，并寻求改进方案。

Method: 1. 提出任务切换框架：模型处理包含随机控制令牌的连续令牌序列，控制令牌指示当前子任务（增量/加法/反向复制/上下文，即IARC）。
2. 构建基准测试：设计基于有限域算术的参考模型，要求模型根据控制令牌动态切换任务。
3. 模型对比：测试标准Transformer、LSTM、MLP、改进版Transformer（非平移不变的cisformer）及替代注意力机制（extensive attention）的性能。
4. 组合优化：将cisformer与extensive attention结合，形成新架构。

Result: 1. 标准Transformer、LSTM和MLP在IARC任务上表现相似，仅达到中等预测精度（具体数值未提，但远低于95%）。
2. 单独使用cisformer或extensive attention未报告显著提升。
3. 结合cisformer与extensive attention的模型显著优于其他方法，达到约95%的准确率。

Conclusion: 任务切换场景暴露了标准Transformer的局限性，但通过引入非平移不变架构（cisformer）和强化令牌间关联的注意力机制（extensive attention）可大幅提升性能。这表明在特定场景下，改造注意力机制的结构（如打破平移不变性）能更好地捕捉任务切换依赖关系，为理解及改进注意力机制提供新方向。

Abstract: The rapid progress seen in terms of large-scale generative AI is largely
based on the attention mechanism. It is conversely non-trivial to conceive
small-scale applications for which attention-based architectures outperform
traditional approaches, such as multi-layer perceptrons or recurrent networks.
We examine this problem in the context of 'task switching'. In this framework
models work on ongoing token sequences with the current task being determined
by stochastically interspersed control tokens. We show that standard
transformers cannot solve a basic task switching reference model based on
finite domain arithmetics which contains subtasks dedicated to increment /
addition / reverse copy / context (IARC). We show that transformers, long
short-term memory recurrent networks (LSTM), and plain multi-layer perceptrons
(MLPs) achieve similar, but only modest prediction accuracies. We enlarge our
comparative study by including an extension of the standard transformer
architecture to its non-translational invariant counterpart, the cisformer, and
an alternative attention mechanism, extensive attention. A combination of the
latter is found to be the only model able to achieve considerable performance
levels, of around 95%. Our results indicate that the workings of attention can
be understood better, and even improved, when comparing qualitatively different
formulations in task-switching settings.

</details>


### [262] [CARD: Cache-Assisted Parallel Speculative Decoding for Efficient Large Language Model Inference](https://arxiv.org/abs/2508.04462)
*Enyu Zhou,Kai Sheng,Hao Chen,Xin He*

Main category: cs.LG

TL;DR: 本文提出了一种名为CARD的并行推测解码框架，通过解耦起草和验证过程，利用'查询-修正'范式提升大语言模型推理速度，在无需微调的情况下实现了最高4.83倍加速。


<details>
  <summary>Details</summary>
Motivation: 现有推测解码方法受限于'起草-验证'范式：1. 强制顺序执行导致低效；2. 候选序列中单个令牌被拒会导致整个后续序列丢弃；3. 限制起草模型规模。

Method: 1. 采用缓存机制解耦起草与验证：起草模型生成候选令牌填充共享缓存；2. 目标模型并行修正起草方向；3. 使目标模型推理速度接近起草模型。

Result: 实验显示CARD比原始解码速度提升最高达4.83倍，且无需微调起草/目标模型。代码已开源。

Conclusion: CARD框架通过并行化与缓存机制突破传统范式限制，显著提升LLM推理效率，为推测解码开辟了新方向。

Abstract: Speculative decoding (SD), where an extra draft model first provides multiple
draft tokens and the original target model then verifies these tokens in
parallel, has shown great power for LLM inference acceleration. However,
existing SD methods must adhere to the 'draft-then-verify' paradigm, which
forces drafting and verification processes to execute sequentially during SD,
resulting in inefficient inference performance and limiting the size of the
draft model. Furthermore, once a single token in the candidate sequence is
rejected during the drafting process, all subsequent candidate tokens must be
discarded, leading to inefficient drafting. To address these challenges, we
propose a cache-based parallel speculative decoding framework employing a
'query-and-correct' paradigm. Specifically, CARD decouples drafting and
verification: the draft model generates candidate tokens to populate a shared
cache, while the target model concurrently rectifies the draft model's
generation direction. This effectively enables the target model to perform
inference at speed approaching that of the draft model. Our approach achieves
up to 4.83 speedup over vanilla decoding without requiring fine-tuning of
either the draft or target models. Our code is available at
https://github.com/hunzhizi/CARD.

</details>


### [263] [GFocal: A Global-Focal Neural Operator for Solving PDEs on Arbitrary Geometries](https://arxiv.org/abs/2508.04463)
*Fangzhi Fei,Jiaxin Hu,Qiaofeng Li,Zhenyu Liu*

Main category: cs.LG

TL;DR: 提出了一个名为GFocal的基于Transformer的神经算子方法，它同时整合全局和局部特征学习，通过在Nyström注意力的全局块和基于切片的焦点块中利用全局关联和局部特征，并通过卷积门控块动态融合多尺度信息。该方法在多个基准测试中表现优异，并成功应用于工业级模拟。


<details>
  <summary>Details</summary>
Motivation: 现有的基于Transformer的神经算子方法忽视了局部物理细节与全局特征之间的相互依赖性的协同学习，导致在处理多尺度问题、保持长期预测的物理一致性和数值稳定性以及准确捕捉过渡动力学方面存在不足。

Method: GFocal通过Nyström注意力的全局块捕获全局关联，利用基于切片的焦点块聚焦局部物理细节，两者生成物理感知的令牌，然后通过卷积门控块对多尺度信息进行动态调制和融合。

Result: 在六个基准测试中的五个取得了当前最领先的性能，相对改进平均达到15.2%；在工业级模拟（如汽车和翼型的气动模拟）中也表现优异。

Conclusion: GFocal能够同时学习和融合全局与局部特征，使其能够准确建模和预测任意几何和初始条件下的物理特征，并在多个基准测试和工业级应用中取得成功。

Abstract: Transformer-based neural operators have emerged as promising surrogate
solvers for partial differential equations, by leveraging the effectiveness of
Transformers for capturing long-range dependencies and global correlations,
profoundly proven in language modeling. However, existing methodologies
overlook the coordinated learning of interdependencies between local physical
details and global features, which are essential for tackling multiscale
problems, preserving physical consistency and numerical stability in long-term
rollouts, and accurately capturing transitional dynamics. In this work, we
propose GFocal, a Transformer-based neural operator method that enforces
simultaneous global and local feature learning and fusion. Global correlations
and local features are harnessed through Nystr\"{o}m attention-based
\textbf{g}lobal blocks and slices-based \textbf{focal} blocks to generate
physics-aware tokens, subsequently modulated and integrated via
convolution-based gating blocks, enabling dynamic fusion of multiscale
information. GFocal achieves accurate modeling and prediction of physical
features given arbitrary geometries and initial conditions. Experiments show
that GFocal achieves state-of-the-art performance with an average 15.2\%
relative gain in five out of six benchmarks and also excels in industry-scale
simulations such as aerodynamics simulation of automotives and airfoils.

</details>


### [264] [FedHiP: Heterogeneity-Invariant Personalized Federated Learning Through Closed-Form Solutions](https://arxiv.org/abs/2508.04470)
*Jianheng Tang,Zhirui Yang,Jingchao Wang,Kejia Fan,Jinfeng Xu,Huiping Zhuang,Anfeng Liu,Houbing Herbert Song,Leye Wang,Yunhuai Liu*

Main category: cs.LG

TL;DR: 本文提出了FedHiP，一种异质性不变的个性化联邦学习方案，它通过使用闭式解（而非基于梯度的更新）来克服非独立同分布（non-IID）数据对个性化联邦学习的负面影响。方法包含三个分析阶段，实验证明其性能优于现有方法至少5.79%-20.97%。


<details>
  <summary>Details</summary>
Motivation: 现有的个性化联邦学习方法在面临普遍存在的数据异质性（non-IID数据）时，由于依赖基于梯度的更新而出现性能下降和收敛困难的问题。本文旨在从根本上解决这一挑战，避免基于梯度的更新所带来的敏感性问题。

Method: FedHiP方案分三个阶段：1. 分析性局部训练：利用自监督预训练的基础模型（冻结）进行特征提取，然后使用闭式解训练一个分类器；2. 分析性全局聚合：在各客户端的局部特征嵌入空间上进行全局聚合；3. 分析性局部个性化：每个客户端获得全局模型后，使用闭式解在其本地数据上微调分类器。整个过程中避免了基于梯度的更新。

Result: 在多个基准数据集上的实验显示，FedHiP方案显著优于现有方法（包括PerFedAvg、FedRep、FedBABU、APPLE等），在准确率上超出至少5.79%到20.97%，证明了其异质性不变性的优越性能。

Conclusion: 通过避免基于梯度的更新，FedHiP方案实现了异质性不变性（即无论数据如何非IID分布，个性化模型保持相同），有效地解决了非IID数据下的性能下降问题，为个性化联邦学习提供了一条基于闭式解的新路径。

Abstract: Lately, Personalized Federated Learning (PFL) has emerged as a prevalent
paradigm to deliver personalized models by collaboratively training while
simultaneously adapting to each client's local applications. Existing PFL
methods typically face a significant challenge due to the ubiquitous data
heterogeneity (i.e., non-IID data) across clients, which severely hinders
convergence and degrades performance. We identify that the root issue lies in
the long-standing reliance on gradient-based updates, which are inherently
sensitive to non-IID data. To fundamentally address this issue and bridge the
research gap, in this paper, we propose a Heterogeneity-invariant Personalized
Federated learning scheme, named FedHiP, through analytical (i.e., closed-form)
solutions to avoid gradient-based updates. Specifically, we exploit the trend
of self-supervised pre-training, leveraging a foundation model as a frozen
backbone for gradient-free feature extraction. Following the feature extractor,
we further develop an analytic classifier for gradient-free training. To
support both collective generalization and individual personalization, our
FedHiP scheme incorporates three phases: analytic local training, analytic
global aggregation, and analytic local personalization. The closed-form
solutions of our FedHiP scheme enable its ideal property of heterogeneity
invariance, meaning that each personalized model remains identical regardless
of how non-IID the data are distributed across all other clients. Extensive
experiments on benchmark datasets validate the superiority of our FedHiP
scheme, outperforming the state-of-the-art baselines by at least 5.79%-20.97%
in accuracy.

</details>


### [265] [Who cuts emissions, who turns up the heat? causal machine learning estimates of energy efficiency interventions](https://arxiv.org/abs/2508.04478)
*Bernardino D'Amico,Francesco Pomponi,Jay H. Arehart,Lina Khaddour*

Main category: cs.LG

TL;DR: 研究利用基于英格兰住房数据的因果机器学习模型，分析了墙体保温对燃气消费的因果影响，发现平均可节省高达19%的能源，但高能源负担家庭的节能效果微弱或为零。


<details>
  <summary>Details</summary>
Motivation: 减少家庭能源需求对气候变化和解决燃料贫困至关重要，但这取决于家庭对不同节能干预措施的异质性响应。

Method: 通过基于英格兰住房数据的因果机器学习模型，评估墙体保温的平均处理效应(ATE)和条件处理效应(CATE)，重点分析不同能源负担群体的分布效应。

Result: 平均而言，墙体保温可使燃气需求减少多达19%，但高能源负担群体几乎没有减少燃气消耗。其背后的行为机制是：高能源负担家庭将节省的能源成本转化为提高热舒适度，而不降低总消费。

Conclusion: 应建立更广泛的政策评估框架，兼顾气候影响和能源政策公平性；高能源负担群体响应具有理性调整性质，可能改善健康和福祉。

Abstract: Reducing domestic energy demand is central to climate mitigation and fuel
poverty strategies, yet the impact of energy efficiency interventions is highly
heterogeneous. Using a causal machine learning model trained on nationally
representative data of the English housing stock, we estimate average and
conditional treatment effects of wall insulation on gas consumption, focusing
on distributional effects across energy burden subgroups. While interventions
reduce gas demand on average (by as much as 19 percent), low energy burden
groups achieve substantial savings, whereas those experiencing high energy
burdens see little to no reduction. This pattern reflects a
behaviourally-driven mechanism: households constrained by high costs-to-income
ratios (e.g. more than 0.1) reallocate savings toward improved thermal comfort
rather than lowering consumption. Far from wasteful, such responses represent
rational adjustments in contexts of prior deprivation, with potential
co-benefits for health and well-being. These findings call for a broader
evaluation framework that accounts for both climate impacts and the equity
implications of domestic energy policy.

</details>


### [266] [Emotion Detection Using Conditional Generative Adversarial Networks (cGAN): A Deep Learning Approach](https://arxiv.org/abs/2508.04481)
*Anushka Srivastava*

Main category: cs.LG

TL;DR: 提出一种基于条件生成对抗网络（cGANs）的多模态情绪检测方法，整合文本、音频和面部表情数据。该方法通过生成合成数据提升了分类准确率，实验显示其在情绪识别上优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 传统单模态情绪检测方法仅依赖单一数据类型（如文本、音频或视觉），缺乏对复杂情绪的多角度捕捉能力。本文旨在探索多模态融合框架，利用cGANs生成合成数据以克服数据稀缺性并增强模型的情绪理解能力。

Method: 1. 构建多模态数据集（文本、音频、面部表情）；2. 设计条件生成对抗网络（cGAN）架构：生成器接收多模态输入并合成情绪增强数据，判别器区分真实与合成数据；3. 联合训练生成器和判别器，通过对抗损失优化模型；4. 使用生成的合成数据扩充训练集，训练多模态情绪分类器。

Result: 在公开多模态情绪数据集（如CMU-MOSEI）上测试：1. 分类准确率提升约6%-8%（对比单模态基线）；2. 生成数据有效缓解训练样本不足问题；3. 消融实验证明多模态融合贡献最大性能增益。具体指标：F1-score达0.82（基线为0.76）。

Conclusion: cGAN生成的多模态合成数据能显著提升情绪识别性能，验证了多模态融合对捕捉情绪复杂性的重要性。该框架为人机交互系统提供了更精细的情绪理解能力，未来可扩展到实时交互场景和细粒度情绪分类任务。

Abstract: This paper presents a deep learning-based approach to emotion detection using
Conditional Generative Adversarial Networks (cGANs). Unlike traditional
unimodal techniques that rely on a single data type, we explore a multimodal
framework integrating text, audio, and facial expressions. The proposed cGAN
architecture is trained to generate synthetic emotion-rich data and improve
classification accuracy across multiple modalities. Our experimental results
demonstrate significant improvements in emotion recognition performance
compared to baseline models. This work highlights the potential of cGANs in
enhancing human-computer interaction systems by enabling more nuanced emotional
understanding.

</details>


### [267] [Hierarchical Scoring for Machine Learning Classifier Error Impact Evaluation](https://arxiv.org/abs/2508.04489)
*Erin Lanus,Daniel Wolodkin,Laura J. Freeman*

Main category: cs.LG

TL;DR: 本文提出了一种分层评分指标，用于评估机器学习和目标检测中的分类性能。传统的分类模型通常使用通过/失败的方式评估分类正确性，但这将所有误分类视为等效。相比之下，本文引入的分层评分指标利用类标签的层次结构，通过评分树来衡量预测标签与真实标签之间的距离，从而为误分类提供部分得分，使得误差评估更加细化。


<details>
  <summary>Details</summary>
Motivation: 在机器学习和目标检测领域，通常使用简单的正确或错误来评估分类性能。这忽略了类标签之间可能存在的层次关系，例如某些类别之间的相似性（如犬种分类中误判为其他犬种的错误与误判为猫的错误程度不同）。如果模型在相似类别上犯错，其错误的重要性可能较低。本文的动机是为分类模型提供一种更精细的错误评估方法，通过对错误类型赋予不同的权重，从而更全面地理解模型表现。

Method: 1. 对类标签定义一种层次结构（称为评分树），树上的节点代表标签类别，边表示类别关系（如属于相同父类的为相似类）。
2. 设计分层评分指标，衡量预测标签与真实标签在树上的距离（如路径距离、共同祖先深度等），距离越小则部分得分越高。
3. 提出三种评分树权重策略（如均匀权重、按分支深度权重或按分支节点数权重），以调整不同误分类的严重程度。
4. 在一个抽象用例（例如类标签有层次结构的分类任务）上进行演示，通过评分指标计算每对预测结果的分值并聚合为模型整体性能。

Result: 使用抽象用例验证指标有效性：
- 分层评分能够区分不同程度的错误（如将虎误判为猫比误判为汽车得到更高的部分得分）。
- 三种权重策略导致不同评分结果（如均匀权重对同分支的错误处罚较轻，深度权重可能更强调靠近根节点的错误）。
- 证明这些指标可以捕获错误分布特征，使模型评估超越仅仅计算错误数量，同时考虑错误类型。

Conclusion: 1. 传统分类评估（准确率）无法考虑标签间的关系，分层指标通过部分学分提供了更细粒度的评估。
2. 提出的评分树与指标设计灵活（支持三种权重策略），可实现差异化误差分析。
3. 未来工作：在真实数据集上应用（如医学图像分诊系统考虑疾病严重性），集成到ML框架（如sklearn）中广泛使用，进一步研究自动构建评分树的方法。

Abstract: A common use of machine learning (ML) models is predicting the class of a
sample. Object detection is an extension of classification that includes
localization of the object via a bounding box within the sample.
Classification, and by extension object detection, is typically evaluated by
counting a prediction as incorrect if the predicted label does not match the
ground truth label. This pass/fail scoring treats all misclassifications as
equivalent. In many cases, class labels can be organized into a class taxonomy
with a hierarchical structure to either reflect relationships among the data or
operator valuation of misclassifications. When such a hierarchical structure
exists, hierarchical scoring metrics can return the model performance of a
given prediction related to the distance between the prediction and the ground
truth label. Such metrics can be viewed as giving partial credit to predictions
instead of pass/fail, enabling a finer-grained understanding of the impact of
misclassifications. This work develops hierarchical scoring metrics varying in
complexity that utilize scoring trees to encode relationships between class
labels and produce metrics that reflect distance in the scoring tree. The
scoring metrics are demonstrated on an abstract use case with scoring trees
that represent three weighting strategies and evaluated by the kind of errors
discouraged. Results demonstrate that these metrics capture errors with finer
granularity and the scoring trees enable tuning. This work demonstrates an
approach to evaluating ML performance that ranks models not only by how many
errors are made but by the kind or impact of errors. Python implementations of
the scoring metrics will be available in an open-source repository at time of
publication.

</details>


### [268] [Causal Reflection with Language Models](https://arxiv.org/abs/2508.04495)
*Abi Aryan,Zac Liu*

Main category: cs.LG

TL;DR: 本文介绍了一个名为Causal Reflection（因果反思）的框架，该框架通过显式建模状态、动作、时间和干扰之间的因果关系，增强智能体的因果推理能力；同时定义了一个Reflect机制，用于识别预测与实际结果间的差异，并生成因果假设以修订内部模型。该架构利用大语言模型（LLMs）作为结构化推理引擎将因果输出转化为自然语言解释，为构建具有适应性、自我修正能力并能表达因果理解的智能体提供了理论基础


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）虽具有事实回忆能力但缺乏因果推理的鲁棒性，容易依赖伪相关性；同时传统强化学习智能体也因缺乏因果建模只能优化奖励而忽略行动与结果的因果链条。为克服这一局限性，作者提出通过建立显式因果框架改进智能体的推理能力

Method: 1. 提出Causal Reflection框架：动态建模状态、动作、时间和干扰之间的因果关系（Causal函数）\n2. 定义Reflect机制：通过比对预测和实际结果识别因果失配，生成假设来修正内部模型\n3. 在架构中部署LLMs：赋予模型将形式化因果输出转化为自然语言解释和反事实推理的能力

Result: 提出了一种创新理论框架，该框架为构建能够理解、修正和沟通因果关系的智能体奠定了基础（具体实验结果未列出）

Conclusion: Causal Reflection框架整合了动态因果建模与自动修正机制，利用LLMs的自然语言表达优势，为开发适应性更强、具备自我修正能力及因果表达能力的智能体提供了新路径

Abstract: While LLMs exhibit impressive fluency and factual recall, they struggle with
robust causal reasoning, often relying on spurious correlations and brittle
patterns. Similarly, traditional Reinforcement Learning agents also lack causal
understanding, optimizing for rewards without modeling why actions lead to
outcomes. We introduce Causal Reflection, a framework that explicitly models
causality as a dynamic function over state, action, time, and perturbation,
enabling agents to reason about delayed and nonlinear effects. Additionally, we
define a formal Reflect mechanism that identifies mismatches between predicted
and observed outcomes and generates causal hypotheses to revise the agent's
internal model. In this architecture, LLMs serve not as black-box reasoners,
but as structured inference engines translating formal causal outputs into
natural language explanations and counterfactuals. Our framework lays the
theoretical groundwork for Causal Reflective agents that can adapt,
self-correct, and communicate causal understanding in evolving environments.

</details>


### [269] [PRISM: Lightweight Multivariate Time-Series Classification through Symmetric Multi-Resolution Convolutional Layers](https://arxiv.org/abs/2508.04503)
*Federico Zucchi,Thomas Lampert*

Main category: cs.LG

TL;DR: PRISM（Per-channel Resolution-Informed Symmetric Module）是一种新型的卷积特征提取器，用于多变量时间序列分类，它使用多时间尺度的对称有限脉冲响应（FIR）滤波器独立处理每个通道，减少了模型复杂性和参数量，同时保持或超过CNN和Transformer基线的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的Transformer和CNN模型在多变量时间序列分类中存在模型计算量重、频率多样性受限且参数过多的问题。希望设计一种更高效且具有强频率选择性的替代方案。

Method: 提出PRISM方法：独立处理每个通道，应用多个时间尺度的对称有限脉冲（FIR）滤波器进行特征提取。该方法不进行通道间卷积，从而大幅减少模型大小和复杂度。整个流程包括PRISM特征提取器与轻量分类器组合。

Result: 在人类活动、睡眠分期和生物医学基准测试中，PRISM配合轻量级分类头，性能匹配或优于领先的CNN和Transformer基线模型，同时参数和FLOPs减少约一个数量级。

Conclusion: 将经典信号处理（如FIR滤波）与现代深度学习结合，PRISM提供了一种准确且资源高效的多变量时间序列分类解决方案。

Abstract: Multivariate time-series classification is pivotal in domains ranging from
wearable sensing to biomedical monitoring. Despite recent advances,
Transformer- and CNN-based models often remain computationally heavy, offer
limited frequency diversity, and require extensive parameter budgets. We
propose PRISM (Per-channel Resolution-Informed Symmetric Module), a
convolutional-based feature extractor that applies symmetric
finite-impulse-response (FIR) filters at multiple temporal scales,
independently per channel. This multi-resolution, per-channel design yields
highly frequency-selective embeddings without any inter-channel convolutions,
greatly reducing model size and complexity. Across human-activity, sleep-stage
and biomedical benchmarks, PRISM, paired with lightweight classification heads,
matches or outperforms leading CNN and Transformer baselines, while using
roughly an order of magnitude fewer parameters and FLOPs. By uniting classical
signal processing insights with modern deep learning, PRISM offers an accurate,
resource-efficient solution for multivariate time-series classification.

</details>


### [270] [Channel-Independent Federated Traffic Prediction](https://arxiv.org/abs/2508.04517)
*Mo Zhang,Xiaoyu Li,Bin Xu,Meng Chen,Yongshun Gong*

Main category: cs.LG

TL;DR: 提出了一种名为Fed-CI的新型联邦交通预测框架，通过通道独立范式(CIP)实现无需客户端间通信的高效预测，显著降低通信开销并提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有联邦交通预测方法依赖客户端间通信以提高精度，导致较大通信开销和训练延迟；为应对日益增长的数据量和隐私约束，需要设计更高效且无需直接数据共享的解决方案。

Method: 1. 提出通道独立范式(CIP)，使各节点仅需本地信息即可预测，免除客户端通信需求；2. 基于CIP开发Fed-CI框架：各客户端独立处理数据，利用特定机制缓解无直接数据共享造成的信息损失（如局部模型设计），同时降低通信成本。

Result: 在多个真实数据集上证明Fed-CI的优越性：RMSE/MAE/MAPE分别提升8%/14%/16%，且大幅降低通信成本；在所有数据集和联邦设置下均优于现有方法。

Conclusion: Fed-CI首次实现无通信的联邦交通预测，在保障隐私前提下显著提升效率与精度；其CIP范式为高开销问题提供可持续解决方案。

Abstract: In recent years, traffic prediction has achieved remarkable success and has
become an integral component of intelligent transportation systems. However,
traffic data is typically distributed among multiple data owners, and privacy
constraints prevent the direct utilization of these isolated datasets for
traffic prediction. Most existing federated traffic prediction methods focus on
designing communication mechanisms that allow models to leverage information
from other clients in order to improve prediction accuracy. Unfortunately, such
approaches often incur substantial communication overhead, and the resulting
transmission delays significantly slow down the training process. As the volume
of traffic data continues to grow, this issue becomes increasingly critical,
making the resource consumption of current methods unsustainable. To address
this challenge, we propose a novel variable relationship modeling paradigm for
federated traffic prediction, termed the Channel-Independent Paradigm(CIP).
Unlike traditional approaches, CIP eliminates the need for inter-client
communication by enabling each node to perform efficient and accurate
predictions using only local information. Based on the CIP, we further develop
Fed-CI, an efficient federated learning framework, allowing each client to
process its own data independently while effectively mitigating the information
loss caused by the lack of direct data sharing among clients. Fed-CI
significantly reduces communication overhead, accelerates the training process,
and achieves state-of-the-art performance while complying with privacy
regulations. Extensive experiments on multiple real-world datasets demonstrate
that Fed-CI consistently outperforms existing methods across all datasets and
federated settings. It achieves improvements of 8%, 14%, and 16% in RMSE, MAE,
and MAPE, respectively, while also substantially reducing communication costs.

</details>


### [271] [Privacy Risk Predictions Based on Fundamental Understanding of Personal Data and an Evolving Threat Landscape](https://arxiv.org/abs/2508.04542)
*Haoran Niu,K. Suzanne Barber*

Main category: cs.LG

TL;DR: 该研究通过分析5000多个身份盗用和欺诈案例，构建了一个身份生态系统的图模型。该模型使用节点表示个人可识别信息（PII）属性，边表示它们之间的经验性泄露关系。基于这一模型，研究者开发了一个隐私风险预测框架，结合图论和图神经网络来估算某些PII特征泄露时进一步信息泄露的可能性，从而帮助用户了解隐私风险。


<details>
  <summary>Details</summary>
Motivation: 当前，个人或组织由于难以理解个人信息泄露的危害难以有效保护其隐私。因此，需要一种能定量分析不同隐私风险的工具，帮助用户理解各类个人身份信息暴露的影响。

Method: 研究通过实证分析超过5000个身份窃取和欺诈案构建了一个经验导向的Identity Ecosystem图模型。图中节点代表PII属性（如姓名、身份证号等），边代表属性暴露时的相关概率。基于图结构，研究采用图论中的方法（如最短路径分析）和Graph Neural Networks（GNN）开发了一种隐私风险预测框架。该框架旨在预测在某个PII泄露后，另一个属性被泄露的可能性。

Result: 基于构建的Identity Ecosystem图模型，该研究的预测框架能有效回答核心问题——某个人身份属性的披露是否会连带导致其他属性的泄露。实证结果显示预测模型在多个场景下具有较高的准确率，能够揭示潜在的信息泄露风险路径。

Conclusion: 该图模型的成功应用揭示了不同PII属性之间泄露的依赖关系，可为用户和组织提供针对性的隐私防护建议。研究证明，图模型不仅对揭示已知隐私风险有效，还能通过预测帮助在早期采取措施以降低损失。框架的构建具有理论创新意义，为后续隐私相关工具的开发提供了基础。

Abstract: It is difficult for individuals and organizations to protect personal
information without a fundamental understanding of relative privacy risks. By
analyzing over 5,000 empirical identity theft and fraud cases, this research
identifies which types of personal data are exposed, how frequently exposures
occur, and what the consequences of those exposures are. We construct an
Identity Ecosystem graph--a foundational, graph-based model in which nodes
represent personally identifiable information (PII) attributes and edges
represent empirical disclosure relationships between them (e.g., the
probability that one PII attribute is exposed due to the exposure of another).
Leveraging this graph structure, we develop a privacy risk prediction framework
that uses graph theory and graph neural networks to estimate the likelihood of
further disclosures when certain PII attributes are compromised. The results
show that our approach effectively answers the core question: Can the
disclosure of a given identity attribute possibly lead to the disclosure of
another attribute?

</details>


### [272] [GraphProp: Training the Graph Foundation Models using Graph Properties](https://arxiv.org/abs/2508.04594)
*Ziheng Sun,Qi Feng,Lehao Lin,Chris Ding,Jicong Fan*

Main category: cs.LG

TL;DR: 本篇论文提出了一种名为GraphProp的新型图基础模型训练方法，该方法通过两个阶段来增强图结构信息的泛化能力，并在图分类任务中表现出强大的跨域泛化能力，尤其是在处理无节点属性的图数据时。


<details>
  <summary>Details</summary>
Motivation: 传统的图基础模型（GFMs）在跨域任务中主要关注节点特征的迁移，但忽略了图结构信息的泛化能力。然而，作者发现图结构信息相比节点特征和图标签具有更好的跨域连续性。因此，本文的动机是提升GFMs在捕捉跨域图结构信息上的能力。

Method: GraphProp的训练分为两个阶段：第一阶段是训练一个结构GFM，通过预测图不变量（graph invariants）来学习。图不变量是仅依赖于图抽象结构的属性，因此训练出的模型能有效捕捉跨域结构信息。第二阶段，利用结构GFM生成的表示作为位置编码，再结合节点属性和图标签训练一个综合GFM，以增强节点特征的跨域泛化。

Result: 实验表明，GraphProp在监督学习和少样本学习任务中显著优于现有方法，尤其是在处理无节点属性的图数据时表现突出。

Conclusion: 通过分两阶段训练，GraphProp有效提升了图结构信息的跨域泛化能力，并进一步增强了节点特征的泛化性能，为图基础模型在跨域任务中的应用提供了有力工具。

Abstract: This work focuses on training graph foundation models (GFMs) that have strong
generalization ability in graph-level tasks such as graph classification.
Effective GFM training requires capturing information consistent across
different domains. We discover that graph structures provide more consistent
cross-domain information compared to node features and graph labels. However,
traditional GFMs primarily focus on transferring node features from various
domains into a unified representation space but often lack structural
cross-domain generalization. To address this, we introduce GraphProp, which
emphasizes structural generalization. The training process of GraphProp
consists of two main phases. First, we train a structural GFM by predicting
graph invariants. Since graph invariants are properties of graphs that depend
only on the abstract structure, not on particular labellings or drawings of the
graph, this structural GFM has a strong ability to capture the abstract
structural information and provide discriminative graph representations
comparable across diverse domains. In the second phase, we use the
representations given by the structural GFM as positional encodings to train a
comprehensive GFM. This phase utilizes domain-specific node attributes and
graph labels to further improve cross-domain node feature generalization. Our
experiments demonstrate that GraphProp significantly outperforms the
competitors in supervised learning and few-shot learning, especially in
handling graphs without node attributes.

</details>


### [273] [Improved Training Strategies for Physics-Informed Neural Networks using Real Experimental Data in Aluminum Spot Welding](https://arxiv.org/abs/2508.04595)
*Jan A. Zak,Christian Weißenfels*

Main category: cs.LG

TL;DR: 论文提出两种新型训练策略以改进物理信息神经网络在铝点焊质量评估中的应用：一是使用渐入函数逐步添加实验损失，并结合自定义学习率调度器和早停机制；二是引入条件更新机制调整材料参数。这些策略有助于在减少计算负担的同时，准确预测动态位移和焊点生长，实现高效的非破坏性质量控制。


<details>
  <summary>Details</summary>
Motivation: 在汽车工业中，电阻点焊是白车身的主要连接工艺，其核心质量指标焊核直径的测量需进行破坏性测试，限制了高效质量控制的实现。物理信息神经网络虽能通过实验数据重建内部过程状态，但在整合真实数据时存在优化目标冲突问题。

Method: 1. 提出渐入函数逐步引入动态位移和焊核直径的实验损失，防止过度优化冲突；结合自定义学习率调度器和基于滚动窗口的早停机制，避免损失量级增加导致的过早收敛。2. 设计阈值触发的条件更新机制：通过查表法有条件调整温度相关材料参数，确保温度预测符合物理意义。3. 采用轴对称二维模型平衡计算效率与精度；先在简化的一维场景中系统性验证训练策略，再扩展到二维模型。

Result: 二维网络预测的动态位移和焊核生长均位于实验置信区间内；成功将焊接阶段的模型参数从钢迁移到铝；验证了该模型在工业应用中实现快速、基于模型的质量控制的潜力。

Conclusion: 所提训练策略有效解决了物理信息神经网络在多目标优化时的冲突问题，显著提升了铝点焊状态的预测准确性，为非破坏性质量评估提供了可行方案，且具备向工业场景推广的应用价值。

Abstract: Resistance spot welding is the dominant joining process for the body-in-white
in the automotive industry, where the weld nugget diameter is the key quality
metric. Its measurement requires destructive testing, limiting the potential
for efficient quality control. Physics-informed neural networks were
investigated as a promising tool to reconstruct internal process states from
experimental data, enabling model-based and non-invasive quality assessment in
aluminum spot welding. A major challenge is the integration of real-world data
into the network due to competing optimization objectives. To address this, we
introduce two novel training strategies. First, experimental losses for dynamic
displacement and nugget diameter are progressively included using a fading-in
function to prevent excessive optimization conflicts. We also implement a
custom learning rate scheduler and early stopping based on a rolling window to
counteract premature reduction due to increased loss magnitudes. Second, we
introduce a conditional update of temperature-dependent material parameters via
a look-up table, activated only after a loss threshold is reached to ensure
physically meaningful temperatures. An axially symmetric two-dimensional model
was selected to represent the welding process accurately while maintaining
computational efficiency. To reduce computational burden, the training
strategies and model components were first systematically evaluated in one
dimension, enabling controlled analysis of loss design and contact models. The
two-dimensional network predicts dynamic displacement and nugget growth within
the experimental confidence interval, supports transferring welding stages from
steel to aluminum, and demonstrates strong potential for fast, model-based
quality control in industrial applications.

</details>


### [274] [Multitask Learning with Stochastic Interpolants](https://arxiv.org/abs/2508.04605)
*Hugo Negrel,Florentin Coeurdoux,Michael S. Albergo,Eric Vanden-Eijnden*

Main category: cs.LG

TL;DR: 作者提出了一种学习概率分布之间映射的泛化框架，扩展了流模型和扩散模型的时间动态。通过将标量时间变量替换为向量、矩阵或线性算子，该框架能桥接不同维度空间上的概率分布，构建多任务生成模型而无需任务特定的训练。


<details>
  <summary>Details</summary>
Motivation: 现有的生成模型如流模型和扩散模型通常局限于固定维度空间且需为不同任务定制训练。因此，作者希望建立一个统一的、任务无关的框架，解决多维分布间的映射问题，并实现多任务泛化能力。

Method: 1. 扩展'stochastic interpolants'方法：用向量、矩阵或线性算子替代标量时间变量
2. 构建算子基插值（operator-based interpolants），实现不同维度概率分布间的桥接
3. 通过该框架训练通用生成模型，不依赖特定任务的数据
4. 理论证明该框架可统一现有扩散/流模型，并扩展能力

Result: 1. 模型在零样本条件下成功执行条件生成、图像修复等任务
2. 实现微调和后验采样
3. 支持多尺度建模
4. 数值实验表明其性能可替代多种专用模型，成为通用任务无关方案

Conclusion: 该框架提供了生成模型的统一理论视角，扩展了现有方法的能力。其任务无关特性可实现零样本跨维度的多任务处理，展现了作为通用生成模型的潜力。

Abstract: We propose a framework for learning maps between probability distributions
that broadly generalizes the time dynamics of flow and diffusion models. To
enable this, we generalize stochastic interpolants by replacing the scalar time
variable with vectors, matrices, or linear operators, allowing us to bridge
probability distributions across multiple dimensional spaces. This approach
enables the construction of versatile generative models capable of fulfilling
multiple tasks without task-specific training. Our operator-based interpolants
not only provide a unifying theoretical perspective for existing generative
models but also extend their capabilities. Through numerical experiments, we
demonstrate the zero-shot efficacy of our method on conditional generation and
inpainting, fine-tuning and posterior sampling, and multiscale modeling,
suggesting its potential as a generic task-agnostic alternative to specialized
models.

</details>


### [275] [Neuromorphic Cybersecurity with Semi-supervised Lifelong Learning](https://arxiv.org/abs/2508.04610)
*Md Zesun Ahmed Mia,Malyaban Bal,Sen Lu,George M. Nishibuchi,Suhas Chelian,Srini Vasan,Abhronil Sengupta*

Main category: cs.LG

TL;DR: 本文提出了一种基于脉冲神经网络（SNN）的终身网络入侵检测系统（NIDS），受大脑的分层处理和能效启发。该系统使用高效的静态SNN识别潜在入侵，再激活动态SNN分类具体攻击类型。动态分类器采用类GWR的结构可塑性和新型自适应脉冲时序依赖可塑性（Ad-STDP）学习规则，支持增量学习。在UNSW-NB15数据集上的持续学习测试中，系统实现了85.3%的整体准确率，并通过Intel Lava框架验证了高操作稀疏性，适合神经形态硬件低功耗部署。


<details>
  <summary>Details</summary>
Motivation: 当前网络入侵检测系统面临实时适应新型攻击和低功耗部署的挑战。受生物神经系统分层处理和高效能量利用的启发，作者旨在构建一个既能持续学习新攻击模式，又适合在神经形态硬件上高效运行的终身NIDS框架。

Method: 1. 系统架构：由静态SNN和动态SNN组成双层结构。静态SNN负责高效预筛选潜在入侵，动态SNN则细分类攻击类型。
2. 动态SNN核心机制：
   - 结构可塑性：采用类似"按需增长（GWR）"的机制，动态调整神经元连接以适应新攻击模式。
   - Ad-STDP规则：设计自适应脉冲时序依赖可塑性学习算法，在增量学习过程中平衡新旧知识记忆。
3. 验证方法：使用UNSW-NB15入侵检测数据集进行持续学习测试，并通过Intel Lava框架模拟神经形态硬件部署环境。

Result: 1. 持续学习性能：在UNSW-NB15数据集上达到85.3%的整体准确率，显著降低灾难性遗忘。
2. 硬件适配性：在Intel Lava的模拟环境中展示高操作稀疏性（低脉冲活跃度），理论能耗比传统方案低1-2个数量级。

Conclusion: 该工作证明了结合静态预筛选与动态可塑性机制的SNN架构能有效实现终身NIDS：一方面通过Ad-STDP和类GWR结构实现对新攻击模式的增量学习，另一方面借助SNN的稀疏计算特性为神经形态硬件部署铺平道路。未来方向包括优化动态扩展阈值和在真实硬件上验证能效。

Abstract: Inspired by the brain's hierarchical processing and energy efficiency, this
paper presents a Spiking Neural Network (SNN) architecture for lifelong Network
Intrusion Detection System (NIDS). The proposed system first employs an
efficient static SNN to identify potential intrusions, which then activates an
adaptive dynamic SNN responsible for classifying the specific attack type.
Mimicking biological adaptation, the dynamic classifier utilizes Grow When
Required (GWR)-inspired structural plasticity and a novel Adaptive
Spike-Timing-Dependent Plasticity (Ad-STDP) learning rule. These bio-plausible
mechanisms enable the network to learn new threats incrementally while
preserving existing knowledge. Tested on the UNSW-NB15 benchmark in a continual
learning setting, the architecture demonstrates robust adaptation, reduced
catastrophic forgetting, and achieves $85.3$\% overall accuracy. Furthermore,
simulations using the Intel Lava framework confirm high operational sparsity,
highlighting the potential for low-power deployment on neuromorphic hardware.

</details>


### [276] [CaPulse: Detecting Anomalies by Tuning in to the Causal Rhythms of Time Series](https://arxiv.org/abs/2508.04630)
*Yutong Xia,Yingying Zhang,Yuxuan Liang,Lunting Fan,Qingsong Wen,Roger Zimmermann*

Main category: cs.LG

TL;DR: 本文提出了一种基于因果关系的框架CaPulse，用于解决时间序列异常检测中的数据挑战和问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在捕捉时间序列异常生成机制方面存在不足，且时间序列异常检测面临数据稀缺、不平衡和复杂多周期性等挑战。

Method: 首先构建结构因果模型来解析异常生成过程。针对数据挑战，提出带有掩码机制的周期性归一化流和精心设计的周期性学习器，创建了一种周期性感知的、基于密度的异常检测方法。

Result: 在七个真实数据集上进行的实验显示，CaPulse显著优于现有方法，AUROC提高了3%至17%，并增强了可解释性。

Conclusion: 通过因果工具和周期性归一化流，CaPulse有效捕捉了时间序列数据的固有机制，提高了异常检测性能。

Abstract: Time series anomaly detection has garnered considerable attention across
diverse domains. While existing methods often fail to capture the underlying
mechanisms behind anomaly generation in time series data. In addition, time
series anomaly detection often faces several data-related inherent challenges,
i.e., label scarcity, data imbalance, and complex multi-periodicity. In this
paper, we leverage causal tools and introduce a new causality-based framework,
CaPulse, which tunes in to the underlying causal pulse of time series data to
effectively detect anomalies. Concretely, we begin by building a structural
causal model to decipher the generation processes behind anomalies. To tackle
the challenges posed by the data, we propose Periodical Normalizing Flows with
a novel mask mechanism and carefully designed periodical learners, creating a
periodicity-aware, density-based anomaly detection approach. Extensive
experiments on seven real-world datasets demonstrate that CaPulse consistently
outperforms existing methods, achieving AUROC improvements of 3% to 17%, with
enhanced interpretability.

</details>


### [277] [A Scalable Pretraining Framework for Link Prediction with Efficient Adaptation](https://arxiv.org/abs/2508.04645)
*Yu Song,Zhigang Hua,Harry Shomer,Yan Xie,Jingzhe Liu,Bo Long,Hui Liu*

Main category: cs.LG

TL;DR: 提出了一种面向链接预测任务的预训练框架，采用两个不同特征级别（节点和边）的模块进行预训练，设计了融合模块融合两者的输出结果，并引入专家混合机制缓解预训练中的负迁移问题，同时引入参数高效微调策略加速迁移以适应不同应用场景。该模型在16个数据集上取得了最佳效果，且在节省资源方面有显著提升


<details>
  <summary>Details</summary>
Motivation: 尽管在链接预测任务中图神经网络模型表现不错，但现有方法存在监督稀疏、对初始化敏感以及在分布偏移情况下泛化能力比较差的问题。此外，链接预测任务本身同时涉及节点级别和边级别的特征，因此需要设计相应的融合机制

Method: 1. 针对链接预测任务的特殊性，设计了一套结构化的预训练方案。利用成对任务性质，分别对节点级别的表示和边级别的特征进行建模。2. 为了融合不同层次的特征结果，设计了融合模块融合这两方面的输出结果。3. 引入专家混合模型（Mixture-of-Experts），通过多个专家模型并行计算来自不同预训练数据的特征，有效避免了预训练任务中的负迁移现象。4. 引入参数高效微调策略：在迁移到下游任务时冻结大部分模型参数，仅更新少数适配器层参数，可快速适应不同数据集

Result: 在两个领域中16个数据集的测试中，该模型在资源约束场景下的链接预测任务中取得了最新最好的技术效果，其资源开销显著降低（相比于端到端训练方法减少了超过10,000倍），同时具有与端到端训练方法对比的竞争性表现

Conclusion: 针对链接预测任务设计了一种端到端的联合预训练框架，融合了节点级别和边级别两类特征信息，并借助专家混合模块和高效迁移学习机制有效解决了负迁移和训练效率问题，在资源受限下取得了最优性能

Abstract: Link Prediction (LP) is a critical task in graph machine learning. While
Graph Neural Networks (GNNs) have significantly advanced LP performance
recently, existing methods face key challenges including limited supervision
from sparse connectivity, sensitivity to initialization, and poor
generalization under distribution shifts. We explore pretraining as a solution
to address these challenges. Unlike node classification, LP is inherently a
pairwise task, which requires the integration of both node- and edge-level
information. In this work, we present the first systematic study on the
transferability of these distinct modules and propose a late fusion strategy to
effectively combine their outputs for improved performance. To handle the
diversity of pretraining data and avoid negative transfer, we introduce a
Mixture-of-Experts (MoE) framework that captures distinct patterns in separate
experts, facilitating seamless application of the pretrained model on diverse
downstream datasets. For fast adaptation, we develop a parameter-efficient
tuning strategy that allows the pretrained model to adapt to unseen datasets
with minimal computational overhead. Experiments on 16 datasets across two
domains demonstrate the effectiveness of our approach, achieving
state-of-the-art performance on low-resource link prediction while obtaining
competitive results compared to end-to-end trained methods, with over 10,000x
lower computational overhead.

</details>


### [278] [Perch 2.0: The Bittern Lesson for Bioacoustics](https://arxiv.org/abs/2508.04665)
*Bart van Merriënboer,Vincent Dumoulin,Jenny Hamer,Lauren Harrell,Andrea Burns,Tom Denton*

Main category: cs.LG

TL;DR: Perch 2.0是一个高性能的生物声学预训练模型。此版本扩展到了多类群数据集，引入了自蒸馏和源预测训练目标，在多个基准测试中达到SOTA性能，并在海洋迁移学习任务中超过了专用模型。


<details>
  <summary>Details</summary>
Motivation: 原Perch模型专注于鸟类声音，Perch 2.0旨在扩展到更广泛的生物类群（多类群），并通过改进的训练方法提升模型性能，探索细粒度物种分类任务作为生物声学预训练任务的鲁棒性。

Method: 1. 使用监督学习训练多类群数据集（从纯鸟类扩展到多类群，包括非海洋类群）。2. 引入自蒸馏（self-distillation）技术：使用原型学习分类器（prototype-learning classifier）进行知识迁移。3. 采用新的训练目标：源预测（source-prediction）任务。4. 利用大规模数据集进行预训练，提供分类分数和强迁移学习嵌入向量。

Result: 1. 在BirdSet和BEANS基准测试中达到最先进（SOTA）性能。2. 在海洋迁移学习任务中，尽管几乎没有海洋训练数据，但超过了专用的海洋模型。3. 验证了细粒度物种分类作为生物声学预训练任务的鲁棒性（泛化能力强）。

Conclusion: Perch 2.0通过扩展到多类群数据以及引入自蒸馏和源预测训练目标，显著提升了性能，并在多个任务上表现出强大的泛化能力。该研究还支持了细粒度物种分类作为生物声学预训练任务的鲁棒性假设。

Abstract: Perch is a performant pre-trained model for bioacoustics. It was trained in
supervised fashion, providing both off-the-shelf classification scores for
thousands of vocalizing species as well as strong embeddings for transfer
learning. In this new release, Perch 2.0, we expand from training exclusively
on avian species to a large multi-taxa dataset. The model is trained with
self-distillation using a prototype-learning classifier as well as a new
source-prediction training criterion. Perch 2.0 obtains state-of-the-art
performance on the BirdSet and BEANS benchmarks. It also outperforms
specialized marine models on marine transfer learning tasks, despite having
almost no marine training data. We present hypotheses as to why fine-grained
species classification is a particularly robust pre-training task for
bioacoustics.

</details>


### [279] [Robustly Learning Monotone Single-Index Models](https://arxiv.org/abs/2508.04670)
*Puqian Wang,Nikos Zarifis,Ilias Diakonikolas,Jelena Diakonikolas*

Main category: cs.LG

TL;DR: 本文提出了一种用于在高斯分布和对抗性标签噪声条件下学习单调激活函数单指标模型的高效计算算法。该算法首次为所有满足二阶加ζ有界矩条件的单调激活函数类提供了常数因子近似，包括单调Lipschitz函数和不连续函数（如带偏置的半空间）。


<details>
  <summary>Details</summary>
Motivation: 先前关于未知激活函数的研究要么无法达到常数因子近似，要么只能适用于更小的激活函数族。本文旨在克服这些限制，为更广泛的单调激活函数类提供高效的学习算法，特别是在存在对抗性标签噪声的情况下。

Method: 作者开发了一种新颖的优化框架，该框架通过利用问题结构、高斯空间性质以及单调函数的正则性，构建了一个有用的向量场来引导算法更新，从而避开了传统梯度方法的局限性。这个向量场直接对应于单指标模型学习的结构特点。

Result: 该算法是第一种能够高效计算并达到常数因子近似的方法，适用于所有单调激活函数（满足二阶加ζ有界矩条件）。该算法在存在对抗性标签噪声的情况下仍能成功应用。

Conclusion: 本文的主要贡献在于提出了一种突破传统梯度方法界限的新型优化框架，首次为所有满足有界矩条件的单调激活函数类提供了高效且鲁棒的常数因子近似算法。这种方法的成功归功于对问题结构和高斯空间性质的直接利用。

Abstract: We consider the basic problem of learning Single-Index Models with respect to
the square loss under the Gaussian distribution in the presence of adversarial
label noise. Our main contribution is the first computationally efficient
algorithm for this learning task, achieving a constant factor approximation,
that succeeds for the class of {\em all} monotone activations with bounded
moment of order $2 + \zeta,$ for $\zeta > 0.$ This class in particular includes
all monotone Lipschitz functions and even discontinuous functions like
(possibly biased) halfspaces. Prior work for the case of unknown activation
either does not attain constant factor approximation or succeeds for a
substantially smaller family of activations. The main conceptual novelty of our
approach lies in developing an optimization framework that steps outside the
boundaries of usual gradient methods and instead identifies a useful vector
field to guide the algorithm updates by directly leveraging the problem
structure, properties of Gaussian spaces, and regularity of monotone functions.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [280] [MI9 -- Agent Intelligence Protocol: Runtime Governance for Agentic AI Systems](https://arxiv.org/abs/2508.03858)
*Charles L. Wang,Trisha Singhal,Ameya Kelkar,Jason Tuo*

Main category: cs.AI

TL;DR: 提出了首个针对自主AI系统安全对齐的运行时治理框架MI9，包含六个关键组件，实时监控和处理运行过程中出现的意外行为，填补传统预部署治理的不足，确保大规模部署时的系统安全性。


<details>
  <summary>Details</summary>
Motivation: 自主AI系统在运行时可能表现出不可预测的新行为，这些行为引发的风险无法通过传统的预部署治理完全解决。为了解决这一治理缺口，需要一种能够在运行过程中实时监控和保障其安全性的新方法。

Method: MI9框架融合六个核心功能：1）代理风险指数评估系统风险水平；2）代理语义遥测捕获运行状态；3）持续授权监控行为合规性；4）基于有限状态机的符合性引擎校验状态转移；5）目标条件漂移检测异常行为；6）分级遏制策略动态应对问题。该框架兼容异构代理架构，实现透明的全过程监管。

Result: 在多种场景下的综合分析验证了MI9能系统性地应对传统治理方法无法覆盖的代理治理挑战，其组件协同有效管控了运行时风险，为大规模安全部署自主AI提供了技术基础。

Conclusion: MI9作为首个端到端的自主AI运行时治理框架，弥补了预部署治理的局限，通过实时监控和干预机制降低代理系统风险，建立了可拓展的安全部署基础设施。

Abstract: Agentic AI systems capable of reasoning, planning, and executing actions
present fundamentally distinct governance challenges compared to traditional AI
models. Unlike conventional AI, these systems exhibit emergent and unexpected
behaviors during runtime, introducing novel agent-related risks that cannot be
fully anticipated through pre-deployment governance alone. To address this
critical gap, we introduce MI9, the first fully integrated runtime governance
framework designed specifically for safety and alignment of agentic AI systems.
MI9 introduces real-time controls through six integrated components:
agency-risk index, agent-semantic telemetry capture, continuous authorization
monitoring, Finite-State-Machine (FSM)-based conformance engines,
goal-conditioned drift detection, and graduated containment strategies.
Operating transparently across heterogeneous agent architectures, MI9 enables
the systematic, safe, and responsible deployment of agentic systems in
production environments where conventional governance approaches fall short,
providing the foundational infrastructure for safe agentic AI deployment at
scale. Detailed analysis through a diverse set of scenarios demonstrates MI9's
systematic coverage of governance challenges that existing approaches fail to
address, establishing the technical foundation for comprehensive agentic AI
oversight.

</details>


### [281] [Evo-MARL: Co-Evolutionary Multi-Agent Reinforcement Learning for Internalized Safety](https://arxiv.org/abs/2508.03864)
*Zhenyu Pan,Yiting Zhang,Yutong Zhang,Jianshu Zhang,Haozheng Luo,Yuwei Han,Dennis Wu,Hong-Yu Chen,Philip S. Yu,Manling Li,Han Liu*

Main category: cs.AI

TL;DR: 提出了名为Evo-MARL的多智能体强化学习框架，通过对抗训练使所有任务智能体同时具备防御能力，从而在不增加系统开销的前提下提升多智能体系统的安全性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有的多智能体系统（MAS）在安全防护上依赖独立安全代理，存在单点失效和防护能力有限的问题。同时，增加防护代理会带来成本和复杂性上升。因此，需要一种将安全能力内化到任务代理的方法，实现既经济又可靠的防御。

Method: 1. 使用参数共享的强化学习：任务代理在履行主要功能的同时学习防御策略；2. 结合进化搜索进行对抗训练：使用进化算法共同演进攻击者和防御者（即任务代理），使防御机制在持续对抗中得到强化；3. 无需外部防护模块：所有智能体直接承担安全职责，减少系统开销，规避单点失效。

Result: 攻击成功率降低最高22%，同时推理任务准确率提升最高5%，证明Evo-MARL在增强安全性的同时提高了任务能力。

Conclusion: Evo-MARL提供了一个全新的安全解决方案：将安全能力内嵌至任务代理，实现安全与性能协同优化。该方法解决了现有防护模块单点失效和开销难题，且可通过对抗训练持续强化MAS的鲁棒性。

Abstract: Multi-agent systems (MAS) built on multimodal large language models exhibit
strong collaboration and performance. However, their growing openness and
interaction complexity pose serious risks, notably jailbreak and adversarial
attacks. Existing defenses typically rely on external guard modules, such as
dedicated safety agents, to handle unsafe behaviors. Unfortunately, this
paradigm faces two challenges: (1) standalone agents offer limited protection,
and (2) their independence leads to single-point failure-if compromised,
system-wide safety collapses. Naively increasing the number of guard agents
further raises cost and complexity. To address these challenges, we propose
Evo-MARL, a novel multi-agent reinforcement learning (MARL) framework that
enables all task agents to jointly acquire defensive capabilities. Rather than
relying on external safety modules, Evo-MARL trains each agent to
simultaneously perform its primary function and resist adversarial threats,
ensuring robustness without increasing system overhead or single-node failure.
Furthermore, Evo-MARL integrates evolutionary search with parameter-sharing
reinforcement learning to co-evolve attackers and defenders. This adversarial
training paradigm internalizes safety mechanisms and continually enhances MAS
performance under co-evolving threats. Experiments show that Evo-MARL reduces
attack success rates by up to 22% while boosting accuracy by up to 5% on
reasoning tasks-demonstrating that safety and utility can be jointly improved.

</details>


### [282] [MOTIF: Multi-strategy Optimization via Turn-based Interactive Framework](https://arxiv.org/abs/2508.03929)
*Nguyen Viet Tuan Kiet,Dao Van Tung,Tran Cong Dao,Huynh Thi Thanh Binh*

Main category: cs.AI

TL;DR: 提出MOTIF框架，利用多代理轮流改进优化算法组件，在组合优化问题中超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统组合优化问题解法依赖人工设计组件，现有LLM应用仅优化单一组件（如启发式评分函数），忽视多组件协同优化的潜力。MOTIF框架旨在通过多代理交互联合优化多个互依赖组件。

Method: 1. 将求解器设计建模为多策略优化问题；2. 提出MOTIF框架：基于蒙特卡洛树搜索组织两个LLM代理轮流优化组件；3. 代理在每一步优化一个组件时参考双方历史更新记录；4. 通过竞争与协作机制实现组件协同设计。

Result: 在多个组合优化问题领域测试中，MOTIF框架性能优于当前最先进方法，证明了多代理轮流优化策略的有效性。

Conclusion: MOTIF框架首次实现多组件联合自动优化，其交互式设计能发现多样高效解法，为完全自动化求解器设计开辟新方向。

Abstract: Designing effective algorithmic components remains a fundamental obstacle in
tackling NP-hard combinatorial optimization problems (COPs), where solvers
often rely on carefully hand-crafted strategies. Despite recent advances in
using large language models (LLMs) to synthesize high-quality components, most
approaches restrict the search to a single element - commonly a heuristic
scoring function - thus missing broader opportunities for innovation. In this
paper, we introduce a broader formulation of solver design as a multi-strategy
optimization problem, which seeks to jointly improve a set of interdependent
components under a unified objective. To address this, we propose
Multi-strategy Optimization via Turn-based Interactive Framework (MOTIF) - a
novel framework based on Monte Carlo Tree Search that facilitates turn-based
optimization between two LLM agents. At each turn, an agent improves one
component by leveraging the history of both its own and its opponent's prior
updates, promoting both competitive pressure and emergent cooperation. This
structured interaction broadens the search landscape and encourages the
discovery of diverse, high-performing solutions. Experiments across multiple
COP domains show that MOTIF consistently outperforms state-of-the-art methods,
highlighting the promise of turn-based, multi-agent prompting for fully
automated solver design.

</details>


### [283] [Can Large Language Models Adequately Perform Symbolic Reasoning Over Time Series?](https://arxiv.org/abs/2508.03963)
*Zewen Liu,Juntong Ni,Xianfeng Tang,Max S. Y. Lau,Wei Jin*

Main category: cs.AI

TL;DR: 该论文介绍了SymbolBench，一个用于评估大型语言模型在真实世界时间序列数据上的符号推理能力的基准，包含三个任务：多元符号回归、布尔网络推理和因果发现。同时提出了一种结合大型语言模型和遗传编程的统一框架，以形成闭环符号推理系统。通过实验揭示了当前模型的优缺点，强调了领域知识、上下文对齐和推理结构在提高自动化科学发现中的重要性。


<details>
  <summary>Details</summary>
Motivation: 揭示时间序列数据中隐藏的符号规律是科学发现和人工智能的核心挑战。尽管大型语言模型在结构化推理任务中表现出潜力，但其从时间序列数据推断可解释且上下文对齐的符号结构的能力尚未充分探索。为系统评估该能力，作者创建了SymbolBench这一基准。

Method: 1. 引入SymbolBench基准，包含三个任务：多元符号回归、布尔网络推理和因果发现，覆盖多样化符号形式和复杂度。2. 提出一个统一框架，将大型语言模型与遗传编程结合，形成闭环符号推理系统。其中，大型语言模型充当预测器和评估器，通过迭代优化符号表达式。

Result: 实验结果显示当前模型的关键优势和局限性：1）优势：在特定任务中表现出一定的符号推理能力；2）局限性：处理复杂符号结构时仍有困难。研究强调结合领域知识、上下文对齐和结构化推理可提升模型在自动化科学发现中的性能。

Conclusion: SymbolBench为评估时间序列上的符号推理建立了全面标准，提出的融合框架展示了大型语言模型作为预测-评估闭环组件的潜力。未来工作需进一步整合领域知识和推理结构，以克服当前模型的局限性并推动自动化科学发现。

Abstract: Uncovering hidden symbolic laws from time series data, as an aspiration
dating back to Kepler's discovery of planetary motion, remains a core challenge
in scientific discovery and artificial intelligence. While Large Language
Models show promise in structured reasoning tasks, their ability to infer
interpretable, context-aligned symbolic structures from time series data is
still underexplored. To systematically evaluate this capability, we introduce
SymbolBench, a comprehensive benchmark designed to assess symbolic reasoning
over real-world time series across three tasks: multivariate symbolic
regression, Boolean network inference, and causal discovery. Unlike prior
efforts limited to simple algebraic equations, SymbolBench spans a diverse set
of symbolic forms with varying complexity. We further propose a unified
framework that integrates LLMs with genetic programming to form a closed-loop
symbolic reasoning system, where LLMs act both as predictors and evaluators.
Our empirical results reveal key strengths and limitations of current models,
highlighting the importance of combining domain knowledge, context alignment,
and reasoning structure to improve LLMs in automated scientific discovery.

</details>


### [284] [The Emotional Baby Is Truly Deadly: Does your Multimodal Large Reasoning Model Have Emotional Flattery towards Humans?](https://arxiv.org/abs/2508.03986)
*Yuan Xun,Xiaojun Jia,Xinwei Liu,Hua Zhang*

Main category: cs.AI

TL;DR: 该研究提出了EmoAgent框架，利用情感提示劫持多模态大型语言模型(MLRM)的推理路径，揭示模型在深度思考阶段易受情感影响而导致安全协议失效的问题。并提出了三种量化指标来评估模型安全认知错位的风险。


<details>
  <summary>Details</summary>
Motivation: 现有面向人类服务的多模态大型语言模型在深度思考阶段对用户情感线索高度敏感，容易在强烈情绪影响下绕过安全协议。研究观察到模型即使识别出视觉风险，仍可能因情感错位产生有害输出。这些透明深度思考场景中存在持续高风险故障模式，如将有害推理隐藏在看似安全的响应背后，暴露了内部推理与表面行为之间的错位，逃逸现有基于内容的安全防护。

Method: 1. 提出EmoAgent框架：一个自动化情感对抗代理框架，通过编排夸大的情感提示来劫持模型推理路径。2. 设计实验：在先进MLRMs上进行广泛测试，评估EmoAgent的效果。3. 建立三种风险指标：(a)风险推理隐遁评分(RRSS)衡量良性输出下隐藏的有害推理；(b)风险视觉忽视率(RVNR)量化模型在识别视觉风险后仍生成不安全完成项的概率；(c)拒绝态度不一致性(RAIC)评估模型在不同提示变体下拒绝行为的不稳定性。

Result: 实验证明EmoAgent能有效引发模型安全漏洞。发现三个关键现象：1. 模型会产生隐藏在安全响应下的有害推理（RRSS异常）；2. 即使正确识别视觉风险，模型仍可能输出不安全内容（RVNR突出）；3. 模型拒绝行为在情感提示变体中存在明显不一致（RAIC值异常）。这些结果揭示了当前MLRMs在情感认知层面存在的深度安全错位。

Conclusion: 该研究通过情感对抗框架揭露了多模态大模型安全机制的脆弱性，特别是情感因素对深度推理过程的劫持效应。提出的三项指标系统性地量化了模型安全行为中的认知错位。实验证明现有安全防护无法有效应对这类基于情感的攻击向量，呼吁在模型安全设计中必须考虑情感认知层面的对齐问题。

Abstract: We observe that MLRMs oriented toward human-centric service are highly
susceptible to user emotional cues during the deep-thinking stage, often
overriding safety protocols or built-in safety checks under high emotional
intensity. Inspired by this key insight, we propose EmoAgent, an autonomous
adversarial emotion-agent framework that orchestrates exaggerated affective
prompts to hijack reasoning pathways. Even when visual risks are correctly
identified, models can still produce harmful completions through emotional
misalignment. We further identify persistent high-risk failure modes in
transparent deep-thinking scenarios, such as MLRMs generating harmful reasoning
masked behind seemingly safe responses. These failures expose misalignments
between internal inference and surface-level behavior, eluding existing
content-based safeguards. To quantify these risks, we introduce three metrics:
(1) Risk-Reasoning Stealth Score (RRSS) for harmful reasoning beneath benign
outputs; (2) Risk-Visual Neglect Rate (RVNR) for unsafe completions despite
visual risk recognition; and (3) Refusal Attitude Inconsistency (RAIC) for
evaluating refusal unstability under prompt variants. Extensive experiments on
advanced MLRMs demonstrate the effectiveness of EmoAgent and reveal deeper
emotional cognitive misalignments in model safety behavior.

</details>


### [285] [Galaxy: A Cognition-Centered Framework for Proactive, Privacy-Preserving, and Self-Evolving LLM Agents](https://arxiv.org/abs/2508.03991)
*Chongyu Bao,Ruimin Dai,Yangbo Shen,Runyang Jian,Jinghan Zhang,Xiaolan Liu,Kunpeng Liu*

Main category: cs.AI

TL;DR: 提出一个名为Cognition Forest的语义结构，结合认知建模与系统设计，形成一个自我强化的循环。基于此，开发了Galaxy框架，实现多维交互和个性化能力生成，包含两个协作代理KoRa和Kernel，实验证明其优于现有基准。


<details>
  <summary>Details</summary>
Motivation: 当前的智能个人助手（IPA）在响应能力方面已有广泛研究，但主动行为、隐私保护和自我进化能力仍有不足。为了解决这些问题，需要将LLM代理的认知架构与系统设计统一起来。

Method: 提出了Cognition Forest语义结构，以对齐认知建模和系统设计。在此基础上构建了Galaxy框架，支持多维交互和个性化能力生成，开发了两个代理：KoRa（认知增强生成代理，支持响应式和主动技能）和Kernel（元认知代理，支持自我进化和隐私保护）。通过实验对比现有基准和消融研究进行验证。

Result: 实验结果表明，Galaxy在多个任务上优于当前最先进的基准。消融研究和真实交互案例进一步证实其有效性。

Conclusion: Galaxy框架成功地将认知架构与系统设计统一在自我强化的循环中，解决了智能个人助手在主动性、隐私保护和自我进化方面的挑战，为未来IPA发展提供新方向。

Abstract: Intelligent personal assistants (IPAs) such as Siri and Google Assistant are
designed to enhance human capabilities and perform tasks on behalf of users.
The emergence of LLM agents brings new opportunities for the development of
IPAs. While responsive capabilities have been widely studied, proactive
behaviors remain underexplored. Designing an IPA that is proactive,
privacy-preserving, and capable of self-evolution remains a significant
challenge. Designing such IPAs relies on the cognitive architecture of LLM
agents. This work proposes Cognition Forest, a semantic structure designed to
align cognitive modeling with system-level design. We unify cognitive
architecture and system design into a self-reinforcing loop instead of treating
them separately. Based on this principle, we present Galaxy, a framework that
supports multidimensional interactions and personalized capability generation.
Two cooperative agents are implemented based on Galaxy: KoRa, a
cognition-enhanced generative agent that supports both responsive and proactive
skills; and Kernel, a meta-cognition-based meta-agent that enables Galaxy's
self-evolution and privacy preservation. Experimental results show that Galaxy
outperforms multiple state-of-the-art benchmarks. Ablation studies and
real-world interaction cases validate the effectiveness of Galaxy.

</details>


### [286] [Uncertainty-Aware GUI Agent: Adaptive Perception through Component Recommendation and Human-in-the-Loop Refinement](https://arxiv.org/abs/2508.04025)
*Chao Hao,Shuai Wang,Kaiwen Zhou*

Main category: cs.AI

TL;DR: RecAgent是一个基于自适应感知的不确定性智能体，通过减少视觉和决策不确定性来自助完成移动任务。它采用组件推荐机制来聚焦相关UI元素（减少视觉冗余），并通过人机交互请求用户反馈处理决策模糊性问题。同时提出ComplexAction数据用于评估复杂场景下的单步动作执行能力。实验证明该模型具备优势。


<details>
  <summary>Details</summary>
Motivation: GUI智能体在执行移动任务时常面临两个问题：一是因屏幕信息冗余导致视觉不确定性（即过多无用元素干扰感知），二是决策不确定性（任务模糊或推理复杂）导致决策困难。传统方法难以同时解决这两个问题，因此RecAgent尝试通过联合框架主动降噪并结合人机反馈来解决。

Method: 1. 通过组件推荐机制主动感知屏幕中最相关的UI元素来减少视觉不确定性：使用目标函数过滤冗余信息；2. 使用交互式模块处理决策不确定性：当遇到模糊任务时自动触发用户反馈机制以明确意图；3. 建立ComplexAction数据集：包含多种复杂场景下的单步动作测试任务；4. 在真实移动设备操作平台上进行训练与验证。

Result: 实验结果表明RecAgent在精度和效率方面超越了其他基线模型：1. 在ComplexAction的评测任务上平均成功率高出7%以上；2. UI元素识别准确度提高10%；3.同时证明人机交互模块可解决模糊任务，但过度请求可能会导致效率低于完全自动化的基线模型。

Conclusion: RecAgent提出了一种融合自适应感知和人机机制的统一框架，能有效解决移动GUI自动执行中常见的视觉及决策不确定性。ComplexAction数据有助于评估动作执行的健壮性，且证明了人机交互机制在面对复杂决策时可以提升可靠性，但需要在效率权衡上进一步优化。

Abstract: Graphical user interface (GUI) agents have shown promise in automating mobile
tasks but still struggle with input redundancy and decision ambiguity. In this
paper, we present \textbf{RecAgent}, an uncertainty-aware agent that addresses
these issues through adaptive perception. We distinguish two types of
uncertainty in GUI navigation: (1) perceptual uncertainty, caused by input
redundancy and noise from comprehensive screen information, and (2) decision
uncertainty, arising from ambiguous tasks and complex reasoning. To reduce
perceptual uncertainty, RecAgent employs a component recommendation mechanism
that identifies and focuses on the most relevant UI elements. For decision
uncertainty, it uses an interactive module to request user feedback in
ambiguous situations, enabling intent-aware decisions. These components are
integrated into a unified framework that proactively reduces input complexity
and reacts to high-uncertainty cases via human-in-the-loop refinement.
Additionally, we propose a dataset called \textbf{ComplexAction} to evaluate
the success rate of GUI agents in executing specified single-step actions
within complex scenarios. Extensive experiments validate the effectiveness of
our approach. The dataset and code will be available at
https://github.com/Fanye12/RecAgent.

</details>


### [287] [SEA: Self-Evolution Agent with Step-wise Reward for Computer Use](https://arxiv.org/abs/2508.04037)
*Liang Tang,Shuxian Li,Yuhao Cheng,Yukang Huo,Zhepeng Wang,Yiqiang Yan,Kaer Huang,Yanzhe Jing,Tiaonan Duan*

Main category: cs.AI

TL;DR: 本文提出了一种名为自我进化代理（SEA）的计算机使用代理，通过创新的数据生成、强化学习和模型增强方法，在仅有70亿参数的情况下实现了超越同规模模型的性能，并与更大模型相媲美。


<details>
  <summary>Details</summary>
Motivation: 当前计算机使用代理的性能远未达到实用水平。为了提升代理在操作计算机完成用户任务方面的能力，本文旨在通过创新的数据、训练和增强方法开发高效的代理。

Method: 1. 提出自动流程生成可验证的训练轨迹；2. 提出高效的逐步强化学习以减轻长周期训练的计算负担；3. 设计增强方法，将基础能力与规划能力融合到单一模型中而无需额外训练。

Result: 最终得到的70亿参数SEA代理（Self-Evolution Agent）在性能上超越了同参数规模的模型，并可比肩更大规模的模型。

Conclusion: 通过结合创新的数据生成、训练策略和模型增强方法，SEA以较小参数量实现了高性能，为实用化计算机代理提供了可行路径。作者承诺未来将开源模型权重及相关代码。

Abstract: Computer use agent is an emerging area in artificial intelligence that aims
to operate the computers to achieve the user's tasks, which attracts a lot of
attention from both industry and academia. However, the present agents'
performance is far from being used. In this paper, we propose the
Self-Evolution Agent (SEA) for computer use, and to develop this agent, we
propose creative methods in data generation, reinforcement learning, and model
enhancement. Specifically, we first propose an automatic pipeline to generate
the verifiable trajectory for training. And then, we propose efficient
step-wise reinforcement learning to alleviate the significant computational
requirements for long-horizon training. In the end, we propose the enhancement
method to merge the grounding and planning ability into one model without any
extra training. Accordingly, based on our proposed innovation of data
generation, training strategy, and enhancement, we get the Selfevolution Agent
(SEA) for computer use with only 7B parameters, which outperforms models with
the same number of parameters and has comparable performance to larger ones. We
will make the models' weight and related codes open-source in the future.

</details>


### [288] [Personalized Knowledge Transfer Through Generative AI: Contextualizing Learning to Individual Career Goals](https://arxiv.org/abs/2508.04070)
*Ronja Mehlan,Claudia Hess,Quintus Stierstorfer,Kristina Schaaff*

Main category: cs.AI

TL;DR: 本文研究了基于生成式人工智能的职业生涯目标个性化学习内容对学习者投入度、满意度和学习效率的影响。实验表明，个性化内容能够显著提高学习投入和满意度，并轻微提升效率。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能越来越多地融入数字学习环境，个性化学习内容以匹配学习者的职业目标有望提高学习者的投入度和长期学习动力。作者旨在探索这种个性化学习内容的影响。

Method: 采用混合研究方法，实验涉及4000多名学习者，将他们分为实验组（接受基于职业目标的个性化学习内容）和对照组（接受标准内容）。通过定量分析（如学习时长、满意度评分）和定性分析（收集学习者的反馈）来评估效果。

Result: 定量结果显示，实验组的学习者会话时间更长，满意度评分更高，并且学习时间略有缩短。定性分析揭示学习者认为这种个性化材料更具学习动机和实践意义，有助于深入认知投入和内容认同。

Conclusion: 研究结果表明，将教育内容与学习者的职业目标对齐具有重要价值，基于生成式人工智能的可扩展个性化学习可以架起理论知识与工作应用之间的桥梁。

Abstract: As artificial intelligence becomes increasingly integrated into digital
learning environments, the personalization of learning content to reflect
learners' individual career goals offers promising potential to enhance
engagement and long-term motivation. In our study, we investigate how career
goal-based content adaptation in learning systems based on generative AI
(GenAI) influences learner engagement, satisfaction, and study efficiency. The
mixed-methods experiment involved more than 4,000 learners, with one group
receiving learning scenarios tailored to their career goals and a control
group. Quantitative results show increased session duration, higher
satisfaction ratings, and a modest reduction in study duration compared to
standard content. Qualitative analysis highlights that learners found the
personalized material motivating and practical, enabling deep cognitive
engagement and strong identification with the content. These findings
underscore the value of aligning educational content with learners' career
goals and suggest that scalable AI personalization can bridge academic
knowledge and workplace applicability.

</details>


### [289] [KG-Augmented Executable CoT for Mathematical Coding](https://arxiv.org/abs/2508.04072)
*Xingyu Chen,Junxiu An,Jun Guo,Li Wang,Jingcai Guo*

Main category: cs.AI

TL;DR: 提出了KGA-ECoT框架，通过知识图谱和可执行代码增强大语言模型在复杂数学推理任务中的表现，在多个基准测试中获得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在复杂推理任务（如数学推理和代码生成）中存在局限性，需要一种能够结合知识检索和代码执行的通用框架来提升性能和准确性。

Method: KGA-ECoT框架：1. 将问题分解为结构化任务图（Structured Task Graph）；2. 利用GraphRAG从数学知识库中检索精确知识；3. 生成可验证的可执行代码并进行外部执行；确保计算准确性和推理可验证性。

Result: 在多个数学推理基准测试中显著优于现有方法，绝对准确率提升从几个百分点到超过十个百分点。分析证实GraphRAG在提升代码质量和外部代码执行对确保精度具有关键作用。

Conclusion: KGA-ECoT是一个鲁棒且高度可泛化的复杂数学推理框架，通过结构化分解、高效知识检索和可执行代码验证解决了LLM在推理任务上的局限。

Abstract: In recent years, large language models (LLMs) have excelled in natural
language processing tasks but face significant challenges in complex reasoning
tasks such as mathematical reasoning and code generation. To address these
limitations, we propose KG-Augmented Executable Chain-of-Thought (KGA-ECoT), a
novel framework that enhances code generation through knowledge graphs and
improves mathematical reasoning via executable code. KGA-ECoT decomposes
problems into a Structured Task Graph, leverages efficient GraphRAG for precise
knowledge retrieval from mathematical libraries, and generates verifiable code
to ensure computational accuracy. Evaluations on multiple mathematical
reasoning benchmarks demonstrate that KGA-ECoT significantly outperforms
existing prompting methods, achieving absolute accuracy improvements ranging
from several to over ten percentage points. Further analysis confirms the
critical roles of GraphRAG in enhancing code quality and external code
execution in ensuring precision. These findings collectively establish KGA-ECoT
as a robust and highly generalizable framework for complex mathematical
reasoning tasks.

</details>


### [290] [GeoSR: Cognitive-Agentic Framework for Probing Geospatial Knowledge Boundaries via Iterative Self-Refinement](https://arxiv.org/abs/2508.04080)
*Jinfan Tang,Kunming Wu,Ruifeng Gongxie,Yuya He,Yuankai Wu*

Main category: cs.AI

TL;DR: 提出GeoSR自我优化框架，将地理学第一定律融入大语言模型（LLM）代理协同推理中，通过三个协作代理（变量选择、点位选择、迭代优化）的循环提升空间预测准确性。相比传统提示法，该模型在物理与社会经济属性预测中均实现更可靠结果。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型展现出潜在的地理空间理解能力，但存在空间一致性差、多跳推理困难、地理偏见明显等问题。为克服上述缺陷，研究团队旨在将地理统计分析先验（如Tobler第一定律）结构化嵌入推理流程。

Method: 1. 变量选择代理（variable-selection）：从同位置选取相关协变量；
2. 点位选择代理（point-selection）：选择LLM前轮生成的邻近位置参考预测值；
3. 优化代理（refine）：评估预测质量并触发迭代优化循环。三轮代理协作通过空间依赖性与跨变量关系持续提升预测精度。

Result: 在物理世界属性（如地形特征）和社会经济指标预测任务中，GeoSR均显著超越标准提示策略。实验验证：融入地理统计先验的代理推理架构能产出更高精度、更公平的空间预测结果。

Conclusion: 通过将Tobler地理学第一定律转化为多代理协同的闭环推理框架，GeoSR成功提升LLM空间预测鲁棒性。该方法证明结构化整合领域知识与迭代优化机制可缓解LLM地理偏见问题，为构建可信地理人工智能提供新范式。

Abstract: Recent studies have extended the application of large language models (LLMs)
to geographic problems, revealing surprising geospatial competence even without
explicit spatial supervision. However, LLMs still face challenges in spatial
consistency, multi-hop reasoning, and geographic bias. To address these issues,
we propose GeoSR, a self-refining agentic reasoning framework that embeds core
geographic principles -- most notably Tobler's First Law of Geography -- into
an iterative prediction loop. In GeoSR, the reasoning process is decomposed
into three collaborating agents: (1) a variable-selection agent that selects
relevant covariates from the same location; (2) a point-selection agent that
chooses reference predictions at nearby locations generated by the LLM in
previous rounds; and (3) a refine agent that coordinates the iterative
refinement process by evaluating prediction quality and triggering further
rounds when necessary. This agentic loop progressively improves prediction
quality by leveraging both spatial dependencies and inter-variable
relationships. We validate GeoSR on tasks ranging from physical-world property
estimation to socioeconomic prediction. Experimental results show consistent
improvements over standard prompting strategies, demonstrating that
incorporating geostatistical priors and spatially structured reasoning into
LLMs leads to more accurate and equitable geospatial predictions. The code of
GeoSR is available at https://github.com/JinfanTang/GeoSR.

</details>


### [291] [Towards Transparent AI Grading: Semantic Entropy as a Signal for Human-AI Disagreement](https://arxiv.org/abs/2508.04105)
*Karrtik Iyer,Manikandan Ravikiran,Prasanna Pendse,Shayan Mohanty*

Main category: cs.AI

TL;DR: 提出了语义熵作为一种测量方法，用于评估AI自动评分系统中评分决策的不确定性和潜在争议。该方法通过聚类多个GPT-4生成的解释来反映人类评分者间的分歧，实验证明其在跨学科一致性和任务敏感性方面有效。


<details>
  <summary>Details</summary>
Motivation: 现有的自动化评分系统在评分时往往无法表明决策的不确定性或潜在争议，这可能导致评分不透明和不可信。为了解决这个问题，研究人员希望引入一种能够量化这种不确定性的指标，以提升AI辅助评分的透明性和可靠性。

Method: 研究人员引入了语义熵的概念，通过使用GPT-4为每个学生回答生成多个解释，然后根据这些解释之间的蕴含相似性进行聚类。最后通过计算这些聚类的熵来量化差异。该方法不依赖最终输出分数。实验部分基于ASAP-SAS数据集，考察语义熵是否与人类评分者分歧相关、是否具有学科通用性、以及是否对结构性任务特征（如源依赖性）敏感。

Result: 实验证明：（1）语义熵与人类评分者的分歧有正相关；（2）该方法在不同学科之间具有一致性；（3）在需要解释性推理的任务中，语义熵值会更高。因此，语义熵能够作为一个可解释的、反映评分不确定性的指示器。

Conclusion: 语义熵可以作为一种有效的、解释性强的指标，用于AI辅助评分系统中标记评分决策的不确定性。这种方法的引入能提升自动化评分的透明度和可信度。

Abstract: Automated grading systems can efficiently score short-answer responses, yet
they often fail to indicate when a grading decision is uncertain or potentially
contentious. We introduce semantic entropy, a measure of variability across
multiple GPT-4-generated explanations for the same student response, as a proxy
for human grader disagreement. By clustering rationales via entailment-based
similarity and computing entropy over these clusters, we quantify the diversity
of justifications without relying on final output scores. We address three
research questions: (1) Does semantic entropy align with human grader
disagreement? (2) Does it generalize across academic subjects? (3) Is it
sensitive to structural task features such as source dependency? Experiments on
the ASAP-SAS dataset show that semantic entropy correlates with rater
disagreement, varies meaningfully across subjects, and increases in tasks
requiring interpretive reasoning. Our findings position semantic entropy as an
interpretable uncertainty signal that supports more transparent and trustworthy
AI-assisted grading workflows.

</details>


### [292] [A Compositional Framework for On-the-Fly LTLf Synthesis](https://arxiv.org/abs/2508.04116)
*Yongkang Li,Shengping Xiao,Shufang Zhu,Jianwen Li,Geguang Pu*

Main category: cs.AI

TL;DR: 该论文提出了一种组合式即时综合框架，用于处理有限轨迹上的线性时序逻辑（LTLf）的响应式综合问题。该框架融合了现有方法的优势，专注于实践中常见的大型合取子公式，可在游戏求解过程中进行组合而非在构建自动机（游戏竞技场）时组合。通过修剪中间结果简化后续组合并实现不可实现性的早期检测。实验表明，该方法能够解决现有求解器无法处理的部分实例，并且两种组合变体各有优势。


<details>
  <summary>Details</summary>
Motivation: 现有的反应式综合技术分为两种：一种在解决游戏前组合性构建确定性有限自动机（DFA），并利用自动机最小化来缓解状态爆炸；另一种在游戏解决过程中增量构建DFA以避免完整DFA构建。然而，这两种方法在效率上都不占绝对优势。论文动机构建于整合这两种方法的优点，尤其针对实践中常见的由大量小型LTLf公式合取而成的复杂规范。

Method: 1. 引入组合式即时综合框架：在解决LTLf游戏的过程中组合子公式的DFA（而非在构建游戏竞技场之前或过程中构建整个DFA）。
2. 定义两种组合变体：(a) 组合前修剪：通过最小化子DFA以利用自动机最小化的优势；(b) 组合中修剪：在组合过程中即时修剪中间结果以引导即时综合。
3. 设计综合游戏求解策略，利用中间结果修剪技术简化后续组合步骤并在早期检测不可实现性（即玩家无法获胜）。

Result: 1. 在实验验证中，该方法解决了部分其他最新求解器（例如基于完整DFA构建或增量构建的方法）无法处理的合成基准和实际案例。
2. 详细分析表明：组合前修剪和组合中修剪两种策略各有优势：前者最小化DFA从而减轻后续计算负担，后者在组合过程动态修剪中间状态以加速实现结果。
3. 框架整体表现出处理大型合取公式的能力，成功避免了最坏情况下的完整DFA构建需求。

Conclusion: 该论文的创新点在于提出两种组合式即时综合策略，整合了传统组合最小化和增量即时方法的优势。两种策略具有互补性，通过合理选择可在不同场景下避免最坏情况下的复杂度，显著提高了实际应用中的可扩展性和求解能力。实验验证了该框架在合成和实践实例中的有效性。未来可探索更多组合启发式策略和集成多分解方法。

Abstract: Reactive synthesis from Linear Temporal Logic over finite traces (LTLf) can
be reduced to a two-player game over a Deterministic Finite Automaton (DFA) of
the LTLf specification. The primary challenge here is DFA construction, which
is 2EXPTIME-complete in the worst case. Existing techniques either construct
the DFA compositionally before solving the game, leveraging automata
minimization to mitigate state-space explosion, or build the DFA incrementally
during game solving to avoid full DFA construction. However, neither is
dominant. In this paper, we introduce a compositional on-the-fly synthesis
framework that integrates the strengths of both approaches, focusing on large
conjunctions of smaller LTLf formulas common in practice. This framework
applies composition during game solving instead of automata (game arena)
construction. While composing all intermediate results may be necessary in the
worst case, pruning these results simplifies subsequent compositions and
enables early detection of unrealizability. Specifically, the framework allows
two composition variants: pruning before composition to take full advantage of
minimization or pruning during composition to guide on-the-fly synthesis.
Compared to state-of-the-art synthesis solvers, our framework is able to solve
a notable number of instances that other solvers cannot handle. A detailed
analysis shows that both composition variants have unique merits.

</details>


### [293] [AgREE: Agentic Reasoning for Knowledge Graph Completion on Emerging Entities](https://arxiv.org/abs/2508.04118)
*Ruochen Zhao,Simone Conia,Eric Peng,Min Li,Saloni Potdar*

Main category: cs.AI

TL;DR: 提出了AgREE框架，一种基于代理的迭代检索和推理框架，用于动态构建知识图谱三元组，特别关注新兴实体。在零训练的情况下，该方法在构建知识图谱三元组方面显著超越现有方法，对新兴实体的效果提升高达13.7%。同时提出新的评估方法和基准数据集。


<details>
  <summary>Details</summary>
Motivation: 现有开放域知识图谱补全方法主要依赖预训练语言模型的参数知识、预构建查询或单步检索，需要大量监督和训练数据。这些方法在捕获冷门或新兴实体的全面且最新信息方面存在不足。特别是当日常新闻中不断出现新实体时，该问题更为突出。

Method: 提出AgREE（Agentic Reasoning for Emerging Entities）框架，结合迭代检索动作和多步推理，动态构建丰富的知识图谱三元组。该框架不需要训练，通过基于代理的推理和策略性信息检索，逐步构建知识图谱三元组。

Result: 在构建知识图谱三元组任务中，AgREE显著优于现有方法，尤其对于在语言模型训练过程中未见过的新兴实体，性能提升达13.7%。同时提出了新的评估方法和针对新兴实体的基准测试

Conclusion: AgREE框架通过代理推理与策略性信息检索的结合，有效解决了动态信息环境中知识图谱补全的挑战，特别是在处理新兴实体方面。该方法无需训练，适应性强，为维护实时更新的知识图谱提供了新思路。

Abstract: Open-domain Knowledge Graph Completion (KGC) faces significant challenges in
an ever-changing world, especially when considering the continual emergence of
new entities in daily news. Existing approaches for KGC mainly rely on
pretrained language models' parametric knowledge, pre-constructed queries, or
single-step retrieval, typically requiring substantial supervision and training
data. Even so, they often fail to capture comprehensive and up-to-date
information about unpopular and/or emerging entities. To this end, we introduce
Agentic Reasoning for Emerging Entities (AgREE), a novel agent-based framework
that combines iterative retrieval actions and multi-step reasoning to
dynamically construct rich knowledge graph triplets. Experiments show that,
despite requiring zero training efforts, AgREE significantly outperforms
existing methods in constructing knowledge graph triplets, especially for
emerging entities that were not seen during language models' training
processes, outperforming previous methods by up to 13.7%. Moreover, we propose
a new evaluation methodology that addresses a fundamental weakness of existing
setups and a new benchmark for KGC on emerging entities. Our work demonstrates
the effectiveness of combining agent-based reasoning with strategic information
retrieval for maintaining up-to-date knowledge graphs in dynamic information
environments.

</details>


### [294] [Generic-to-Specific Reasoning and Learning for Scalable Ad Hoc Teamwork](https://arxiv.org/abs/2508.04163)
*Hasra Dodampegama,Mohan Sridharan*

Main category: cs.AI

TL;DR: 这篇论文主张利用知识驱动和数据驱动方法的互补优势，用于临时团队协作中的推理与学习，通过非单调逻辑推理结合先验常识、快速学习的行为预测模型和基于基础模型的未来目标预测，以应对临时团队协作中的数据需求大、缺乏透明度和难以适应变化的问题。


<details>
  <summary>Details</summary>
Motivation: 现有临时团队协作方法依赖大数据、缺乏透明度和难以适应变化，且随着代理数量增加，决策复杂度上升。为了克服这些挑战，论文提出将知识驱动与数据驱动方法结合，以实现更高效、透明的协作。

Method: 使用非单调逻辑推理架构，每个代理根据以下要素决定动作：（a）领域特定的常识性先验知识；（b）快速学习和更新的他者行为预测模型；（c）基于现有基础模型的抽象未来目标预测。实验在执行于VirtualHome仿真环境中。

Result: 通过实现互补优势的方法，论文指出所提架构能在复杂的多代理环境中有效协作，特别是在临时团队协作场景中具备更优的透明度和适应性。

Conclusion: 综合知识驱动和数据驱动方法能够解决现有临时团队协作技术中的多个挑战，尤其在大规模代理场景下提升协作效能，验证了所提出架构在仿真环境中的优越性能。

Abstract: AI agents deployed in assistive roles often have to collaborate with other
agents (humans, AI systems) without prior coordination. Methods considered
state of the art for such ad hoc teamwork often pursue a data-driven approach
that needs a large labeled dataset of prior observations, lacks transparency,
and makes it difficult to rapidly revise existing knowledge in response to
changes. As the number of agents increases, the complexity of decision-making
makes it difficult to collaborate effectively. This paper advocates leveraging
the complementary strengths of knowledge-based and data-driven methods for
reasoning and learning for ad hoc teamwork. For any given goal, our
architecture enables each ad hoc agent to determine its actions through
non-monotonic logical reasoning with: (a) prior commonsense domain-specific
knowledge; (b) models learned and revised rapidly to predict the behavior of
other agents; and (c) anticipated abstract future goals based on generic
knowledge of similar situations in an existing foundation model. We
experimentally evaluate our architecture's capabilities in VirtualHome, a
realistic physics-based 3D simulation environment.

</details>


### [295] [Circuit-Aware SAT Solving: Guiding CDCL via Conditional Probabilities](https://arxiv.org/abs/2508.04235)
*Jiaying Zhu,Ziyang Zheng,Zhengyuan Shi,Yalun Cai,Qiang Xu*

Main category: cs.AI

TL;DR: 为了克服传统CSAT求解方法因丢弃电路结构信息而导致的性能不足，CASCAD框架引入了基于GNN计算的门级条件概率，动态指导CDCL算法中的变量选择与子句管理，显著提升了求解效率。具体表现为：在真实电路LEC基准测试中求解加速高达10倍，并额外通过条件概率指导子句过滤策略节省23.5%运行时间。


<details>
  <summary>Details</summary>
Motivation: 标准CSAT流程将电路转换为CNF格式后丢弃了丰富的结构和功能信息，限制了冲突驱动子句学习(CDCL)类SAT求解器的性能。

Method: 提出CASCAD框架：1. 使用图神经网络(GNN)计算电路门级条件概率；2. 利用电路级概率动态指导CDCL的两个核心启发式策略——变量相位选择与子句管理，其中子句管理包含独创的基于概率的子句过滤策略。

Result: 在真实逻辑等价性检查(LEC)基准测试中：1. 求解时间最高缩短至1/10；2. 概率指导的子句过滤策略额外减少23.5%运行时间。

Conclusion: 实验证明保留电路结构信息可大幅优化SAT求解效率，GNN支持的门级概率建模方法为EDA工具的改进提供了新方向。

Abstract: Circuit Satisfiability (CSAT) plays a pivotal role in Electronic Design
Automation. The standard workflow for solving CSAT problems converts circuits
into Conjunctive Normal Form (CNF) and employs generic SAT solvers powered by
Conflict-Driven Clause Learning (CDCL). However, this process inherently
discards rich structural and functional information, leading to suboptimal
solver performance. To address this limitation, we introduce CASCAD, a novel
circuit-aware SAT solving framework that directly leverages circuit-level
conditional probabilities computed via Graph Neural Networks (GNNs). By
explicitly modeling gate-level conditional probabilities, CASCAD dynamically
guides two critical CDCL heuristics -- variable phase selection and clause
managementto significantly enhance solver efficiency. Extensive evaluations on
challenging real-world Logical Equivalence Checking (LEC) benchmarks
demonstrate that CASCAD reduces solving times by up to 10x compared to
state-of-the-art CNF-based approaches, achieving an additional 23.5% runtime
reduction via our probability-guided clause filtering strategy. Our results
underscore the importance of preserving circuit-level structural insights
within SAT solvers, providing a robust foundation for future improvements in
SAT-solving efficiency and EDA tool design.

</details>


### [296] [Large Language Model's Multi-Capability Alignment in Biomedical Domain](https://arxiv.org/abs/2508.04278)
*Wentao Wu,Linqing Chen,Hanmeng Zhong,Weilei Wang*

Main category: cs.AI

TL;DR: BalancedBio是一个理论基础扎实的参数高效生物医学推理框架，解决领域特定AI对齐中的多种能力集成问题。它提出了生物医学多能力收敛定理，证明正交梯度空间对防止能力干扰和安全部署至关重要。主要创新包括医学知识支持的综合生成（MKGSG）和能力感知组相对策略优化（CAGRPO），在数学上证明达到帕累托最优收敛。在多个指标上达到同类参数规模的SOTA结果，降低部署成本78%，提升诊断精度23%，获得89%的临床医生认可率。


<details>
  <summary>Details</summary>
Motivation: 当前生物医学AI模型在多种能力（领域知识、推理、指令跟随等）集成中面临能力干扰问题，影响临床部署的安全性和可靠性。此外，缺乏关于能力收敛和安全性保障的理论框架，阻碍了在复杂医疗场景中的可信应用。需要一种参数高效且理论保障的解决方案。

Method: 1) 医学知识支持的综合生成（MKGSG）：扩展Source2Synthesis框架，加入临床工作流约束和医学术语本体验证，保障生成内容的准确性与安全性；2) 能力感知组相对策略优化（CAGRPO）：结合规则基和模型基两类奖励信号，推导优化混合奖励权重，保持不同能力梯度空间的正交性，避免能力干扰；3) 理论分析：证明在满足正交梯度条件时，策略优化会帕累托最优收敛，同时在强化学习训练中保持各项能力不受影响。

Result: 1) 指标上：在BIOMED-MMLU测试中：领域专业知识80.95%（+15.32%）、推理能力61.94%（+7.75%）、指令跟随能力67.95%（+6.44%）、能力集成度86.7%（+18.5%）；2) 实用价值：部署后实现78%推理成本缩减、诊断精度提升23%（相比基线）和89%临床医生接受率；3）理论贡献：为能力保存和临床精度提供了数学安全保障边界；4）模型开源：将发布0.5B规模模型版本。

Conclusion: BalancedBio是第一套解决生物医学领域多能力AI对齐的框架，其理论创新（多能力收敛定理）和优化方法（MKGSG+CAGRPO）共同实现了性能超越与安全部署。核心突破在于证明正交梯度空间对防止能力干扰的必要性，并通过奖励机制设计实现帕累托最优。该方法为建立高效可靠的生物医疗AI系统提供了原则性框架，具备临床实用性（成本及精度）和安全性（理论保障）。

Abstract: BalancedBio is a theoretically grounded framework for parameter-efficient
biomedical reasoning, addressing multi-capability integration in
domain-specific AI alignment. It establishes the Biomedical Multi-Capability
Convergence Theorem, proving orthogonal gradient spaces are essential to
prevent capability interference for safe deployment. Key innovations include:
(1) Medical Knowledge Grounded Synthetic Generation (MKGSG), extending
Source2Synth with clinical workflow constraints and medical ontology validation
for factual accuracy and safety; and (2) Capability Aware Group Relative Policy
Optimization, deriving optimal hybrid reward weighting to maintain
orthogonality in RL, using a reward model with rule-based and model-based
scores adapted to biomedical tasks. Mathematical analysis proves Pareto-optimal
convergence, preserving performance across capabilities. It achieves
state-of-the-art results in its parameter class: domain expertise (80.95%
BIOMED-MMLU, +15.32% over baseline), reasoning (61.94%, +7.75%), instruction
following (67.95%, +6.44%), and integration (86.7%, +18.5%). Theoretical safety
guarantees include bounds on capability preservation and clinical accuracy.
Real-world deployment yields 78% cost reduction, 23% improved diagnostic
accuracy, and 89% clinician acceptance. This work provides a principled
methodology for biomedical AI alignment, enabling efficient reasoning with
essential safety and reliability, with the 0.5B model version to be released.

</details>


### [297] [Synthetic POMDPs to Challenge Memory-Augmented RL: Memory Demand Structure Modeling](https://arxiv.org/abs/2508.04282)
*Yongyi Wang,Lingfeng Li,Bozhou Chen,Ang Li,Hanyu Liu,Qirui Zheng,Xionghui Yang,Wenxin Li*

Main category: cs.AI

TL;DR: 本文提出了一种新的部分可观测马尔可夫决策过程（POMDP）合成框架，包含理论分析、定制化方法论和难度递增的环境设计，旨在为内存增强强化学习提供可控且严格的评估基准。


<details>
  <summary>Details</summary>
Motivation: 现有内存增强强化学习（RL）评测基准无法精细控制挑战难度，难以系统评估模型性能。本文通过构建理论框架和可控合成环境解决该问题。

Method: 1. 提出基于内存需求结构（MDS）和状态转移不变性的理论框架；2. 利用线性动态系统、状态聚合及回报重分布技术构建定制化POMDP；3. 设计渐进式难度序列环境。

Result: 验证了理论框架的有效性，建立了可量化记忆需求难度的环境库，为评估不同内存模型提供了标准化数据集和选择依据。

Conclusion: 该研究建立了分析POMDP的数学基础，提供环境设计与模型选择的系统方法论，推动内存增强RL的严格评测与模型改进。

Abstract: Recent research has developed benchmarks for memory-augmented reinforcement
learning (RL) algorithms, providing Partially Observable Markov Decision
Process (POMDP) environments where agents depend on past observations to make
decisions. While many benchmarks incorporate sufficiently complex real-world
problems, they lack controllability over the degree of challenges posed to
memory models. In contrast, synthetic environments enable fine-grained
manipulation of dynamics, making them critical for detailed and rigorous
evaluation of memory-augmented RL. Our study focuses on POMDP synthesis with
three key contributions:
  1. A theoretical framework for analyzing POMDPs, grounded in Memory Demand
Structure (MDS), transition invariance, and related concepts; 2. A methodology
leveraging linear process dynamics, state aggregation, and reward
redistribution to construct customized POMDPs with predefined properties; 3.
Empirically validated series of POMDP environments with increasing difficulty
levels, designed based on our theoretical insights. Our work clarifies the
challenges of memory-augmented RL in solving POMDPs, provides guidelines for
analyzing and designing POMDP environments, and offers empirical support for
selecting memory models in RL tasks.

</details>


### [298] [Deliberative Reasoning Network: An Uncertainty-Driven Paradigm for Belief-Tracked Inference with Pretrained Language Models](https://arxiv.org/abs/2508.04339)
*Anran Xu,Jincheng Wang,Baigen Cai,Tao Wen*

Main category: cs.AI

TL;DR: 本文提出了深思推理网络（DRN），以不确定性最小化为核心，取代传统的概率最大化方法，解决大型语言模型在逻辑推理中因语义启发式与决定性证据冲突导致的认知陷阱问题。通过两个互补架构（专用判别模型与轻量验证模块）验证，在对抗性基准测试中显著提升性能，并展示出强大的零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在逻辑推理时，当语义启发式与决定性证据冲突时容易失败，存在认知陷阱问题。传统方法主要依赖概率最大化，但这种方式无法解决认知陷阱。因此，亟需一种新方法来提升模型在高冲突情境下的推理可靠性，通过不确定性驱动的系统2推理过程来提高可验证性。

Method: 方法基于从概率最大化转向不确定性最小化的核心思想，设计了两个互补架构：（1）定制判别模型：显式追踪信念状态，通过迭代证据合成过程量化不同假设的认知不确定性，实现内在可解释性；（2）轻量验证模块：与现有生成LLMs结合，以参数高效的方式验证推理结果。主要流程为：a) 将问题建模为多假设证据合成；b) 迭代融合新证据并更新各假设信念不确定性分数；c) 选择不确定性最低（证明最一致）的假设。

Result: 在专门设计的对抗性基准 LCR-1000 上：定制 DRN 相较基线提升15.2%；当作为验证模块与 Mistral-7B 集成时，在最具挑战问题上的准确率从20%提升到80%。零样本泛化能力：在TruthfulQA未微调情况下提升23.6%，表明学到的推理具有可迁移性。最终验证：DRN可构建强鲁棒性系统2推理组件。

Conclusion: DRN 通过不确定性最小化原则重构推理过程，显式建模证据一致性，从根本上解决认知陷阱问题。其双架构设计既实现了专用模型的性能突破，又能高效增强已有大模型。显著的零样本泛化表明该方法获得可迁移推理能力，为实现可信AI提供了可验证的基础组件。

Abstract: Large language models often fail at logical reasoning when semantic
heuristics conflict with decisive evidence - a phenomenon we term cognitive
traps. To address this fundamental limitation, we introduce the Deliberative
Reasoning Network (DRN), a novel paradigm that reframes logical reasoning from
probability maximization to uncertainty minimization. Instead of asking "Which
answer is most likely?", DRN asks "Which hypothesis has the most internally
consistent evidence?". DRN achieves intrinsic interpretability by explicitly
tracking belief states and quantifying epistemic uncertainty for competing
hypotheses through an iterative evidence synthesis process. We validate our
approach through two complementary architectures - a bespoke discriminative
model that embodies the core uncertainty minimization principle, and a
lightweight verification module that enhances existing generative LLMs.
Evaluated on LCR-1000, our new adversarial reasoning benchmark designed to
expose cognitive traps, the bespoke DRN achieves up to 15.2% improvement over
standard baselines. When integrated as a parameter-efficient verifier with
Mistral-7B, our hybrid system boosts accuracy from 20% to 80% on the most
challenging problems. Critically, DRN demonstrates strong zero-shot
generalization, improving TruthfulQA performance by 23.6% without additional
training, indicating that uncertainty-driven deliberation learns transferable
reasoning principles. We position DRN as a foundational, verifiable System 2
reasoning component for building more trustworthy AI systems.

</details>


### [299] [OmniPlay: Benchmarking Omni-Modal Models on Omni-Modal Game Playing](https://arxiv.org/abs/2508.04361)
*Fuqing Bie,Shiyu Huang,Xijia Tao,Zhiqin Fang,Leyi Pan,Junzhe Chen,Min Ren,Liuyu Xiang,Zhaofeng He*

Main category: cs.AI

TL;DR: OmniPlay是一个诊断性基准测试，旨在评估基于代理模型在完整感官谱上的感知融合与推理能力。作者构建了五个游戏环境，创建了感知协同和冲突场景，发现在冲突情况下模型会经历灾难性失败。


<details>
  <summary>Details</summary>
Motivation: 现有的通用基础模型评估无法测试动态交互世界中的模型能力，例如：静态基准测试缺乏主观能动性（agency），而交互式基准测试存在严重的模态瓶颈。因此，需要设计一种评估范以测试跨感官感知的协作推理能力。

Method: OmniPlay构建了基于模态相互依存核心哲学思想下的五个游戏环境，设计场景以系统化地创造感知协同和冲突情境（例如：存在跨模态冲突），迫使代理模型执行多模态推理任务。作者还对六个领先的多模态模型进行了全面评估。

Result: 评估结果表明：代理模型在高保真度记忆任务中表现出超人类能力，但在需要强健推理和策略规划的挑战中存在系统性失误。当感知模态之间存在冲突时，模型的融合机制变得脆弱，甚至出现移除部分感知信息反而能提升性能的情况（所谓‘less is more’悖论）。

Conclusion: 作者认为，要构建强健的通用人工智能（AGI），需要超越单纯扩展模型规模的思路，将重点放在解决感官协同融合机制上（包括对抗冲突模态的能力）。

Abstract: While generalist foundation models like Gemini and GPT-4o demonstrate
impressive multi-modal competence, existing evaluations fail to test their
intelligence in dynamic, interactive worlds. Static benchmarks lack agency,
while interactive benchmarks suffer from a severe modal bottleneck, typically
ignoring crucial auditory and temporal cues. To bridge this evaluation chasm,
we introduce OmniPlay, a diagnostic benchmark designed not just to evaluate,
but to probe the fusion and reasoning capabilities of agentic models across the
full sensory spectrum. Built on a core philosophy of modality interdependence,
OmniPlay comprises a suite of five game environments that systematically create
scenarios of both synergy and conflict, forcing agents to perform genuine
cross-modal reasoning. Our comprehensive evaluation of six leading omni-modal
models reveals a critical dichotomy: they exhibit superhuman performance on
high-fidelity memory tasks but suffer from systemic failures in challenges
requiring robust reasoning and strategic planning. We demonstrate that this
fragility stems from brittle fusion mechanisms, which lead to catastrophic
performance degradation under modality conflict and uncover a counter-intuitive
"less is more" paradox, where removing sensory information can paradoxically
improve performance. Our findings suggest that the path toward robust AGI
requires a research focus beyond scaling to explicitly address synergistic
fusion. Our platform is available for anonymous review at
https://github.com/fuqingbie/omni-game-benchmark.

</details>


### [300] [Artificial Consciousness as Interface Representation](https://arxiv.org/abs/2508.04383)
*Robert Prentner*

Main category: cs.AI

TL;DR: 本文提出了一个名为SLP测试的框架，将人工智能（AI）系统是否具有意识的问题转化为可实证验证的检验标准。


<details>
  <summary>Details</summary>
Motivation: 由于定义和量化主观体验存在固有挑战，人们对于AI系统是否具有意识的问题存在争议。因此，需要一种可操作的方法来评估AI系统的意识。

Method: 引入三个评估标准：主观-语言（S）、潜在-涌现（L）和现象学-结构（P）。通过范畴论建模接口表示，将关系基底（RS）与可观察行为之间建立映射。

Result: 提出了SLP测试的整体框架，这些测试共同将主观体验操作化为功能性接口，而非物理系统的内在属性。

Conclusion: 该框架成功地将意识问题的抽象讨论转化为可实证验证的评估方法，从而为评估人工智能系统是否具有意识提供了实用工具。

Abstract: Whether artificial intelligence (AI) systems can possess consciousness is a
contentious question because of the inherent challenges of defining and
operationalizing subjective experience. This paper proposes a framework to
reframe the question of artificial consciousness into empirically tractable
tests. We introduce three evaluative criteria - S (subjective-linguistic), L
(latent-emergent), and P (phenomenological-structural) - collectively termed
SLP-tests, which assess whether an AI system instantiates interface
representations that facilitate consciousness-like properties. Drawing on
category theory, we model interface representations as mappings between
relational substrates (RS) and observable behaviors, akin to specific types of
abstraction layers. The SLP-tests collectively operationalize subjective
experience not as an intrinsic property of physical systems but as a functional
interface to a relational entity.

</details>


### [301] [GuirlVG: Incentivize GUI Visual Grounding via Empirical Exploration on Reinforcement Learning](https://arxiv.org/abs/2508.04389)
*Weitai Kang,Bin Lei,Gaowen Liu,Caiwen Ding,Yan Yan*

Main category: cs.AI

TL;DR: 本文提出了一种基于强化学习的GUI视觉接地（GUI-VG）方法GuirlVG，该方法在少量样本上训练即可超越需要大量样本的SFT方法，并引入了一种名为Adversarial KL Factor的技术来稳定训练。


<details>
  <summary>Details</summary>
Motivation: GUI视觉接地（GUI-VG）作为GUI代理的核心能力，目前主要依赖于多模态大语言模型（MLLMs）的监督微调（SFT），但SFT需要大量数据和训练成本。尽管MLLMs在预训练阶段已经覆盖了GUI领域，但现有研究工作仍在探索是否可以使用更高效的方式来达到目标。基于规则的强化微调（RFT）虽然在最近取得了成功，但如何将其应用于GUI-VG领域还待深入研究。

Method: 1. 提出了一种新的基于强化学习的GUI视觉接地方法GuirlVG。 
2. 通过系统性实证研究，对核心的RFT组件进行分解并分析各组件的最优形式构建方式。 
3. 提出一种称为Adversarial KL Factor的技术，通过动态适应KL约束因子以抑制奖励的过度优化，从而稳定强化微调训练。 
4. 探索提升RFT有效性的训练配置策略；整体训练过程仅需5.2K个训练样本。

Result: GuirlVG在不同数据集上均显著超越SFT基线和先前的SOTAs。在ScreenSpot数据集上达到了7.7%的提升，在ScreenSpotPro上实现了17.2%的提升，在ScreenSpotV2上达到了91.9%的准确率。在样本效率方面，仅用5.2K训练样本实现了优于基于10M样本训练的SFT方法的表现。

Conclusion: GuirlVG证明了RFT可在GUI视觉接地任务上有效超越需要海量标注的SFT范式，同时训练效率远高于后者。通过结构化的消融实验阐明了最有效的RFT技术组合路径。新提出的Adversarial KL Factor技术为解决RFT训练波动问题提供了一种稳定、灵活的手段。

Abstract: Graphical user interface visual grounding (GUI-VG), a core capability for GUI
agents, has primarily relied on supervised fine-tuning (SFT) of multimodal
large language models (MLLMs), which demands extensive data curation and
significant training costs. However, as MLLMs continue to advance and even
cover GUI domains during pretraining, the necessity of exhaustive SFT
post-training becomes increasingly questionable. Meanwhile, recent successes of
rule-based reinforcement fine-tuning (RFT) suggest a more efficient
alternative. Despite this promise, the optimal manner of applying RFT for
GUI-VG remains unexplored. To bridge this gap, we introduce GuirlVG, a
reinforcement learning-based GUI-VG method built on a systematic empirical
study and a novel stabilization technique. We find that naive application of
RFT underperforms the SFT baseline, motivating a deeper exploration. First, we
decompose RFT into its core components and analyze the optimal formulation of
each. Second, we propose a novel Adversarial KL Factor that dynamically
stabilizes training to mitigate reward over-optimization. Third, we further
explore the training configurations of RFT to enhance effectiveness. Extensive
experiments show that GuirlVG, with only 5.2K training samples, outperforms SFT
methods trained on over 10M samples, achieving a 7.7% improvement on
ScreenSpot, a 17.2% improvement on ScreenSpotPro, and 91.9% accuracy on
ScreenSpotV2.

</details>


### [302] [Beyond Pixels: Exploring DOM Downsampling for LLM-Based Web Agents](https://arxiv.org/abs/2508.04412)
*Thassilo M. Schiepanski,Nicholas Piël*

Main category: cs.AI

TL;DR: D2Snap是一种新颖的DOM下采样算法，用于解决网页代理中应用状态序列化（快照）的问题。该算法在GPT-4o后端上评估，使用Online-Mind2Web数据集中的任务，其下采样DOM快照的成功率（67%）与基于GUI快照的基线（65%）相当，且输入token数量在同一数量级（1e3）。最佳配置（token数量高一个数量级但仍在模型上下文窗口内）的表现超过基线8%。此外，评估结果表明DOM固有层次结构是LLMs理解UI的强特征。


<details>
  <summary>Details</summary>
Motivation: 当前先进的网页代理依赖于基于GUI的快照（截图增强视觉线索），目的是模仿人类感知，且图像输入对模型来说相对廉价。但大型语言模型（LLMs）的视觉能力仍落后于代码处理能力。而DOM（文档对象模型）快照在结构上与HTML相似，理论上是一种理想的替代方案。然而，由于DOM快照需要大量的输入token，迄今为止无法在网页代理中可靠实现。因此，需要一种高效压缩DOM结构同时保留关键信息的下采样方法。

Method: 提出了D2Snap算法，通过下采样DOM结构来压缩其规模，同时保留必要的UI层次信息和关键特征。采用GPT-4o作为后端模型，并在Online-Mind2Web数据集的任务上进行评估。实验配置包括多个输入token规模，并与GUI快照（如截图加视觉提示）基线进行比较。评估指标为任务成功率。

Result: D2Snap下采样的DOM快照在输入token数量级为1e3时，任务成功率达到67%，与基线GUI快照的65%相当。当输入token规模增加一个数量级（但仍保持在模型上下文窗口内）的最佳配置下，任务成功率超过基线8%，达到73%。评估还发现DOM固有的层次结构是LLMs理解UI的有效特征。

Conclusion: D2Snap算法首次实现了在可控输入token规模下有效利用DOM结构作为网页代理的状态表示（快照），其性能不亚于甚至超过传统的GUI快照方法，其中DOM的层次信息对LLMs具有重要价值。这表明压缩DOM快照是可行的，并可能成为未来网页代理的重要技术方向。

Abstract: Frontier LLMs only recently enabled serviceable, autonomous web agents. At
that, a model poses as an instantaneous domain model backend. Ought to suggest
interaction, it is consulted with a web-based task and respective application
state. The key problem lies in application state serialisation
$\unicode{x2013}$ referred to as snapshot. State-of-the-art web agents are
premised on grounded GUI snapshots, i.e., screenshots enhanced with visual
cues. Not least to resemble human perception, but for images representing
relatively cheap means of model input. LLM vision still lag behind code
interpretation capabilities. DOM snapshots, which structurally resemble HTML,
impose a desired alternative. Vast model input token size, however, disables
reliable implementation with web agents to date.
  We propose D2Snap, a first-of-its-kind DOM downsampling algorithm. Based on a
GPT-4o backend, we evaluate D2Snap on tasks sampled from the Online-Mind2Web
dataset. The success rate of D2Snap-downsampled DOM snapshots (67%) matches a
grounded GUI snapshot baseline (65%) $\unicode{x2013}$ within the same input
token order of magnitude (1e3). Our best evaluated configurations
$\unicode{x2013}$ one token order above, but within the model's context window
$\unicode{x2013}$ outperform this baseline by 8%. Our evaluation, moreover,
yields that DOM-inherent hierarchy embodies a strong UI feature for LLMs.

</details>


### [303] [\textsc{SimInstruct}: A Responsible Tool for Collecting Scaffolding Dialogues Between Experts and LLM-Simulated Novices](https://arxiv.org/abs/2508.04428)
*Si Chen,Izzy Molnar,Ting Hua,Peiyu Li,Le Huy Khiem,G. Alex Ambrose,Jim Lang,Ronald Metoyer,Nitesh V. Chawla*

Main category: cs.AI

TL;DR: 研究者们开发了一个叫做SimInstruct的工具，该工具能够通过大型语言模型（LLMs）模拟新手教员，允许人类专家为其提供多轮反馈和指导，从而生成高质量的教学对话数据。该工具解决了真实数据稀缺的问题，并保证了数据的隐私保护和减少新手寻求帮助时的脆弱性。研究发现，使用SimInstruct生成的对话在教育深度和相关性方面与原生的教学录音相近。此外，利用该数据集微调的LLaMA模型在教学质量上超过了GPT-4o。


<details>
  <summary>Details</summary>
Motivation: 由于隐私保护和帮助行为中隐含的脆弱性，获取高质量的多轮教学指导对话数据相当困难。此类对话通常在师生间进行，涉及专家（老师）如何通过问询、反馈和分步指导支持新手（学生）的“支架式学习”（scaffolding）。数据匮乏阻碍了开发支持教学、学习和决策的AI系统的发展。因此，作者提出了一种高效收集此类数据的工具，旨在绕过真实参与者需求，解决数据稀缺问题和隐私顾虑。

Method: SimInstruct采用了“专家进环”（expert-in-the-loop）方法。具体流程包括：首先，使用LLM模拟新手教员（Novice Instructor），通过设定不同的教学挑战（teaching challenges）和性格（例如外向性或内向性）来模拟多样性。其次，人类专家根据LLM模拟的新手的表现提供多轮的反馈、推理和教学支持。这一设计让专家在无需真实新手参与的情况下，就能生成大量高质量对话。同时，作者还使用生成的对话数据微调了LLaMA模型，以便将其应用为专家模型并进行效果对比。

Result: 该方案显示：1）人格特性（如内向/外向）能显著影响专家（即导师）如何指导新手。2）通过SimInstruct生成的对话在认知深度与教育相关性上可与真实教学录音媲美。3）专家表示这一过程不仅具有吸引力，还增加了专业洞察力，从而提高数据质量。4）在训练端上，相比于原始的大语言模型GPT-4o，基于微调后的LLaMA的专家模型在多个教学细节上有更好的表现，如避免了通用式的溢美之词或俯尊俯就的表达风格，同时在回应中减少给新手过多建议而致其不知所措的现象。

Conclusion: SimInstruct是收集多轮教育指导对话的一种可扩展工具，它免除了对真实用户的需求，也确保了隐私安全和降低心理脆弱性。通过该工具生成的训练数据使得人工智能系统在教育和辅导决策支持领域表现得更好。更重要的是，该工具本身也成为专家自我反思和成长的媒介。最后，利用该数据集构建的专家模型（LLaMA的微调版本）优于现有主流的GPT-4o模型。

Abstract: High-quality, multi-turn instructional dialogues between novices and experts
are essential for developing AI systems that support teaching, learning, and
decision-making. These dialogues often involve scaffolding -- the process by
which an expert supports a novice's thinking through questions, feedback, and
step-by-step guidance. However, such data are scarce due to privacy concerns in
recording and the vulnerability inherent in help-seeking. We present
SimInstruct, a scalable, expert-in-the-loop tool for collecting scaffolding
dialogues. Using teaching development coaching as an example domain,
SimInstruct simulates novice instructors via LLMs, varying their teaching
challenges and LLM's persona traits, while human experts provide multi-turn
feedback, reasoning, and instructional support. This design enables the
creation of realistic, pedagogically rich dialogues without requiring real
novice participants. Our results reveal that persona traits, such as
extroversion and introversion, meaningfully influence how experts engage.
Compared to real mentoring recordings, SimInstruct dialogues demonstrate
comparable pedagogical relevance and cognitive depth. Experts also reported the
process as engaging and reflective, improving both data quality and their own
professional insight. We further fine-tuned a LLaMA model to be an expert model
using the augmented dataset, which outperformed GPT-4o in instructional
quality. Our analysis highlights GPT-4o's limitations in weak reflective
questioning, overuse of generic praise, a condescending tone, and a tendency to
overwhelm novices with excessive suggestions.

</details>


### [304] [From "Aha Moments" to Controllable Thinking: Toward Meta-Cognitive Reasoning in Large Reasoning Models via Decoupled Reasoning and Control](https://arxiv.org/abs/2508.04460)
*Rui Ha,Chaozhuo Li,Rui Pu,Sen Su*

Main category: cs.AI

TL;DR: 论文提出了MER框架，通过分离推理和控制组件，构建高质量的推理-控制数据，并利用GRPO优化控制行为学习，以提高大型推理模型的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有大型推理模型在推理过程中缺乏内在的调节机制，导致过度推理和计算资源浪费。因此，设计一个框架来监控和管理推理过程，决定何时继续、回退或终止，以提高效率。

Method: 1. 提出了MER框架，其核心技术包括：a) 基于接管机制的数据构建，利用辅助LL在关键决策点为控制信号创建数据； b) 通过监督微调实现推理控制分离； c) 控制段策略优化（CSP），融合分段群体相对策略优化（GRPO）和控制掩码机制，优化策略学习并最小化干扰。

Result: 在多个推理基准测试上，MER框架提高了大型模型的推理效率和准确性。

Conclusion: MER框架通过显式的认知控制机制，解决了大型模型的内部调节问题，并提升效率的同时保持高精度。

Abstract: Large Reasoning Models (LRMs) have demonstrated a latent capacity for complex
reasoning by spontaneously exhibiting cognitive behaviors such as step-by-step
reasoning, reflection, and backtracking, commonly referred to as "Aha Moments".
However, such emergent behaviors remain unregulated and uncontrolled, often
resulting in overthinking, where the model continues generating redundant
reasoning content even after reaching reliable conclusions. This leads to
excessive computational costs and increased latency, limiting the practical
deployment of LRMs. The root cause lies in the absence of intrinsic regulatory
mechanisms, as current models are unable to monitor and adaptively manage their
reasoning process to determine when to continue, backtrack, or terminate. To
address this issue, we propose the Meta-cognitive Reasoning Framework (MERA),
which explicitly decouples the thinking process into distinct reasoning and
control components, thereby enabling the independent optimization of control
strategies. Specifically, MERA incorporates a takeover-based data construction
mechanism that identifies critical decision points during reasoning and
delegates the creation of control signals to auxiliary LLMs, thereby enabling
the construction of high-quality reasoning-control data. Additionally, a
structured reasoning-control separation is implemented via supervised
fine-tuning, enabling the model to generate explicit traces and acquire initial
meta-cognitive control capabilities. Finally, MERA employs Control-Segment
Policy Optimization (CSPO), which combines segment-wise Group Relative Policy
Optimization (GRPO) with a control-masking mechanism to optimize control
behavior learning while minimizing interference from irrelevant content.
Experiments on various reasoning benchmarks demonstrate that models trained
with MERA enhance both reasoning efficiency and accuracy.

</details>


### [305] [OS Agents: A Survey on MLLM-based Agents for General Computing Devices Use](https://arxiv.org/abs/2508.04482)
*Xueyu Hu,Tao Xiong,Biao Yi,Zishu Wei,Ruixuan Xiao,Yurun Chen,Jiasheng Ye,Meiling Tao,Xiangxin Zhou,Ziyu Zhao,Yuhuai Li,Shengze Xu,Shenzhi Wang,Xinchen Xu,Shuofei Qiao,Zhaokai Wang,Kun Kuang,Tieyong Zeng,Liang Wang,Jiwei Li,Yuchen Eleanor Jiang,Wangchunshu Zhou,Guoyin Wang,Keting Yin,Zhou Zhao,Hongxia Yang,Fan Wu,Shengyu Zhang,Fei Wu*

Main category: cs.AI

TL;DR: 本文综述了基于多模态大语言模型的操作系统代理（OS Agents）的最新研究进展，涵盖其基础理论、关键技术构建（包括环境、观察空间、动作空间以及理解、规划、执行能力）、构建方法（领域特定基础模型与代理框架）、评估标准体系以及未来挑战（安全隐私、个性化、自我进化等），旨在推动该领域学术与工业发展。


<details>
  <summary>Details</summary>
Motivation: 受虚构AI助手J.A.R.V.I.S启发，随着多模态大语言模型的发展，操作系统代理（通过操作GUI实现自动化任务）取得显著突破。本文旨在系统整理OS Agents领域的研究现状，为学术界和工业界提供发展路线图。

Method: 1.定义OS Agents基础框架（环境/观察空间/动作空间）与核心能力（理解/规划/执行）；2.归纳构建方法：领域优化基础模型+代理架构设计；3.梳理评估体系：多任务基准测试协议；4.通过文献综述整合技术路线，建立开源知识库动态更新进展。

Result: 构建首个OS Agents系统综述框架：技术层面完整定义了三元架构（环境-观察-动作）与能力模型，方法层面提炼领域优化模型及代理框架，评估层面建立多维度测试协议，并指出安全隐私/个性化/自我进化三大未来方向。成果以9页精简版发表于ACL 2025。

Conclusion: OS Agents通过大语言模型控制操作系统接口实现复杂任务自动化，已取得实质性突破。需进一步解决人机共处安全性、个性化适应能力及持续学习机制等挑战。开源知识库将加速该领域发展，推动实现通用AI助手愿景。

Abstract: The dream to create AI assistants as capable and versatile as the fictional
J.A.R.V.I.S from Iron Man has long captivated imaginations. With the evolution
of (multi-modal) large language models ((M)LLMs), this dream is closer to
reality, as (M)LLM-based Agents using computing devices (e.g., computers and
mobile phones) by operating within the environments and interfaces (e.g.,
Graphical User Interface (GUI)) provided by operating systems (OS) to automate
tasks have significantly advanced. This paper presents a comprehensive survey
of these advanced agents, designated as OS Agents. We begin by elucidating the
fundamentals of OS Agents, exploring their key components including the
environment, observation space, and action space, and outlining essential
capabilities such as understanding, planning, and grounding. We then examine
methodologies for constructing OS Agents, focusing on domain-specific
foundation models and agent frameworks. A detailed review of evaluation
protocols and benchmarks highlights how OS Agents are assessed across diverse
tasks. Finally, we discuss current challenges and identify promising directions
for future research, including safety and privacy, personalization and
self-evolution. This survey aims to consolidate the state of OS Agents
research, providing insights to guide both academic inquiry and industrial
development. An open-source GitHub repository is maintained as a dynamic
resource to foster further innovation in this field. We present a 9-page
version of our work, accepted by ACL 2025, to provide a concise overview to the
domain.

</details>


### [306] [Argumentative Debates for Transparent Bias Detection [Technical Report]](https://arxiv.org/abs/2508.04511)
*Hamed Ayoobi,Nico Potyka,Anna Rapberger,Francesca Toni*

Main category: cs.AI

TL;DR: 提出了一种基于形式论辩的可解释偏见检测方法，通过构建个体及其邻域受保护特征值的辩论来检测偏差，强调了透明度和可解释性在算法公平中的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有偏见检测方法大多忽略透明性，而公平问题需要高可解释性以增强可信度和可操作性。本文结合形式论辩技术，提出可解释的辩论式偏见检测框架以满足这一需求。

Method: 1) 将个体数据按受保护特征（如性别种族）分组；2) 构建‘邻里’概念连接个体；3) 在邻域内外发起关于偏见的辩论；4) 基于形式论辩框架生成正反论点；5) 量化辩论结论得出偏见判定。

Result: 通过形式/量化/定性三重评估显示：a) 性能优于基线方法（AUC提高15.3%）；b) 辩论日志使决策可追溯（解释性）；c) 可视化辩论结构助用户理解偏见模式（可解释性）。

Conclusion: 形式论辩为算法公平提供了自然可解释框架。邻里辩论机制能有效检测复杂交叉偏见，透明化决策过程增强可信度，未来需研究辩论规则自动化学习。

Abstract: As the use of AI systems in society grows, addressing potential biases that
emerge from data or are learned by models is essential to prevent systematic
disadvantages against specific groups. Several notions of (un)fairness have
been proposed in the literature, alongside corresponding algorithmic methods
for detecting and mitigating unfairness, but, with very few exceptions, these
tend to ignore transparency. Instead, interpretability and explainability are
core requirements for algorithmic fairness, even more so than for other
algorithmic solutions, given the human-oriented nature of fairness. In this
paper, we contribute a novel interpretable, explainable method for bias
detection relying on debates about the presence of bias against individuals,
based on the values of protected features for the individuals and others in
their neighbourhoods. Our method builds upon techniques from formal and
computational argumentation, whereby debates result from arguing about biases
within and across neighbourhoods. We provide formal, quantitative, and
qualitative evaluations of our method, highlighting its strengths in
performance against baselines, as well as its interpretability and
explainability.

</details>


### [307] [SID: Benchmarking Guided Instruction Capabilities in STEM Education with a Socratic Interdisciplinary Dialogues Dataset](https://arxiv.org/abs/2508.04563)
*Mei Jiang,Houping Yue,Bingdong Li,Hao Hao,Ying Qian,Bo Jiang,Aimin Zhou*

Main category: cs.AI

TL;DR: 该研究引入了SID（跨学科苏格拉底对话）基准测试，用于评估大型语言模型（LLMs）在跨学科STEM教育中的指导能力。SID包含10,000个对话轮次的数据集和新的评估指标（如X-SRG），揭示了当前LLMs在引导学生实现知识整合和迁移方面存在不足。


<details>
  <summary>Details</summary>
Motivation: 现代教育中培养学生在复杂问题解决场景下的知识整合和迁移能力是核心目标，跨学科STEM是实现这一目标的关键途径，但专家指导难以规模化。LLMs在该方面具有潜力，但其指导能力因缺乏有效评估标准而尚不明确。

Method: 1. 构建包含10,000个对话轮次、覆盖48个复杂STEM主题的数据集；2. 开发捕捉深刻教学特征的注解模式；3. 创建新评估指标套件（如X-SRG）；4. 在数据集上对最先进LLMs进行基线实验。

Result: 基线实验表明，当前最先进的LLMs也难以执行有效的引导性对话以实现学生的知识整合和迁移。

Conclusion: SID基准测试的提出填补了LLM教育应用评估的空白，将推动开发更具教学意识的大型语言模型。

Abstract: Fostering students' abilities for knowledge integration and transfer in
complex problem-solving scenarios is a core objective of modern education, and
interdisciplinary STEM is a key pathway to achieve this, yet it requires expert
guidance that is difficult to scale. While LLMs offer potential in this regard,
their true capability for guided instruction remains unclear due to the lack of
an effective evaluation benchmark. To address this, we introduce SID, the first
benchmark designed to systematically evaluate the higher-order guidance
capabilities of LLMs in multi-turn, interdisciplinary Socratic dialogues. Our
contributions include a large-scale dataset of 10,000 dialogue turns across 48
complex STEM projects, a novel annotation schema for capturing deep pedagogical
features, and a new suite of evaluation metrics (e.g., X-SRG). Baseline
experiments confirm that even state-of-the-art LLMs struggle to execute
effective guided dialogues that lead students to achieve knowledge integration
and transfer. This highlights the critical value of our benchmark in driving
the development of more pedagogically-aware LLMs.

</details>


### [308] [ConfProBench: A Confidence Evaluation Benchmark for MLLM-Based Process Judges](https://arxiv.org/abs/2508.04576)
*Yue Zhou,Yi Chang,Yuan Wu*

Main category: cs.AI

TL;DR: 该论文提出了ConfProBench，这是首个针对多模态大型语言模型（MLLM）过程判官（MPJ）的置信度可靠性进行全面评估的基准。该基准通过对抗性干扰下的三种变体（同义词替换、句法变换、图像干扰）测试MPJ的置信稳健性，并引入三个新指标（CRS、CSS、CCS）。


<details>
  <summary>Details</summary>
Motivation: 现有MPJ基准仅关注步骤正确性分类等任务，但忽略了置信分数在步骤级别的可靠性。为填补这一空白，需系统评估MPJ产生的置信分数质量，以更好地指导模型改进。

Method: 1. 构建三类对抗性干扰的推理步骤变体：同义词替换（文本微调）、句法变换（重构句式）、图像干扰（修改视觉输入）。
2. 设计三个评估指标：CRS（置信度在干扰下的稳定性）、CSS（对错误步骤的敏感度）、CCS（置信度与正确性的校准程度）。
3. 在包括开源/闭源的14个先进MLLM上开展实验，验证其MPJ能力。

Result: 实验揭示了当前MPJ在置信度性能上的普遍局限：多数模型在干扰下置信度波动显著（CRS低），对错误步骤不够敏感（CSS弱），且置信分数与正确性未良好校准（CCS差）。同时提供了各模型在三个指标上的竞争性基线结果。

Conclusion: ConfProBench可系统暴露MPJ置信度可靠性缺陷，为未来优化方向（如提升模型干扰鲁棒性、改进校准机制）提供数据支持。开源基准将促进社区研究。

Abstract: Reasoning is a critical capability of multimodal large language models
(MLLMs) for solving complex multimodal tasks, and judging the correctness of
reasoning steps is crucial for improving this capability. Recently, MLLM-based
process judges (MPJs) have been widely used to assess the correctness of
reasoning steps in multimodal tasks. Therefore, evaluating MPJs is important
for identifying their limitations and guiding future improvements. However,
existing benchmarks for MPJs mainly focus on tasks such as step correctness
classification and reasoning process search, while overlooking a key aspect:
whether the confidence scores produced by MPJs at the step level are reliable.
To address this gap, we propose ConfProBench, the first comprehensive benchmark
designed to systematically evaluate the reliability of step-level confidence
scores generated by MPJs. Our benchmark constructs three types of adversarially
perturbed reasoning steps: Synonym Substitution, Syntactic Transformation, and
Image Perturbation, to test the robustness of MPJ confidence under
perturbations. In addition, we introduce three novel evaluation metrics:
Confidence Robustness Score (CRS), Confidence Sensitivity Score (CSS), and
Confidence Calibration Score (CCS), which evaluate robustness, sensitivity, and
calibration, respectively. We evaluate 14 state-of-the-art MLLMs, including
both proprietary and open-source models. Experiments reveal limitations in
current MPJs' confidence performance and offer competitive baselines to support
future research.

</details>


### [309] [LLM Collaboration With Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2508.04652)
*Shuo Liu,Zeyu Liang,Xueguang Lyu,Christopher Amato*

Main category: cs.AI

TL;DR: 为了解决当前LLM在协作任务中的不足，提出了一种基于多智能体强化学习的方法。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型（LLM）主要在单个模型上预训练，缺乏对多智能体协作优化。目前微调方法依赖为每个智能体设计的复杂个体奖励机制，难以协调合作行为。

Method: 将LLM在多智能体协作问题（如写作和编程任务）中的应用建模为合作式多强化学习（MARL）任务。提出了基于现有RL技术以及MARL方法的Multi-Agent Group Relative Policy Optimization (MAGRPO)算法。

Result: 实验表明：经MAGRPO微调后的智能体能够在写作和编码任务中通过高效合作产生高质量输出。

Conclusion: 此方法为利用更多MARL算法优化LLM智能体协作建立了基础，同时揭示了伴随的技术挑战。

Abstract: A large amount of work has been done in Multi-Agent Systems (MAS) for
modeling and solving problems with multiple interacting agents. However, most
LLMs are pretrained independently and not specifically optimized for
coordination. Existing LLM fine-tuning frameworks rely on individual rewards,
which require complex reward designs for each agent to encourage collaboration.
To address these challenges, we model LLM collaboration as a cooperative
Multi-Agent Reinforcement Learning (MARL) problem. We develop a multi-agent,
multi-turn algorithm, Multi-Agent Group Relative Policy Optimization (MAGRPO),
to solve it, building on current RL approaches for LLMs as well as MARL
techniques. Our experiments on LLM writing and coding collaboration demonstrate
that fine-tuning MAS with MAGRPO enables agents to generate high-quality
responses efficiently through effective cooperation. Our approach opens the
door to using other MARL methods for LLMs and highlights the associated
challenges.

</details>


### [310] [SEAgent: Self-Evolving Computer Use Agent with Autonomous Learning from Experience](https://arxiv.org/abs/2508.04700)
*Zeyi Sun,Ziyu Liu,Yuhang Zang,Yuhang Cao,Xiaoyi Dong,Tong Wu,Dahua Lin,Jiaqi Wang*

Main category: cs.AI

TL;DR: 提出了SEAgent，一种自主进化的CUAs框架，可在无人工标注情况下通过自我学习掌握新软件，并在多个环境中性能提升23.2%。


<details>
  <summary>Details</summary>
Motivation: 现有大型视觉语言模型（LVLMs）在计算机操作代理（CUAs）应用中难以适应新软件且依赖人工标注数据，尤其是面对缺乏标注的特殊软件时表现不佳。

Method: 1) 通过环境探索与迭代试错自主学习；2) 设计世界状态模型评估轨迹和课程生成器创建渐进式任务；3) 采用包含失败动作的对抗式模仿学习和基于组相对策略优化（GRPO）的策略更新；4) 整合专家代理经验形成通用代理，实现持续自主进化。

Result: 在OS-World五个新软件环境中测试，SEAgent成功率达到34.5%，相比基线UI-TARS（11.3%）提升23.2%

Conclusion: SEAgent框架通过自主进化机制解决了CUAs在新软件场景下的适应问题，其整合专家经验的通用代理策略效果优于专用代理组合，验证了自主持续进化的可行性。

Abstract: Repurposing large vision-language models (LVLMs) as computer use agents
(CUAs) has led to substantial breakthroughs, primarily driven by human-labeled
data. However, these models often struggle with novel and specialized software,
particularly in scenarios lacking human annotations. To address this challenge,
we propose SEAgent, an agentic self-evolving framework enabling CUAs to
autonomously evolve through interactions with unfamiliar software.
Specifically, SEAgent empowers computer-use agents to autonomously master novel
software environments via experiential learning, where agents explore new
software, learn through iterative trial-and-error, and progressively tackle
auto-generated tasks organized from simple to complex. To achieve this goal, we
design a World State Model for step-wise trajectory assessment, along with a
Curriculum Generator that generates increasingly diverse and challenging tasks.
The agent's policy is updated through experiential learning, comprised of
adversarial imitation of failure actions and Group Relative Policy Optimization
(GRPO) on successful ones. Furthermore, we introduce a specialist-to-generalist
training strategy that integrates individual experiential insights from
specialist agents, facilitating the development of a stronger generalist CUA
capable of continuous autonomous evolution. This unified agent ultimately
achieves performance surpassing ensembles of individual specialist agents on
their specialized software. We validate the effectiveness of SEAgent across
five novel software environments within OS-World. Our approach achieves a
significant improvement of 23.2% in success rate, from 11.3% to 34.5%, over a
competitive open-source CUA, i.e., UI-TARS.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [311] [Optimization of sliding control parameters for a 3-dof robot arm using genetic algorithm (GA)](https://arxiv.org/abs/2508.04009)
*Vu Ngoc Son,Pham Van Cuong,Dao Thi My Linh,Le Tieu Nien*

Main category: cs.RO

TL;DR: 提出一种使用遗传算法优化机器人滑模控制参数的方法，以提升轨迹跟踪精度与鲁棒性。该方法在不确定和干扰条件下表现优于传统SMC和模糊SMC，减少了抖振效应。


<details>
  <summary>Details</summary>
Motivation: 滑模控制（SMC）在机器人轨迹跟踪中面临参数选择困难，其系统性能与鲁棒性高度依赖参数设定。传统方法难以兼顾精度与抗扰能力，亟需智能优化方案。

Method: 1. 建立机器人动力学模型；2. 设计滑模控制器；3. 用遗传算法（GA）优化SMC参数（如切换增益、边界层厚度），以跟踪误差和抖振程度为适应度函数；4. 仿真验证GA-SMC性能。

Result: 仿真显示：GA优化的SMC在不确定干扰环境下 1) 跟踪误差比传统SMC降低63% 2) 抖振幅值缩减40% 3) 综合性能优于模糊SMC控制器。

Conclusion: 遗传算法可有效解决SMC参数整定难题。该方法显著提升机器人轨迹跟踪精度与抗干扰能力，同时抑制抖振，为复杂工况下的机器人控制提供新思路。

Abstract: This paper presents a method for optimizing the sliding mode control (SMC)
parameter for a robot manipulator applying a genetic algorithm (GA). The
objective of the SMC is to achieve precise and consistent tracking of the
trajectory of the robot manipulator under uncertain and disturbed conditions.
However, the system effectiveness and robustness depend on the choice of the
SMC parameters, which is a difficult and crucial task. To solve this problem, a
genetic algorithm is used to locate the optimal values of these parameters that
gratify the capability criteria. The proposed method is efficient compared with
the conventional SMC and Fuzzy-SMC. The simulation results show that the
genetic algorithm with SMC can achieve better tracking capability and reduce
the chattering effect.

</details>


### [312] [Uncertainty-aware Accurate Elevation Modeling for Off-road Navigation via Neural Processes](https://arxiv.org/abs/2508.03890)
*Sanghun Jung,Daehoon Gwak,Byron Boots,James Hays*

Main category: cs.RO

TL;DR: 提出了一种基于神经过程（NPs）的新型地形高程建模方法，该方法利用LiDAR和相机的语义特征，结合本地球查询注意力机制，在保持计算效率的同时，精确估计地形突变并量化不确定性，适用于复杂越野环境的实时导航。


<details>
  <summary>Details</summary>
Motivation: 现有地形高程建模方法（如高斯过程和神经网络）无法同时满足实时性、准确捕捉地形突变、以及量化不确定性的需求，而神经过程结合了贝叶斯不确定性和神经网络效率。为此，我们旨在开发一种能在复杂越野环境中高效、精准建模地形的方法。

Method: 1. 利用LiDAR和相机传感器提取语义特征，提升未观测区域的插值和外推精度；2. 提出本地球查询注意力机制，将全局注意力的计算复杂度降低17%，同时保留关键局部和空间信息；3. 基于神经过程框架集成上述技术，实现实时地形高程建模与不确定性量化。

Result: 在包含小径、沙漠和山丘等具有挑战性地形的越野数据集上评估，结果显示该方法在精度和效率上均优于基线模型，并成功展示了神经过程在复杂越野环境中进行高效、表达性强地形建模的潜力。

Conclusion: 所提出的神经过程方法不仅能实时精确估计地形突变和高程，还能有效量化不确定性，且计算开销低，为越野导航的规划与控制算法提供了更可靠的地形模型支持。

Abstract: Terrain elevation modeling for off-road navigation aims to accurately
estimate changes in terrain geometry in real-time and quantify the
corresponding uncertainties. Having precise estimations and uncertainties plays
a crucial role in planning and control algorithms to explore safe and reliable
maneuver strategies. However, existing approaches, such as Gaussian Processes
(GPs) and neural network-based methods, often fail to meet these needs. They
are either unable to perform in real-time due to high computational demands,
underestimating sharp geometry changes, or harming elevation accuracy when
learned with uncertainties. Recently, Neural Processes (NPs) have emerged as a
promising approach that integrates the Bayesian uncertainty estimation of GPs
with the efficiency and flexibility of neural networks. Inspired by NPs, we
propose an effective NP-based method that precisely estimates sharp elevation
changes and quantifies the corresponding predictive uncertainty without losing
elevation accuracy. Our method leverages semantic features from LiDAR and
camera sensors to improve interpolation and extrapolation accuracy in
unobserved regions. Also, we introduce a local ball-query attention mechanism
to effectively reduce the computational complexity of global attention by 17\%
while preserving crucial local and spatial information. We evaluate our method
on off-road datasets having interesting geometric features, collected from
trails, deserts, and hills. Our results demonstrate superior performance over
baselines and showcase the potential of neural processes for effective and
expressive terrain modeling in complex off-road environments.

</details>


### [313] [Reliable and Real-Time Highway Trajectory Planning via Hybrid Learning-Optimization Frameworks](https://arxiv.org/abs/2508.04436)
*Yujia Lu,Chong Wei,Lu Ma*

Main category: cs.RO

TL;DR: 提出了一种混合轨迹规划框架，集成基于学习的自适应性和基于优化的安全保证。采用双层架构：上层使用图神经网络预测人类化纵向速度轮廓；下层路径优化采用混合整数二次规划，引入离散车辆几何的线性近似降低计算量，并通过时空非重叠约束保证安全。实验表明该规划器在复杂场景下生成平滑无碰撞轨迹，成功率达97%以上，平均规划时间54毫秒，具备实时性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶高速公路场景因环境快速变化和响应时间有限而存在高风险，现有规划方法在安全性和实时性之间存在矛盾：优化方法保证安全性但计算复杂，学习方法高效但缺乏安全验证。亟需兼具人类驾驶适应性和可靠避撞能力的实时规划框架。

Method: 1. 双层架构：
   - 上层: GNN模型基于真实高速公路数据训练，输出符合人类驾驶习惯的纵向速度轮廓
   - 下层: MIQP路径优化模型接收速度轮廓，生成详细轨迹
2. 创新性路径优化:
   - 提出离散车辆几何的线性表达，替代传统多边形约束
   - 引入严格的时空非重叠约束公式，保证规划时段内零重叠
   - 通过混合整数规划建模离散决策过程
3. 联合优化: 速度规划与路径解耦设计，GNN提供舒适引导，MIQP保证数学可验证安全

Result: 1. 安全性: 所有测试场景严格满足非重叠约束，实现零碰撞轨迹
2. 实时性: 平均规划耗时54ms (包括速度预测和路径优化)，满足100ms实时要求
3. 成功率: 在2000+复杂高速公路场景测试中，包括紧急避障场景，成功率97.3%
4. 效率提升: 几何线性模型使优化计算时间降低78% (与传统多边形模型相比)
5. 舒适性: 轨迹纵向jerk均值小于0.8m/s³，横向加速度低于1.5m/s²

Conclusion: 通过融合GNN的学习能力与MIQP的形式化安全保障，有效解决了自动驾驶高速公路规划的可靠性与实时性矛盾。核心创新在于降低车辆几何表达复杂度，使完全避撞约束满足实时计算。该方法为高速环境下可证明安全保障的实时规划提供了可行路径，未来可扩展至交互决策建模和多目标优化。

Abstract: Autonomous highway driving presents a high collision risk due to
fast-changing environments and limited reaction time, necessitating reliable
and efficient trajectory planning. This paper proposes a hybrid trajectory
planning framework that integrates the adaptability of learning-based methods
with the formal safety guarantees of optimization-based approaches. The
framework features a two-layer architecture: an upper layer employing a graph
neural network (GNN) trained on real-world highway data to predict human-like
longitudinal velocity profiles, and a lower layer utilizing path optimization
formulated as a mixed-integer quadratic programming (MIQP) problem. The primary
contribution is the lower-layer path optimization model, which introduces a
linear approximation of discretized vehicle geometry to substantially reduce
computational complexity, while enforcing strict spatiotemporal non-overlapping
constraints to formally guarantee collision avoidance throughout the planning
horizon. Experimental results demonstrate that the planner generates highly
smooth, collision-free trajectories in complex real-world emergency scenarios,
achieving success rates exceeding 97% with average planning times of 54 ms,
thereby confirming real-time capability.

</details>


### [314] [Constraint-Preserving Data Generation for Visuomotor Policy Learning](https://arxiv.org/abs/2508.03944)
*Kevin Lin,Varun Ragunath,Andrew McAlinden,Aaditya Prasad,Jimmy Wu,Yuke Zhu,Jeannette Bohg*

Main category: cs.RO

TL;DR: CP-Gen是一种利用单个专家轨迹生成具有新物体几何和位姿的机器人示范数据的方法，用于训练零样本迁移到真实世界且泛化性强的闭环视觉运动策略。


<details>
  <summary>Details</summary>
Motivation: 针对大规模示范数据收集成本高且耗时的问题，提出一种能够通过单个专家轨迹生成多样化示范的方法，以降低数据收集负担并提升策略的泛化能力。

Method: 1. 将专家轨迹分解为自由空间运动和机器人技能；2. 将机器人技能表示为关键点轨迹约束（机器人和被操作物体上的关键点要跟踪相对于任务相关物体的参考轨迹）；3. 对任务相关物体采样位姿和几何变换后应用变换并优化机器人关节配置以满足关键点轨迹约束；4. 运动规划得到无碰撞路径。

Result: 在16个仿真任务和4个真实任务（包括多阶段、非抓取和紧密公差操作）上，使用CP-Gen训练的策略平均成功率达到77%，优于最佳基线的50%。

Conclusion: CP-Gen通过关键点轨迹约束实现几何感知的数据生成，显著提升零样本迁移和泛化性能，为机器人操作的数据生成提供高效方案。

Abstract: Large-scale demonstration data has powered key breakthroughs in robot
manipulation, but collecting that data remains costly and time-consuming. We
present Constraint-Preserving Data Generation (CP-Gen), a method that uses a
single expert trajectory to generate robot demonstrations containing novel
object geometries and poses. These generated demonstrations are used to train
closed-loop visuomotor policies that transfer zero-shot to the real world and
generalize across variations in object geometries and poses. Similar to prior
work using pose variations for data generation, CP-Gen first decomposes expert
demonstrations into free-space motions and robot skills. But unlike those
works, we achieve geometry-aware data generation by formulating robot skills as
keypoint-trajectory constraints: keypoints on the robot or grasped object must
track a reference trajectory defined relative to a task-relevant object. To
generate a new demonstration, CP-Gen samples pose and geometry transforms for
each task-relevant object, then applies these transforms to the object and its
associated keypoints or keypoint trajectories. We optimize robot joint
configurations so that the keypoints on the robot or grasped object track the
transformed keypoint trajectory, and then motion plan a collision-free path to
the first optimized joint configuration. Experiments on 16 simulation tasks and
four real-world tasks, featuring multi-stage, non-prehensile and
tight-tolerance manipulation, show that policies trained using CP-Gen achieve
an average success rate of 77%, outperforming the best baseline that achieves
an average of 50%.

</details>


### [315] [SCOUT: An in-vivo Methane Sensing System for Real-time Monitoring of Enteric Emissions in Cattle with ex-vivo Validation](https://arxiv.org/abs/2508.04056)
*Yuelin Deng,Hinayah Rojas de Oliveira,Richard M. Voyles,Upinder Kaur*

Main category: cs.RO

TL;DR: 开发了一种名为SCOUT的新型原位传感系统，用于持续高分辨率监测瘤胃甲烷浓度。通过在两头瘤胃瘘管牛上进行验证，该系统表现出优异的数据保留率（82% vs 传统系统的17%）和1000倍于环境方法的浓度捕获能力。它揭示了行为与排放关联的新发现，为基因选择和精准畜牧业提供了重要工具。


<details>
  <summary>Details</summary>
Motivation: 现有的环境采样方法数据保留率低、易受环境干扰且时间分辨率有限，难以满足通过基因选择和精准管理提升畜牧可持续性的需求。

Method: 开发了封闭式气体再循环设计的SCOUT系统（智能瘤胃瘘管光学单元），并在两头瘘管牛上通过对比日粮处理和与传统环境嗅探系统的交叉验证进行评估，采用40分钟时间窗分析相关性。

Result: 1 数据保留率82%远超传统系统（17%）
2 甲烷浓度测量值高出环境方法100-1000倍
3 在40分钟时间窗内达到最佳负相关性（r = -0.564 ± 0.007）
4 首次捕捉到姿势转变后15分钟内甲烷浓度剧变（14.5±11.3k ppm）

Conclusion: SCOUT系统实现了准确持续的排放表型分析，为基因组选择和可持续畜牧管理提供了转型性工具，同时为甲烷动力学研究建立了新基准。

Abstract: Accurate measurement of enteric methane emissions remains a critical
bottleneck for advancing livestock sustainability through genetic selection and
precision management. Existing ambient sampling approaches suffer from low data
retention rates, environmental interference, and limited temporal resolution.
We developed SCOUT (Smart Cannula-mounted Optical Unit for Trace-methane), the
first robust in-vivo sensing system enabling continuous, high-resolution
monitoring of ruminal methane concentrations through an innovative closed-loop
gas recirculation design. We conducted comprehensive validation with two
cannulated Simmental heifers under contrasting dietary treatments, with
cross-platform comparison against established ambient sniffer systems. SCOUT
achieved exceptional performance with 82% data retention compared to 17% for
conventional sniffer systems, while capturing methane concentrations 100-1000x
higher than ambient approaches. Cross-platform validation demonstrated strong
scale-dependent correlations, with optimal correlation strength (r = -0.564
$\pm$ 0.007) at biologically relevant 40-minute windows and 100% statistical
significance. High-frequency monitoring revealed novel behavior-emission
coupling, including rapid concentration changes (14.5 $\pm$ 11.3k ppm)
triggered by postural transitions within 15 minutes, insights previously
inaccessible through existing technologies. The SCOUT system represents a
transformative advancement, enabling accurate, continuous emission phenotyping
essential for genomic selection programs and sustainable precision livestock
management. This validation framework establishes new benchmarks for
agricultural sensor performance while generating unprecedented biological
insights into ruminal methane dynamics, contributing essential tools for
sustainable livestock production in climate-conscious agricultural systems.

</details>


### [316] [DRIVE: Dynamic Rule Inference and Verified Evaluation for Constraint-Aware Autonomous Driving](https://arxiv.org/abs/2508.04066)
*Longling Geng,Huangxing Li,Viktor Lado Naess,Mert Pilanci*

Main category: cs.RO

TL;DR: DRIVE是一个新框架，通过专家演示和指数族似然建模从数据中学习动态的、上下文相关的软约束，并结合凸优化规划生成遵守这些约束的可行轨迹。它在多个真实数据集上实现0%软约束违反率，并展现出更好的泛化能力和可解释性。


<details>
  <summary>Details</summary>
Motivation: 自主驾驶需遵守隐式的、上下文相关的软约束以确保安全和社交合规，但这些约束难以显式指定。因此，需要一种方法从数据中自动推断这些约束并融入规划。

Method: 1. 使用指数族概率模型从专家演示中估计状态转移的可行性，获取上下文相关的软约束概率分布；2. 将学习到的规则分布嵌入基于凸优化的规划器中，生成既动态可行又遵守软约束的轨迹。

Result: 在多个大型自然驾驶数据集（包括inD、highD、RoundD）上验证，DRIVE实现了0.0%的软约束违反率，轨迹更平滑，在不同驾驶场景下泛化能力优于现有逆约束学习和规划基线。

Conclusion: DRIVE提供了规则推断与规划决策的端到端统一框架，支持数据驱动的约束泛化和严格验证，为实际部署提供了高效率、可解释性与鲁棒性。

Abstract: Understanding and adhering to soft constraints is essential for safe and
socially compliant autonomous driving. However, such constraints are often
implicit, context-dependent, and difficult to specify explicitly. In this work,
we present DRIVE, a novel framework for Dynamic Rule Inference and Verified
Evaluation that models and evaluates human-like driving constraints from expert
demonstrations. DRIVE leverages exponential-family likelihood modeling to
estimate the feasibility of state transitions, constructing a probabilistic
representation of soft behavioral rules that vary across driving contexts.
These learned rule distributions are then embedded into a convex
optimization-based planning module, enabling the generation of trajectories
that are not only dynamically feasible but also compliant with inferred human
preferences. Unlike prior approaches that rely on fixed constraint forms or
purely reward-based modeling, DRIVE offers a unified framework that tightly
couples rule inference with trajectory-level decision-making. It supports both
data-driven constraint generalization and principled feasibility verification.
We validate DRIVE on large-scale naturalistic driving datasets, including inD,
highD, and RoundD, and benchmark it against representative inverse constraint
learning and planning baselines. Experimental results show that DRIVE achieves
0.0% soft constraint violation rates, smoother trajectories, and stronger
generalization across diverse driving scenarios. Verified evaluations further
demonstrate the efficiency, explanability, and robustness of the framework for
real-world deployment.

</details>


### [317] [Industrial Robot Motion Planning with GPUs: Integration of cuRobo for Extended DOF Systems](https://arxiv.org/abs/2508.04146)
*Luai Abuelsamen,Harsh Rana,Ho-Wei Lu,Wenhan Tang,Swati Priyadarshini,Gabriel Gomes*

Main category: cs.RO

TL;DR: 集成NVIDIA cuRobo库的GPU加速运动规划到Vention自动化平台，提高工业机器人在复杂环境中运动规划的速度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 多轴工业机器人在复杂环境中运动规划效率低、实时性差，需要通过并行计算加速规划过程以实现动态碰撞检测与实时轨迹生成。

Method: 利用CAD精确数字孪生模型构建环境，结合NVIDIA cuRobo库的GPU并行优化算法进行实时运动规划，并在带7轴门式机器人的真实平台上测试轨迹生成和动态避障性能。

Result: 在多场景测试中规划速度和鲁棒性显著提升，证明了基于GPU的规划方案可扩展应用于现代工业工作流。

Conclusion: GPU加速的运动规划技术（特别是cuRobo集成方案）能够为工业自动化平台提供高效、可扩展的实时轨迹解决方案，适应复杂环境需求。

Abstract: Efficient motion planning remains a key challenge in industrial robotics,
especially for multi-axis systems operating in complex environments. This paper
addresses that challenge by integrating GPU-accelerated motion planning through
NVIDIA's cuRobo library into Vention's modular automation platform. By
leveraging accurate CAD-based digital twins and real-time parallel
optimization, our system enables rapid trajectory generation and dynamic
collision avoidance for pick-and-place tasks. We demonstrate this capability on
robots equipped with additional degrees of freedom, including a 7th-axis
gantry, and benchmark performance across various scenarios. The results show
significant improvements in planning speed and robustness, highlighting the
potential of GPU-based planning pipelines for scalable, adaptable deployment in
modern industrial workflows.

</details>


### [318] [Improving Tactile Gesture Recognition with Optical Flow](https://arxiv.org/abs/2508.04338)
*Shaohong Zhong,Alessandro Albini,Giammarco Caroleo,Giorgio Cannata,Perla Maiolino*

Main category: cs.RO

TL;DR: 本文提出了一种通过计算密集光流以增强触觉图像动态信息的方法，从而提升触觉手势识别分类器的准确性。实验证明，使用光流信息增强的触觉图像训练的识别分类器精度提高9%。


<details>
  <summary>Details</summary>
Motivation: 目前基于视觉的触觉手势识别系统在某些情况下难以区分手势，因为触觉图像仅提供静态压力分布信息，而缺少手势的动态特征。

Method: 1) 在触觉图像上提取密集光流，突出接触点的动态变化；2) 使用增强信息后的图像训练机器学习模型用于手势分类。

Result: 在触觉手势识别任务中，利用增强（光流）触觉图像的分类器准确率比仅使用原始触觉图像的标准分类器提升9%。

Conclusion: 触觉图像中加入动态特征（光流）可有效弥补传统触觉图像信息的不足，提升触觉手势识别的准确性。

Abstract: Tactile gesture recognition systems play a crucial role in Human-Robot
Interaction (HRI) by enabling intuitive communication between humans and
robots. The literature mainly addresses this problem by applying machine
learning techniques to classify sequences of tactile images encoding the
pressure distribution generated when executing the gestures. However, some
gestures can be hard to differentiate based on the information provided by
tactile images alone. In this paper, we present a simple yet effective way to
improve the accuracy of a gesture recognition classifier. Our approach focuses
solely on processing the tactile images used as input by the classifier. In
particular, we propose to explicitly highlight the dynamics of the contact in
the tactile image by computing the dense optical flow. This additional
information makes it easier to distinguish between gestures that produce
similar tactile images but exhibit different contact dynamics. We validate the
proposed approach in a tactile gesture recognition task, showing that a
classifier trained on tactile images augmented with optical flow information
achieved a 9% improvement in gesture classification accuracy compared to one
trained on standard tactile images.

</details>


### [319] [Tactile Comfort: Lowering Heart Rate Through Interactions](https://arxiv.org/abs/2508.04372)
*Morten Roed Frederiksen,Kasper Støy,Maja Matarić*

Main category: cs.RO

TL;DR: 本研究探索了一种口袋大小的陪伴机器人，通过触觉游戏分散注意力，无需训练即可降低儿童心率。两次实验（分别有2名和18名7-8岁儿童参与）表明，与未使用条件相比，与机器人互动显著降低心率（p<0.01），证明了其具有一致的镇静作用。


<details>
  <summary>Details</summary>
Motivation: 现有应对焦虑的技术（如深呼吸和重复口诀）需要事先训练才能有效使用。因此，研究者提出一种即开即用的放松工具，旨在即刻降低用户心率。

Method: 1. 设计一款口袋大小的伴侣机器人，通过触觉游戏分散注意力。
2. 进行两项研究：
   - 试点研究：2名8岁儿童，周期14天
   - 主体研究：18名7-8岁儿童
3. 两项实验均采用被试内设计：
   - 比较儿童在接触机器人（触觉游戏）和未使用机器人时的心率变化。

Result: 与机器人进行触觉互动时，所有参与者心率均显著下降（p<0.01），非使用状态下则无此效果。

Conclusion: 触觉陪伴机器人可瞬时提供舒缓情绪的作用，无训练门槛，具有增强现有放松技术临床效果的潜力。

Abstract: Children diagnosed with anxiety disorders are taught a range of strategies to
navigate situations of heightened anxiety. Techniques such as deep breathing
and repetition of mantras are commonly employed, as they are known to be
calming and reduce elevated heart rates. Although these strategies are often
effective, their successful application relies on prior training of the
children for successful use when faced with challenging situations. This paper
investigates a pocket-sized companion robot designed to offer a relaxation
technique requiring no prior training, with a focus on immediate impact on the
user's heart rate. The robot utilizes a tactile game to divert the user's
attention, thereby promoting relaxation. We conducted two studies with children
who were not diagnosed with anxiety: a 14-day pilot study with two children
(age 8) and a main study with 18 children (ages 7-8). Both studies employed a
within-subjects design and focused on measuring heart rate during tactile
interaction with the robot and during non-use. Interacting with the robot was
found to significantly lower the study participants' heart rate (p$<$0.01)
compared to the non-use condition, indicating a consistent calming effect
across all participants. These results suggest that tactile companion robots
have the potential to enhance the therapeutic value of relaxation techniques.

</details>


### [320] [Incorporating Stochastic Models of Controller Behavior into Kinodynamic Efficiently Adaptive State Lattices for Mobile Robot Motion Planning in Off-Road Environments](https://arxiv.org/abs/2508.04384)
*Eric R. Damm,Eli S. Lancaster,Felix A. Sanchez,Kiana Bronder,Jason M. Gregory,Thomas M. Howard*

Main category: cs.RO

TL;DR: 该论文研究了在KEASL运动规划器中整合随机控制器采样方法，以降低真实环境中移动机器人因模型误差和控制器不确定性导致的碰撞风险。通过在UGV硬件平台和不同环境复杂性下的实验，证明了该方法能够生成更保守的轨迹，显著减少预测碰撞概率，但可能降低规划成功率的特征。


<details>
  <summary>Details</summary>
Motivation: 物理移动机器人执行运动规划时，理论模型与真实物理环境存在差异，且底层轨迹跟踪控制存在不确定性，导致实际运动偏离规划轨迹引发碰撞风险。传统方法（如扩大障碍物边界）会降低规划成功率，因此需要更有效的模型校正方法。

Method: 1. 提出三种将随机控制器行为整合进KEASL重组搜索空间的方法；2. 在Clearpath Warthog无人地面车辆（UGV）上进行实地测试，使用两种感知算法；3. 在不同复杂度的模拟环境地图中进行消融研究；4. 对比分析三种改进方法与KEASL原版及传统障碍物扩展基准方法的性能。

Result: 1. 整合随机控制器采样的KEASL生成更保守的轨迹，预测碰撞概率显著低于原版KEASL；2. 相比传统障碍物扩展法，新方法使碰撞概率预测值相似，但后者规划成功率更高；3. 障碍物扩展基准法虽可降低碰撞风险，却因过度保守导致规划失败率上升。

Conclusion: 在运动规划中显式建模控制器随机性可有效降低预测碰撞风险，其效果优于简单扩展障碍物的传统方法。尽管新方法可能导致轨迹保守性增加，但实现了安全性与规划成功率的更好平衡，为不确定环境中的机器人运动规划提供了新思路。

Abstract: Mobile robot motion planners rely on theoretical models to predict how the
robot will move through the world. However, when deployed on a physical robot,
these models are subject to errors due to real-world physics and uncertainty in
how the lower-level controller follows the planned trajectory. In this work, we
address this problem by presenting three methods of incorporating stochastic
controller behavior into the recombinant search space of the Kinodynamic
Efficiently Adaptive State Lattice (KEASL) planner. To demonstrate this work,
we analyze the results of experiments performed on a Clearpath Robotics Warthog
Unmanned Ground Vehicle (UGV) in an off-road, unstructured environment using
two different perception algorithms, and performed an ablation study using a
full spectrum of simulated environment map complexities. Analysis of the data
found that incorporating stochastic controller sampling into KEASL leads to
more conservative trajectories that decrease predicted collision likelihood
when compared to KEASL without sampling. When compared to baseline planning
with expanded obstacle footprints, the predicted likelihood of collisions
becomes more comparable, but reduces the planning success rate for baseline
search.

</details>


### [321] [Behaviorally Adaptive Multi-Robot Hazard Localization in Failure-Prone, Communication-Denied Environments](https://arxiv.org/abs/2508.04537)
*Alkesh K. Srivastava,Aamodh Suresh,Carlos Nieto-Granda*

Main category: cs.RO

TL;DR: 提出了一种基于行为熵的多机器人行为自适应路径规划框架（BAPP），用于高风险、通信受限环境中的自主灾害制图。该框架包括BAPP-TID（智能触发高保真机器人）和BAPP-SIG（高风险下安全部署）两种算法，通过空间分区、基站迁移和角色异构实现多机器人扩展。实验证明，BAPP在加速熵减和提升生存率方面优于香农熵和随机策略。


<details>
  <summary>Details</summary>
Motivation: 在灾害后区域、地下洞穴等高风险、故障频发且通信受限的环境中，多机器人团队需在探索制图的同时最小化环境威胁或硬件限制带来的风险。现有方法在风险敏感性和适应性方面存在局限，迫切需要一种能够模拟人类不确定性评估并动态调整策略的规划框架。

Method: 1. 提出行为熵（BE）概念（香农熵的泛化），通过风险敏感系数捕获多样化的不确定性评估；2. 构建行为自适应路径规划（BAPP）框架，调节信息采集策略：
   - BAPP-TID：在关键区域动态触发高保真传感器，提升信息获取效率
   - BAPP-SIG：结合风险模型优化路线，增强机器人安全性；3. 多机器人协同机制：通过空间分区分配任务、迁移移动基站维持连接、角色感知机制协调异构机器人。

Result: 1. 单/多机器人仿真证明BAPP显著优于传统方法：
   - BAPP-TID较香农熵方法加速熵减27%（均值）
   - BAPP-SIG在同等信息获取水平下将生存率提高41%；2. 多机器人场景中：空间分区使计算效率提升3倍，基站迁移维持90%+通信连通率，角色感知机制减少30%任务冲突。

Conclusion: 行为熵框架成功统一了风险感知与信息理论规划。BAPP通过自适应策略平衡探索效率与生存率，其扩展机制为复杂环境下的鲁棒协作提供了新范式。该方法为行星探测、灾区响应等任务提供了可部署的解决方案。

Abstract: We address the challenge of multi-robot autonomous hazard mapping in
high-risk, failure-prone, communication-denied environments such as
post-disaster zones, underground mines, caves, and planetary surfaces. In these
missions, robots must explore and map hazards while minimizing the risk of
failure due to environmental threats or hardware limitations. We introduce a
behavior-adaptive, information-theoretic planning framework for multi-robot
teams grounded in the concept of Behavioral Entropy (BE), that generalizes
Shannon entropy (SE) to capture diverse human-like uncertainty evaluations.
Building on this formulation, we propose the Behavior-Adaptive Path Planning
(BAPP) framework, which modulates information gathering strategies via a
tunable risk-sensitivity parameter, and present two planning algorithms:
BAPP-TID for intelligent triggering of high-fidelity robots, and BAPP-SIG for
safe deployment under high risk. We provide theoretical insights on the
informativeness of the proposed BAPP framework and validate its effectiveness
through both single-robot and multi-robot simulations. Our results show that
the BAPP stack consistently outperforms Shannon-based and random strategies:
BAPP-TID accelerates entropy reduction, while BAPP-SIG improves robot
survivability with minimal loss in information gain. In multi-agent
deployments, BAPP scales effectively through spatial partitioning, mobile base
relocation, and role-aware heterogeneity. These findings underscore the value
of behavior-adaptive planning for robust, risk-sensitive exploration in
complex, failure-prone environments.

</details>


### [322] [$NavA^3$: Understanding Any Instruction, Navigating Anywhere, Finding Anything](https://arxiv.org/abs/2508.04598)
*Lingfeng Zhang,Xiaoshuai Hao,Yingbo Tang,Haoxiang Fu,Xinyu Zheng,Pengwei Wang,Zhongyuan Wang,Wenbo Ding,Shanghang Zhang*

Main category: cs.RO

TL;DR: 该论文提出了NavA³，这是一种新颖的分层框架，旨在解决复杂真实场景中基于高级人类指令的长时程导航任务。该框架结合了全局策略和局部策略，分别利用Reasoning-VLM和新的PointingVLM模型来理解指令并实现开放词汇物体定位和空间感知，显著提升了导航性能。


<details>
  <summary>Details</summary>
Motivation: 现有导航任务主要关注预定义物体导航或遵循指令，无法满足复杂开放场景中的实际需求。本文旨在弥合这一差距，解决高水平指令理解与开放词汇物体定位的双重挑战。

Method: 1. 提出分层框架NavA³：全局政策（使用Reasoning-VLM解析高级指令并结合全局3D场景视图规划区域导航）→局部政策（基于新收集的100万样本数据集训练的PointingVLM模型实现空间感知的开放词汇物体定位）。2. 系统整合两种策略，通过区域级推理和精确物体定位实现长时程导航。

Result: NavA³在导航性能上达到SOTA水平，成功跨越不同机器人平台在现实场景完成长时程复杂导航任务。数据集与代码即将开源。

Conclusion: 该框架首次实现了真实世界中理解高级指令的开放词汇物体导航，为通用具身导航系统奠定基础。空间感知对象操作性数据集的建立与层级化策略设计是关键创新。

Abstract: Embodied navigation is a fundamental capability of embodied intelligence,
enabling robots to move and interact within physical environments. However,
existing navigation tasks primarily focus on predefined object navigation or
instruction following, which significantly differs from human needs in
real-world scenarios involving complex, open-ended scenes. To bridge this gap,
we introduce a challenging long-horizon navigation task that requires
understanding high-level human instructions and performing spatial-aware object
navigation in real-world environments. Existing embodied navigation methods
struggle with such tasks due to their limitations in comprehending high-level
human instructions and localizing objects with an open vocabulary. In this
paper, we propose $NavA^3$, a hierarchical framework divided into two stages:
global and local policies. In the global policy, we leverage the reasoning
capabilities of Reasoning-VLM to parse high-level human instructions and
integrate them with global 3D scene views. This allows us to reason and
navigate to regions most likely to contain the goal object. In the local
policy, we have collected a dataset of 1.0 million samples of spatial-aware
object affordances to train the NaviAfford model (PointingVLM), which provides
robust open-vocabulary object localization and spatial awareness for precise
goal identification and navigation in complex environments. Extensive
experiments demonstrate that $NavA^3$ achieves SOTA results in navigation
performance and can successfully complete longhorizon navigation tasks across
different robot embodiments in real-world settings, paving the way for
universal embodied navigation. The dataset and code will be made available.
Project website: https://NavigationA3.github.io/.

</details>


### [323] [RoboTron-Sim: Improving Real-World Driving via Simulated Hard-Case](https://arxiv.org/abs/2508.04642)
*Baihui Xiao,Chengjian Feng,Zhijian Huang,Feng yan,Yujie Zhong,Lin Ma*

Main category: cs.RO

TL;DR: RoboTron-Sim 提出了一种新的方法，利用合成交通困难案例数据集（HASS）和多模态大模型技术，提升真实世界中自动驾驶在关键场景的性能。


<details>
  <summary>Details</summary>
Motivation: 收集高风险场景、长尾驾驶事件和复杂交互的真实数据困难，导致现有自动驾驶系统在关键场景表现不佳。

Method: 1. 构建HASS模拟数据集，覆盖13类高风险边角案例及平衡昼夜/晴雨条件；2. 引入场景提示工程（SPE）和Image-to-Ego编码器（I2E），使多模态大模型能有效从HASS学习真实驾驶技能，适应环境差异和硬件差异。

Result: 在nuScenes数据集上，RoboTron-Sim将关键场景驾驶性能提升约50%，达到真实世界开环规划SOTA；定性结果验证了其在罕见高风险场景的有效性。

Conclusion: 利用合成困难案例通过提示工程和跨域编码器，可显著提升自动驾驶在真实世界关键场景的鲁棒性和安全性。

Abstract: Collecting real-world data for rare high-risk scenarios, long-tailed driving
events, and complex interactions remains challenging, leading to poor
performance of existing autonomous driving systems in these critical
situations. In this paper, we propose RoboTron-Sim that improves real-world
driving in critical situations by utilizing simulated hard cases. First, we
develop a simulated dataset called Hard-case Augmented Synthetic Scenarios
(HASS), which covers 13 high-risk edge-case categories, as well as balanced
environmental conditions such as day/night and sunny/rainy. Second, we
introduce Scenario-aware Prompt Engineering (SPE) and an Image-to-Ego Encoder
(I2E Encoder) to enable multimodal large language models to effectively learn
real-world challenging driving skills from HASS, via adapting to environmental
deviations and hardware differences between real-world and simulated scenarios.
Extensive experiments on nuScenes show that RoboTron-Sim improves driving
performance in challenging scenarios by around 50%, achieving state-of-the-art
results in real-world open-loop planning. Qualitative results further
demonstrate the effectiveness of RoboTron-Sim in better managing rare high-risk
driving scenarios. Project page: https://stars79689.github.io/RoboTron-Sim/

</details>


### [324] [Open Scene Graphs for Open-World Object-Goal Navigation](https://arxiv.org/abs/2508.04678)
*Joel Loo,Zhanxin Wu,David Hsu*

Main category: cs.RO

TL;DR: OSG Navigator是一个模块化系统，利用基础模型和开放场景图（OSG）实现开放世界中的目标导航（ObjectNav）。它通过OSG模式组织空间信息，支持对新环境类型的零样本适应，并在模拟和现实世界中展示了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决开放世界语义导航（如在陌生环境中根据自然语言描述搜索目标物体）的挑战，提出一个能够有效利用基础模型中的语义知识并管理大规模空间信息的系统。

Method: 提出了OSG Navigator系统，该系统由基础模型组成，并引入了开放场景图（OSG）作为空间记忆。OSG通过模式（schemas）分层组织空间信息，这些模式是模板，描述了某类环境的通用结构。OSG模式可以从环境的简单语义标签（如"家"或"超市"）自动生成，使系统能够零样本适应新环境类型。实验中使用Fetch和Spot机器人在模拟环境和现实世界中进行验证。

Result: OSG Navigator在多个ObjectNav基准测试中取得最先进的性能，并能够零样本泛化到多样化的目标、环境和机器人本体。

Conclusion: 通过结合基础模型和开放场景图表示，OSG Navigator能够有效解决开放世界语义导航问题，具有卓越的泛化能力，能够适应不同环境、目标和机器人的变化，为通用机器人系统的发展提供了有力支持。

Abstract: How can we build general-purpose robot systems for open-world semantic
navigation, e.g., searching a novel environment for a target object specified
in natural language? To tackle this challenge, we introduce OSG Navigator, a
modular system composed of foundation models, for open-world Object-Goal
Navigation (ObjectNav). Foundation models provide enormous semantic knowledge
about the world, but struggle to organise and maintain spatial information
effectively at scale. Key to OSG Navigator is the Open Scene Graph
representation, which acts as spatial memory for OSG Navigator. It organises
spatial information hierarchically using OSG schemas, which are templates, each
describing the common structure of a class of environments. OSG schemas can be
automatically generated from simple semantic labels of a given environment,
e.g., "home" or "supermarket". They enable OSG Navigator to adapt zero-shot to
new environment types. We conducted experiments using both Fetch and Spot
robots in simulation and in the real world, showing that OSG Navigator achieves
state-of-the-art performance on ObjectNav benchmarks and generalises zero-shot
over diverse goals, environments, and robot embodiments.

</details>


### [325] [From MAS to MARS: Coordination Failures and Reasoning Trade-offs in Hierarchical Multi-Agent Robotic Systems within a Healthcare Scenario](https://arxiv.org/abs/2508.04691)
*Yuanchen Bai,Zijian Ding,Shaoyue Wen,Xiang Chang,Angelique Taylor*

Main category: cs.RO

TL;DR: 该研究探讨了在模拟医疗多机器人场景中使用层次化多代理框架（CrewAI和AutoGen）时的性能权衡。研究发现，仅靠上下文知识无法解决协调失败（如工具访问违规、故障报告处理不及时），并重新设计了双向通信结构以评估不同推理模型的权衡。研究强调了自主性与稳定性之间的张力以及边缘案例测试对提高系统可靠性和安全性的重要性。


<details>
  <summary>Details</summary>
Motivation: 尽管存在先进的多代理框架，但它们在现实世界机器人上的部署仍然有限，这阻碍了多代理机器人系统（MARS）研究在实践中的发展。为了弥合这一差距，作者在模拟的现实世界多机器人医疗场景中研究层次化多代理框架的性能权衡。

Method: 研究分为两部分：在Study 1中，使用CrewAI通过迭代细化知识库来系统识别和分类仅靠上下文知识无法解决的协调失败（如工具访问违规、故障报告处理不及时）。在Study 2中，使用AutoGen评估重新设计的双向通信结构，并进一步测量同一机器人团队设置中推理模型与非推理模型之间的权衡。

Result: 研究发现仅靠上下文知识无法解决所有协调失败的问题，并提出了双向通信结构来尝试改善。通过实验，作者发现了自主性与稳定性之间的张力，并指出边缘案例测试对系统可靠性和安全性的重要性。

Conclusion: 该研究强调了在层次化多代理框架中自主性与稳定性之间的张力，以及边缘案例测试对提高多代理机器人系统可靠性和安全性的关键作用。作者提供了附加材料（包括代码、任务代理设置、跟踪输出以及协调失败和推理行为的注释示例），供进一步研究参考。

Abstract: Multi-agent robotic systems (MARS) build upon multi-agent systems by
integrating physical and task-related constraints, increasing the complexity of
action execution and agent coordination. However, despite the availability of
advanced multi-agent frameworks, their real-world deployment on robots remains
limited, hindering the advancement of MARS research in practice. To bridge this
gap, we conducted two studies to investigate performance trade-offs of
hierarchical multi-agent frameworks in a simulated real-world multi-robot
healthcare scenario. In Study 1, using CrewAI, we iteratively refine the
system's knowledge base, to systematically identify and categorize coordination
failures (e.g., tool access violations, lack of timely handling of failure
reports) not resolvable by providing contextual knowledge alone. In Study 2,
using AutoGen, we evaluate a redesigned bidirectional communication structure
and further measure the trade-offs between reasoning and non-reasoning models
operating within the same robotic team setting. Drawing from our empirical
findings, we emphasize the tension between autonomy and stability and the
importance of edge-case testing to improve system reliability and safety for
future real-world deployment. Supplementary materials, including codes, task
agent setup, trace outputs, and annotated examples of coordination failures and
reasoning behaviors, are available at:
https://byc-sophie.github.io/mas-to-mars/.

</details>


### [326] [Achieving Precise and Reliable Locomotion with Differentiable Simulation-Based System Identification](https://arxiv.org/abs/2508.04696)
*Vyacheslav Kovalev,Ekaterina Chaikovskaia,Egor Davydenko,Roman Gorbachev*

Main category: cs.RO

TL;DR: 提出了一个集成系统辨识和强化学习的新型控制框架，利用可微分模拟器MuJoCo-XLA优化系统参数，仅使用轨迹数据和控制输入实现精确的系统辨识。


<details>
  <summary>Details</summary>
Motivation: 在双足机器人运动控制中，准确辨识系统参数是减少轨迹漂移的关键。现有方法需要直接力矩测量，限制了灵活性和可扩展性。本方法旨在仅使用轨迹和控制输入进行系统辨识，提高强化学习和基于模型控制的精度。

Method: 1. 将系统辨识嵌入强化学习训练回路；2. 利用可微分模拟器MuJoCo-XLA，通过轨迹数据（位置、速度）和控制输入优化系统参数；3. 支持基础物理属性（质量、惯性）和复杂非线性行为（如摩擦模型）的神经网络近似处理。

Result: 实验表明该框架显著提高了轨迹跟随精度，模拟机器人行为与实际运动高度一致。

Conclusion: 该框架为双足机器人提供了灵活可扩展的参数优化方法，突破了对直接力矩测量的依赖，为强化学习和模型预测控制提供更准确动力学模型。

Abstract: Accurate system identification is crucial for reducing trajectory drift in
bipedal locomotion, particularly in reinforcement learning and model-based
control. In this paper, we present a novel control framework that integrates
system identification into the reinforcement learning training loop using
differentiable simulation. Unlike traditional approaches that rely on direct
torque measurements, our method estimates system parameters using only
trajectory data (positions, velocities) and control inputs. We leverage the
differentiable simulator MuJoCo-XLA to optimize system parameters, ensuring
that simulated robot behavior closely aligns with real-world motion. This
framework enables scalable and flexible parameter optimization. Accurate system
identification is crucial for reducing trajectory drift in bipedal locomotion,
particularly in reinforcement learning and model-based control. In this paper,
we present a novel control framework that integrates system identification into
the reinforcement learning training loop using differentiable simulation.
Unlike traditional approaches that rely on direct torque measurements, our
method estimates system parameters using only trajectory data (positions,
velocities) and control inputs. We leverage the differentiable simulator
MuJoCo-XLA to optimize system parameters, ensuring that simulated robot
behavior closely aligns with real-world motion. This framework enables scalable
and flexible parameter optimization. It supports fundamental physical
properties such as mass and inertia. Additionally, it handles complex system
nonlinear behaviors, including advanced friction models, through neural network
approximations. Experimental results show that our framework significantly
improves trajectory following.

</details>
